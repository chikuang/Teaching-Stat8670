{
  "hash": "3e28812630fc2c59910eb0653a5e5da3",
  "result": {
    "engine": "knitr",
    "markdown": "# Generating Random Variables\n\n\\newcommand{\\unif}{\\operatorname{Unif}}\n\\newcommand{\\geom}{\\operatorname{Geom}}\n\\newcommand{\\beta}{\\operatorname{Beta}}\n\\newcommand{\\bern}{\\operatorname{Bern}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\iid}{\\overset{iid}{\\sim}}\n\\newcommand{\\cov}{\\mathbb{C}ov}\n\n\n\nOne of the fundamental tools required in computational statistics is the ability to *simulate random variables* (rvs) from specified probability distributions.\n\n## Overview\n\nIn the simplest case, to simulate drawing an observation at random from a finite population, a method of generating rvs from the discrete uniform distribution is required. Therefore, a suitable generator of uniform pseudo-random numbers is essential.\n\nMethods for generating random variates from other probability distributions all depend on the uniform random number generator (RNG).\n\nIn the Appendices, we have seen that how to use the built-in R functions to generate RVs from some common distributions, such as `runif()`, `rnorm()`, `rbinom()`, etc. In this Section, we will go over some of the common methods to generate RVs from a costume distributions.\n\nExample Theorem\n\n::: {.callout-example title=\"Sampling from a finite population\"}\nIf we already have a finite population of size $N$ with values $x_1, x_2, \\ldots, x_N$ in hand, we can sample from this population *with* and *without* replacement.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(777)\nsample(c(0,1), size = 10, replace = TRUE)  # with replacement\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1 0 1 1 1 1 0 1 1 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Lottery ticket\nsample(1:999999, size = 5, replace = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 567561 203418 450070 287692 435311\n```\n\n\n:::\n\n```{.r .cell-code}\nsample(toupper(letters))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"H\" \"N\" \"J\" \"Y\" \"B\" \"U\" \"F\" \"P\" \"Z\" \"V\" \"W\" \"I\" \"L\" \"A\" \"G\" \"Q\" \"X\" \"D\" \"M\"\n[20] \"R\" \"C\" \"T\" \"O\" \"K\" \"E\" \"S\"\n```\n\n\n:::\n:::\n\n:::\n\n| Distribution      | cdf     | Generator | Parameters           |\n|-------------------|---------|-----------|----------------------|\n| beta              | pbeta   | rbeta     | shape1, shape2       |\n| binomial          | pbinom  | rbinom    | size, prob           |\n| chi-squared       | pchisq  | rchisq    | df                   |\n| exponential       | pexp    | rexp      | rate                 |\n| F                 | pf      | rf        | df1, df2             |\n| gamma             | pgamma  | rgamma    | shape, rate or scale |\n| geometric         | pgeom   | rgeom     | prob                 |\n| lognormal         | plnorm  | rlnorm    | meanlog, sdlog       |\n| negative binomial | pnbinom | rnbinom   | size, prob           |\n| normal            | pnorm   | rnorm     | mean, sd             |\n| Poisson           | ppois   | rpois     | lambda               |\n| Student's t       | pt      | rt        | df                   |\n| uniform           | punif   | runif     | min, max             |\n\n: Common probability distributions and their corresponding R functions for cumulative distribution function (CDF) and random number generation (borrowed from Table 3.1 in reference \\[2\\]). {#tbl-my-table}\n\n## Inverse Transformation Method\n\nThe first method to simulate rvs is the *inverse transformation method* (ITM).\n\n::: {.callout-theorem title=\"Probability Integral Transformation\"}\nIf $X\\sim F_X$ is a continuous rv, then the rv $U = F_X(x) \\sim \\operatorname{Unif}(0,1)$.\n:::\n\nITM of generating rvs applies the probability integral transformation. Define the inverse transformation $$ F^{−1}_X(u) = \\inf\\{x : F_X(x) = u\\},\\quad  0 < u < 1.$$ Then, if $U \\sim \\operatorname{Unif}(0,1)$, the rv $X = F^{−1}_X(U)$ has the distribution $F_X$. This can be shown as, for all $x \\in R$ \\begin{align}\nP\\left(F_X^{-1}(U) \\leq x\\right) & =P\\left(\\inf \\left\\{t: F_X(t)=U\\right\\} \\leq x\\right) \\\\\n& =P\\left(U \\leq F_X(x)\\right) \\\\\n& =F_U\\left(F_X(x)\\right)=F_X(x),\n\\end{align}\n\nHence, $F_X^{-1}(U)$ and $X$ have the same distribution. So, in order to generate rv $X$, we can simulate $U\\sim \\operatorname{Unif}(0,1)$ first, then apply the inverse $F_X^{-1}(u)$.\n\n::: {.callout-note title=\"Procedure with inverse transformation method\"}\nGiven a distribution function $F_X(\\cdot)$, we can simulate/generate a rv $X$ using the ITM in **three** steps:\n\n1.  Derive the inverse function $F_X^{-1}(u)$.\n\n2.  Write a (**R**) command or function to compute $F_X^{-1}(u)$.\n\n3.  For each random variate required:\n\n    i)  Generate a random $u\\sim \\operatorname{Unif}(0,1)$.\n    ii) Obtain $x = F_X^{-1}(u)$.\n:::\n\n### Continuous case\n\nWhen the distribution function $F_X(\\cdot)$ is continuous, the ITM is straightforward to implement.\n\n::: callout-example\nSuppose we want to use the ITM to simulate $N=1000$ rvs from the density $f_X(x)=3x^2,\\quad x\\in(0,1)$.\n\n1.  The cdf is $F_X(x)=x^3$, so the inverse function is $F_X^{-1}(u)=u^{1/3}$.\n\n2.  Simulate $N=1000$ rvs from $u\\sim\\operatorname{Unif}(0,1)$ and apply the inverse function to obtain the 1000 $x$ values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(777)\nN <- 1000\nuVec <- runif(N)\nxVec <- uVec^(1/3)\n\ndf <- data.frame(x = xVec)\n\n# Density histogram with theoretical density overlay\nggplot(df, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30,\n                 fill = \"lightblue\", color = \"black\") +\n  stat_function(fun = function(x) 3*x^2,\n                color = \"red\", size = 1) +\n  labs(title = expression(f(x) == 3*x^2),\n       y = \"Density\", x = \"x\") + theme_minimal()\n```\n\n::: {.cell-output-display}\n![](03-generating-rv_files/figure-html/example2-1.png){width=672}\n:::\n:::\n\n:::\n\n::: {.callout-example title = \"Exponential\"}\n\nSuppose $X\\sim \\exp(\\lambda)$ where $\\lambda$ is the rate parameter. Then $F_X(x) = 1 - e^{-\\lambda x}$, so the inverse function is $F_X^{-1}(u) = -\\frac{1}{\\lambda}\\log(1-u)$. The other fact, is the $U$ and $1-U$ have the same distribution, so we can use either of them, i.e., $x= -\\frac{1}{\\lambda}\\log(u)$ or $x= -\\frac{1}{\\lambda}\\log(1-u)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(777)\nN <- 1000\nlambda <- 0.7\nuVec <- runif(N)\nxVec_1 <- - (1/lambda) * log(uVec)\nxVec_2 <- - (1/lambda) * log(1-uVec)\n\n# Put data into long format for ggplot\ndf <- data.frame(\n  value = c(xVec_1, xVec_2),\n  method = rep(c(\"log(U)\", \"log(1-U)\"), each = N)\n)\n\n# Theoretical density function\nexp_density <- function(x) lambda * exp(-lambda * x)\n\n# Plot\nggplot(df, aes(x = value, fill = method)) +\n  geom_histogram(aes(y = ..density..), bins = 40,\n                 position = \"identity\", alpha = 0.4, color = \"black\") +\n  stat_function(fun = exp_density, color = \"red\", size = 1) +\n  labs(title = bquote(\"Exponential(\" ~ lambda == .(lambda) ~ \")\"),\n       x = \"x\", y = \"Density\") \n```\n\n::: {.cell-output-display}\n![](03-generating-rv_files/figure-html/example-exponential-1.png){width=672}\n:::\n:::\n\n\n:::\n\n### Discrete case\n\nAlthough it is slightly more complicated than the continuous case, the ITM can also be applied to *discrete distributions*. Why?\n\nFirst, in the discrete case, the cdf $F_X(x)$ is **NOT continuous**, instead, a step function, so the inverse function $F_X^{-1}(u)$ is not unique.\n\nHere, if we order the random variable $$\\cdots < x_{(i-1)} < x_{(i)} < x_{(i+1)}< cdots,$$ then the inverse transformation is $F_X^{-1}(u)=x_i$, where $F_X(x_{(i-1)}) < u \\leq F_X(x_{(i)})$.\n\nThen the procedure is:\n\n::: {.callout-note title=\"Procedure with ITM for discrete case\"}\n1.  Derive the cdf $F_X(x)$ and tabulate the values of $x_i$ and $F_X(x_i)$.\n\n2.  Write a (**R**) command or function to compute $F_X^{-1}(u)$.\n\n3.  For each random variate required:\n\n    i)  Generate a random $u\\sim \\operatorname{Unif}(0,1)$.\n    ii) Find $x_i$ such that $F_X(x_{(i-1)}) < u \\leq F_X(x_{(i)})$ and set $x = x_i$.\n:::\n\n::: {.callout-example title=\"Two point distribution\"}\nIn this example, $F_X(0) = f_X(0) = 1 - p$ and $F_X(1) = 1$.\n\nThus, $$\nF_X^{-1}(u) = \n\\begin{cases}\n1, & u > 0.6,\\\\\n0, & u \\leq 0.6.\n\\end{cases}\n$$\n\nThe generator should therefore deliver the numerical value of the logical expression $u > 0.6$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(777)\nn <- 1000\np <- 0.4\nu <- runif(n)\nx <- as.integer(u > 0.6)  # (u > 0.6) is a logical vector\n\n(m_x <- mean(x));  (v_x <- var(x))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.381\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2360751\n```\n\n\n:::\n:::\n\n\nCompare the sample statistics with the theoretical moments. The sample mean of a generated sample should be approximately $p = 0.4$ and the sample variance should be approximately $p(1 - p) = 0.24$, versus our simulated values 0.381 and 0.2360751.\n:::\n\n::: {.callout-example title=\"Geometric distribution\"}\nIn this example, we will use ITM to simulate $X\\sim \\operatorname{Geom}(1/4)$.\n\nLet $q:=1-p$. The pmf is $f(x) = p q^x$, $x = 0,1,2,\\ldots$. At the points of discontinuity $x = 0,1,2,\\ldots$, the cdf is $$\nF(x) = 1 - q^{x+1}.\n$$ For each sample element we need to generate a $u\\sim \\operatorname{Unif}(0,1)$ and solve $$\n1 - q^x < u \\leq 1 - q^{x+1}.\n$$\n\nWhich is equivalent to $x < \\frac{\\log(1 - u)}{\\log(q)} \\leq x+1.$ The solution is $$\nx + 1 = \\left\\lceil \\frac{\\log(1 - u)}{\\log(q)} \\right\\rceil,\n$$ where $\\lceil \\cdot \\rceil$ denotes the ceiling function (and $\\lfloor \\cdot \\rfloor$ is the floor function). Hence, we have,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(999)\nn <- 1000\np <- 0.25\nu <- runif(n)\nk1 <- ceiling(log(1-u) / log(1-p)) - 1\n```\n:::\n\n\nNote again that $U$ and $1 - U$ have the same distribution. Also, the probability that $\\log(1 - u)/\\log(1 - p)$ equals an integer is zero. Thus, we can simplify it to\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk2 <- floor(log(u) / log(1-p))\ndf <- data.frame(\n  value = c(k1, k2),\n  group = rep(c(\"k1\", \"k2\"), each = length(k1))\n)\n\n# Plot both histograms side by side\nggplot(df, aes(x = value, fill = group)) +\n  geom_histogram(alpha = 0.6, position = \"identity\", bins = 40, color = \"black\") +\n  labs(title = \"Histograms of k1 and k2\", x = \"Value\", y = \"Count\") + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](03-generating-rv_files/figure-html/geom2-1.png){width=672}\n:::\n:::\n\n\nThe geometric distribution was particularly easy to simulate by the inverse transform method because it was easy to solve the inequality \\[ F(x-1) \\< u \\leq F(x) \\] rather than compare each $u$ to all the possible values $F(x)$. \\medskip\n\nThe same method applied to the Poisson distribution is more complicated because we do not have an explicit formula for the value of $x$ such that $$\nF(x-1) < u \\leq F(x).\n$$\n\nThe R function \\texttt{rpois} generates random Poisson samples. The basic method to generate a Poisson($\\lambda$) variate is to generate and store the cdf via the recursive formula $$\nf(x+1) = \\frac{\\lambda f(x)}{x+1}, \n\\qquad \nF(x+1) = F(x) + f(x+1).\n$$\n:::\n\n## Acceptance-Rejection Method\n\nIn the previous section, we have seen that the ITM is straightforward to implement when the inverse cdf is available in closed form. However, for many distributions, the *inverse cdf is not available in closed form or is difficult to compute*. In those cases, we need to have other strategies!\n\nThe *acceptance-rejection method* (ARM) is a general method for generating rvs from a distribution with pdf $f_X(x)$, when the inverse cdf is not available in closed form or is difficult to compute.\n\n::: {.callout-definition title=\"The Acceptance--Rejection Method\"}\nSuppose $X$ and $Y$ are rvs with pdfs/pmds $f_X(x)$ and $g_Y(y)$, respectively. Further we suppose there is a constant $k$ such that $$\n\\frac{f_X(t)}{g_Y(t)} \\leq k,\n$$ for all $t$ such that $f_X(t) > 0$.\n\nThen we can simulate $X$ using the following procedure:\n\n1.  Find a rv $Y$ with density $g_Y(\\cdot)$ satisfying $f_X(t)/g_Y(t) \\le k,$ for all $t$ such that $f(t) > 0$.\n\n2.  For each rv, required:\n\n    (i) Generate a random $y$ from the distribution with density $g_Y$.\n    (ii) Generate a random $u\\sim \\operatorname{Unif}(0,1)$.\n    (iii) If $u < f_X(y)/(k g_Y(y))$, accept $y$ and set $x = y$; o.w. reject $y$ and jump back to (i)\n:::\n\nWhy it work?Note that in Step 2c, $$\nP(\\text{accept} \\mid Y) \n= P\\!\\left(U < \\frac{f(Y)}{k g(Y)} \\,\\Big|\\, Y\\right) \n= \\frac{f_X(Y)}{k g_X(Y)}.\n$$\n\nThe total probability of acceptance for any iteration is therefore $$\n\\sum_y P(\\text{accept} \\mid y) P(Y = y) \n= \\sum_y \\frac{f(y)}{k g(y)} g(y) \n= \\frac{1}{k},\n$$ and the number of iterations until acceptance has the geometric distribution with mean $k$. That means, in order to sample $X$, in average, we need $k$ iterations.\n\nNote: The choice of $Y$ and $k$ is crucial for the efficiency of the ARM. A poor choice can lead to a large $k$, resulting in many rejections and inefficiency. We want $Y$ to be easy to simulate, and $k$ to be as small as possible.\n\nDoes this have anything to do with $X$?\n\nTo see that the accepted sample has the same distribution as $X$, apply Bayes' Theorem. In the discrete case, for each $\\ell$ such that $f(\\ell) > 0$, $$\nP(\\ell \\mid \\text{accepted}) \n= \\frac{P(\\text{accepted} \\mid \\ell) g(\\ell)}{P(\\text{accepted})} \n= \\frac{\\big[f(\\ell)/(k g(\\ell))\\big] g(\\ell)}{1/k} \n= f(\\ell).\n$$\n\n::: {.callout-example title=\"ARM for beta\"}\nThis example illustrates the acceptance--rejection method for the beta distribution.\n\nQ: On average, how many random numbers must be simulated to generate $N=1000$ samples from the $\\operatorname{Beta}$(\\alpha=2,\\operatorname{Beta}=2)\\$ distribution by ARM?\n\nA: Depends on the upper bound $k$ of $f_X(t)/_Yg(t)$, which depends on the choice of the function $g_Y(\\cdot)$.\n\nRecall that the $\\operatorname{Beta}(2,2)$ density is $$\nf(t) = 6t(1-t), \\quad 0 < t < 1.\n$$ Let $g(\\cdot)$ be the Uniform(0,1) density. Then $$\n\\frac{f(t)}{g(t)} = \\frac{6t(1-t)}{(1)} = 6t(1-t) \\leq k \\quad \\text{for all } 0 < t < 1.\n$$ It is easy to see that $k = 6$. A random $x$ from $g(x)$ is accepted if $$\n\\frac{f(x)}{kg(x)} = \\frac{6x(1-x)}{6(1)} = x(1-x) > u.\n$$\n\nOn average, $kN = 6\\cdot 1000 =6000$ iterations (12000 random numbers as we need $X$ and $Y$) will be required for $N=1000$. In the following simulation, the counter $\\operatorname{iter}$ for iterations is not necessary, but included to record how many iterations were actually needed to generate the 1000 beta rvs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(7777)\nN <- 1000\nell_accept <- 0       # counter for accepted\niter <- 0       # iterations\ny <- rep(0, N)\n\nwhile (ell_accept < N) {\n  u <- runif(1)\n  iter <- iter + 1\n  x <- runif(1)   # random variate from g\n  if (x * (1-x) > u) {\n    # we accept x\n    ell_accept <- ell_accept + 1\n    y[ell_accept] <- x\n  }\n}\n\niter\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5972\n```\n\n\n:::\n:::\n\n\nIn this simulation, 5972 iterations ( 1.1944\\times 10^{4} random numbers) were required to generate the 1000 beta samples.\n:::\n\n## Using known probability distribution theory\n\nMany types of transformations other than the probability inverse transformation can be applied to simulate random variables. Some examples are\n\n1). If $Z \\sim N(0,1)$, then $V = Z^2 \\sim \\chi^2(1)$.\n\n2). If $Z_1,\\ldots,Z_n \\sim N(0,1)$ are independent, then $$\n  U = \\sum_{i=1}^n Z_i^2 \\sim \\chi^2(n).\n  $$\n\n3). If $U \\sim \\chi^2(m)$ and $V \\sim \\chi^2(n)$ are independent, then $$\n  F = \\frac{U/m}{V/n}\n  $$ has the $F$ distribution with $(m,n)$ degrees of freedom.\n\n4). If $Z \\sim N(0,1)$ and $V \\sim \\chi^2(n)$ are independent, then $$\n  T = \\frac{Z}{\\sqrt{V/n}}\n  $$ has the Student $t$ distribution with $n$ degrees of freedom.\n\n5). If $U,V \\sim \\text{Unif}(0,1)$ are independent, then $$\n  Z_1 = \\sqrt{-2 \\log U}\\, \\cos(2\\pi V), \n  \\qquad\n  Z_2 = \\sqrt{-2 \\log U}\\, \\sin(2\\pi V)\n  $$ are independent standard normal variables.\n\n6). If $U \\sim \\text{Gamma}(r,\\lambda)$ and $V \\sim \\text{Gamma}(s,\\lambda)$ are independent, then $$\n  X = \\frac{U}{U+V}\n  $$ has the $\\text{Beta}(r,s)$ distribution.\n\n7). If $U,V \\sim \\text{Unif}(0,1)$ are independent, then $$\n  X = \\left\\lfloor 1 + \\frac{\\log(V)}{\\log\\big(1 - (1-\\theta)U\\big)} \\right\\rfloor.\n  $$ has logarithmic distribution with parameter $\\theta$.\n\n::: {.callout-example title=\"Beta distribution\"}\nUsing the distribution theory, we recall the relationship between beta and gamma distributions provides another beta generator.\n\nIf $U \\sim \\mathrm{Gamma}(r,\\lambda)$ and $V \\sim \\mathrm{Gamma}(s,\\lambda)$ are independent, then $$\nX=\\frac{U}{U+V}\n$$ has the $\\mathrm{Beta}(r,s)$ distribution. This transformation determines an algorithm for generating random $\\mathrm{Beta}(a,b)$ variates.\n\n1.  Generate a random $u$ from $\\mathrm{Gamma}(a,1)$.\n\n2.  Generate a random $v$ from $\\mathrm{Gamma}(b,1)$.\n\n3.  Obtain $x=\\dfrac{u}{u+v}$.\n\nThis method is applied below to generate a random $\\mathrm{Beta}(3,2)$ sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(777)\nn <- 1000\na <- 3\nb <- 2\nu <- rgamma(n, shape = a, rate = 1)\nv <- rgamma(n, shape = b, rate = 1)\nx <- u / (u + v)\n```\n:::\n\n\nThe sample data can be compared with the Beta$(3,2)$ distribution using a quantile--quantile (QQ) plot. If the sampled distribution is Beta$(3,2)$, the QQ plot should be nearly linear.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq <- qbeta(ppoints(n), a, b)\nqqplot(q, x, cex = 0.25, xlab = \"Beta(3, 2)\", ylab = \"Sample\")\nabline(0, 1)\n```\n\n::: {.cell-output-display}\n![](03-generating-rv_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n:::\n\n## Sum and Mixture\n\n### Sum/Convolution\n\nLet $X_1, X_2, \\ldots, X_n \\overset{iid}{\\sim}F_X$. Then we may consider the sum of the random variables $S_n:=\\sum_{i=1}^n X_i$, with distribution $F_{S_n}$, which can be referred as *convoluton*. We can simulate $S_n$ by simulating $X_1, X_2, \\ldots, X_n$ and summing them up. There are several common convolutions we have seen so far\n\n1.  Sum of $n$ independent i.i.d. chi-square with degreee of freedom (df) 1 is chi-square with df $n$.\n\n2.  Sum of $n$ independent i.i.d. exponential with rate $\\lambda$ is gamma with shape $n$ and rate $\\lambda$.\n\n3.  Sum of $n$ independent i.i.d. geometric with parametric $p$ is negative binomial with size $n$ and parameter $p$.\n\n::: {.callout-example title=\"Chi square\"}\nIn order to simulate $\\chi^2$ distribution with df k, we can simulate k independent standard normal rvs and sum their squares. Let's simulate $n$ independent $S \\sim \\chi^2(5)$.\n\n1.  Fill an $n \\times k$ matrix with $n k$ realization of the random variables that follow $N(0,1)$.\n\n2.  Square each entry in the matrix (1).\n\n3.  Compute the row sums of the squared normals. Each row sum is one random observation from the $\\chi^2(k)$ distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(777)\nk <- 5\nn <- 1000\nX <- matrix(rnorm(n*k), nrow = n, ncol = k)^2\nX_row <- rowSums(X)\nmean(X_row)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.065355\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(X_row^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 36.40181\n```\n\n\n:::\n:::\n\n:::\n\n### Mixture\n\nA mixture distribution is a probability distribution constructed as a weighted sum of other distributions. If $X$ is a rv with a mixture distribution, then its pdf is given by $$\nF_X(x) = \\sum_{i=1}^k \\alpha_i F_{X_i}(x),\n$$ where $\\alpha_i\\ge 0$ and $\\sum_i \\alpha_i=1$. We can just simply simulate each *component* $X_i$ first, then multiply with their corresponding weights $\\alpha_i$.\n\n::: {.callout-example title=\"Mixture of normal\"}\nSuppose $X$ and $Y$ is a mixture of two normal distributions, where $X\\sim N(\\mu_1,\\sigma_1^2)$ with probability $\\alpha$ and $Y\\sim N(\\mu_2,\\sigma_2^2)$ with probability $1-\\alpha$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 1000\nx1 <- rnorm(n, 0, 1)\nx2 <- rnorm(n, 3, 3)\ns_convolution <- x1 + x2 #the convolution\n\nu <- runif(n)\nk <- as.integer(u > 0.5) #vector of 0’s and 1’s\n\n## pay attention to it\nm_mixture <- k * x1 + (1-k) * x2 #the mixture\ndf <- data.frame(\n  value = c(s_convolution, m_mixture),\n  type = rep(c(\"convolution\", \"mixture\"), each = n)\n)\nggplot(df, aes(x = value, fill = type)) +\n  geom_histogram(alpha = 0.6, position = \"identity\", bins = 40, color = \"black\") +\n  labs(title = \"Histograms of convolution and mixture\", x = \"Value\", y = \"Count\") + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](03-generating-rv_files/figure-html/mixture normal-1.png){width=672}\n:::\n:::\n\n:::\n\n::: {.callout-note title=\"Simulate for mixture\"}\nPay attention how a mixture is simulated. In particular, we first simulate a vector $U\\sim\\operatorname{Unif}(0,1)$, then use it to select from which distribution/component we want to sample. It is **not an (weighted) addition** as in the convolution.\n:::\n\n::: {.callout-note title=\"Difference between convolution and mixture\"}\nConvolution is the sum of two independent rvs, while mixture is a weighted average of two rvs. Note: Mixture is *non-normal!* but the convolution is *normal*.\n:::\n\n## Simulate Multivaraite RVs\n\nThis section presents generators for the multivariate normal distribution, multivariate normal mixtures, the Wishart distribution, and the uniform distribution on the sphere in $\\mathbb{R}^d$.\n\n### Multivariate normal distribution\n\nRecall that, from Definition 1 in Chapter 2, a $d$-dimensional random vector $X$ is a multivariate normal distribution with mean vector $\\mu$ and covariance matrix $\\Sigma$, denoted by $X \\sim N_d(\\mu, \\Sigma)$, if every linear combination of its components has a univariate normal distribution. That is, for every nonzero vector $a \\in \\mathbb{R}^d$, the rv $a^T X$ has a univariate normal distribution.\n\nTo write this in an explicit form, we have, $X = (X_1,\\dots,X_N)$ follow a multivariate normal (MVN) distribution with mean vector $\\mu = (\\mu_1,\\dots,\\mu_N)$ and covariance matrix $\\Sigma = (\\sigma_{ij})$, if its joint density function is given by $$\nf_X(x) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right), \\quad x \\in \\mathbb{R}^p.\n$$\n\nMore explicitly, we can write this as the $$\\mu=(\\mu_1,\\dots,\\mu_p) = \\begin{pmatrix}\\mu_1\\\\\n\\vdots\\\\\n\\mu_p\\end{pmatrix}\\in \\mathbb{R}^p,$$ and $$\\Sigma=(\\Sigma_{ij}) = (\\mathbb{C}ov(X_i,X_j))= \\left[\\begin{array}{cccc}\n\\sigma_{11} & \\sigma_{12} & \\ldots & \\sigma_{1 d} \\\\\n\\sigma_{21} & \\sigma_{22} & \\ldots & \\sigma_{2 d} \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n\\sigma_{d 1} & \\sigma_{d 2} & \\ldots & \\sigma_{d d}\n\\end{array}\\right] \\in \\mathbb{R}^{d\\times d}.\n$$\n\nA random vector $X\\sim N_d(\\mu,\\Sigma)$ can be simulated by the following steps:\n\n1.  Simulate $Z = (Z_1,\\ldots,Z_d)^T$ where $Z_i \\overset{iid}{\\sim}N(0,1)$.\n\n2.  Find a vector $\\mu$ and a matrix $A$ and such that $\\Sigma = AA^T$. The decomposition can be Cholesky decomposition, eigendecomposition, or singular value decomposition, etc.\n\nIn practice, we *do not do this one at a time*, we want to simulate $n$ sample in as few steps as possible. The following procedure is more efficient. Typically, one applies the transformation to a data matrix and transforms the entire sample. Suppose that $Z = (Z_ij ) \\in \\mathbb{R}^{n\\times d}$, where $Z_{ij}\\overset{iid}{\\sim}N(0,1)$. Then the rows of Z are $n$ random observations from the $d$-dimensional standard MVN distribution. The required transformation applied to the data matrix is $$X = ZQ + J \\mu^\\top,$$ where $Q^\\top Q=\\Sigma$, and $J$ is a column vector of $n$ ones. The rows of $X$ are $n$ random observations from the $d$-dimensional MVN distribution with mean vector $\\mu$ and covariance matrix $\\Sigma$.\n\n::: {.callout-algorithm title=\"Simulate Multivariate Normal\"}\nMethod for generating multivariate \\$n \\$ normal samples from the $N_d(\\mu,\\Sigma)$ distribution.\n\nStep 1. Simulate $Z = (Z_{ij}) \\in \\mathbb{R}^{n\\times d}$, where $Z_{ij} \\overset{iid}{\\sim}N(0,1)$.\n\nStep 2. Compute the decomposition $\\Sigma = Q^\\top Q$.\n\nStep 3. Compute $X = ZQ + J\\mu^\\top$, where $J$ is a column vector of $n$ ones.\n\nStep 4. Return $X\\in \\mathbb{R}^{ n\\times d}$, in which each of the $n$ rows of $X$ is a random observation from the $N_d(\\mu,\\Sigma)$ distribution.\n:::\n\n#### A few things to be considered\n\n1.  Computation of $J \\mu^\\top$\n\nRecall that $J$ is a vector of 1's (i.e., $J=(1,\\dots,1)\\in\\mathbb{R}^d$ and $\\mu^\\top$ is the transpose of $\\mu$. So, $J \\mu^\\top$ is a matrix with each row being $\\mu$. In R, we can use `matrix(mu, n, d, byrow = TRUE)` to create this matrix. Also, for any $i$ and $j$, $\\mu_i=\\mu_j=c$, where c is a constant, then we can write it as $c I_{n\\times d}$, where $I_{n\\times d}$ is a matrix of 1's with dimension $n\\times d$.\n\n``` r\nZ <- matrix(rnorm(n*d), nrow = n, ncol = d)\nX <- Z %*% Q + matrix(mu, n, d, byrow = TRUE)\n```\n\n2.  Decomposition of $\\Sigma$\n\nThere are many different ways to decompose $\\Sigma$. Recall that $\\Sigma$ is a symmetric positive definite matrix, so we can use Cholesky decomposition, eigendecomposition, or singular value decomposition (SVD). In R, we can use `chol()`, `eigen()`, or `svd()` functions to compute the decomposition.\n\n-   **Choleski decomposition**: Choleski of a real symmetric positive-definite matrix is $X = Q^\\top Q$, where $Q$ is an upper triangular matrix.\n\n-   **Spectral decomposition**: The square root of the covariance is $Σ^{1/2} = P^{1/2} \\Lambda P^{−1}$, where $\\Lambda$ is the diagonal matrix with the eigenvalues of $\\Sigma$ along the diagonal and $P$ is the matrix whose columns are the eigenvectors of $\\Sigma$ corresponding to the eigenvalues in $\\Lambda$. This method can also be called the eigen-decomposition method. In the eigen-decomposition we have $P^{-1}= P^\\top$ and therefore $Σ^{1/2} = P \\Lambda^{1/2}P^\\top$ . The matrix $Q = Σ^{1/2}$ is a factorization of $\\Sigma$ such that $Q^\\top Q = \\Sigma$.\n\n-   **Singular Value Decomposition**: The singular value decomposition (svd) generalizes the idea of eigenvectors to rectangular matrices. The svd of a matrix $X$ is $X = U DV^\\top$ , where D is a vector containing the singular values of $X$, $U$ is a matrix whose columns contain the left singular vectors of $X$, and $V$ is a matrix whose columns contain the right singular vectors of $X$. The matrix $X$ in this case is the population covariance matrix $\\Sigma$, and $UV\\top = I$. The svd of a symmetric positive definite matrix $\\Sigma$ gives $U = V = P$ and $Σ^{1/2} = U D^{1/2}V^\\top$ . Thus the svd method for this application is equivalent to the spectral decomposition method, but is **less efficient** because the svd method does not take advantage of the fact that the matrix $\\Sigma$ is square symmetric.\n\n::: {.callout-note title=\"Performance Comparison\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(777)\n\n\nn  <- 100     # sample size per call\nd  <- 30      # dimension\nN  <- 1000    # number of distinct Sigmas to cycle through in Scenario A\nreps_A <- 200 # microbenchmark repetitions for Scenario A\nreps_B <- 500 # microbenchmark repetitions for Scenario B\nmu <- numeric(d)\n\n## ====== MVN generators ======\nrmvn_eigen <- function(n, mu, Sigma) {\n  ev <- eigen(Sigma, symmetric = TRUE)\n  A  <- ev$vectors %*% (sqrt(pmax(ev$values, 0)) * t(ev$vectors))\n  Z  <- matrix(rnorm(n * length(mu)), n)\n  sweep(Z %*% A, 2, mu, `+`)\n}\n\nrmvn_svd <- function(n, mu, Sigma) {\n  sv <- svd(Sigma)\n  A  <- sv$u %*% (sqrt(pmax(sv$d, 0)) * t(sv$v))\n  Z  <- matrix(rnorm(n * length(mu)), n)\n  sweep(Z %*% A, 2, mu, `+`)\n}\n\nrmvn_chol <- function(n, mu, Sigma) {\n  R  <- chol(Sigma)\n  Z  <- matrix(rnorm(n * length(mu)), n)\n  sweep(Z %*% R, 2, mu, `+`)\n}\n\n## ====== Utilities ======\nrand_cov <- function(d) {\n  A <- matrix(rnorm(d*d), d, d)\n  S <- crossprod(A) / d\n  diag(S) <- diag(S) + 1e-6\n  S\n}\n\ndrop_outliers <- function(df) {\n  df %>%\n    group_by(expr) %>%\n    mutate(\n      q1 = quantile(time, 0.25),\n      q3 = quantile(time, 0.75),\n      iqr = q3 - q1,\n      lower = q1 - 1.5 * iqr,\n      upper = q3 + 1.5 * iqr\n    ) %>%\n    filter(time >= lower & time <= upper) %>%\n    ungroup()\n}\n\n## Precompute N random SPD matrices (same pool for all methods)\nSigma_list <- replicate(N, rand_cov(d), simplify = FALSE)\n\n## ====== Scenario A: varying Sigma each call (factorize every time) ======\ni <- 0\nnext_Sigma <- function() { i <<- if (i == N) 1 else i + 1; Sigma_list[[i]] }\n\nbench_A <- microbenchmark(\n  rmvn_eigen       = rmvn_eigen(n, mu, next_Sigma()),\n  rmvn_svd         = rmvn_svd(n,   mu, next_Sigma()),\n  rmvn_chol        = rmvn_chol(n,  mu, next_Sigma()),\n  MASS_mvrnorm     = MASS::mvrnorm(n, mu, next_Sigma()),\n  mvtnorm_rmvnorm  = mvtnorm::rmvnorm(n, mean = mu, sigma = next_Sigma()),\n  times = reps_A\n)\n\nsum_A <- as.data.frame(bench_A) %>%\n  group_by(expr) %>%\n  summarize(median_ms = median(time)/1e6,\n            iqr_ms    = IQR(time)/1e6,\n            .groups = \"drop\") %>%\n  arrange(median_ms) %>%\n  mutate(scenario = \"A: varying Σ\")\n\nprint(sum_A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 4\n  expr            median_ms iqr_ms scenario    \n  <fct>               <dbl>  <dbl> <chr>       \n1 rmvn_chol           0.129 0.0113 A: varying Σ\n2 MASS_mvrnorm        0.194 0.0153 A: varying Σ\n3 rmvn_eigen          0.198 0.0171 A: varying Σ\n4 rmvn_svd            0.245 0.0195 A: varying Σ\n5 mvtnorm_rmvnorm     0.266 0.0215 A: varying Σ\n```\n\n\n:::\n\n```{.r .cell-code}\npA <- drop_outliers(as.data.frame(bench_A)) %>%\n  mutate(ms = time/1e6) %>%\n  ggplot(aes(x = reorder(expr, ms, FUN = median), y = ms)) +\n  geom_boxplot() +\n  stat_summary(fun = median, geom = \"point\", shape = 21, size = 2, stroke = 0.6) +\n  coord_flip() +\n  labs(title = \"MVN generators — Scenario A (varying Σ, outliers removed)\",\n       x = NULL, y = \"Time (ms) per call)\") +\n  theme_minimal(base_size = 12)\n\nprint(pA)\n```\n\n::: {.cell-output-display}\n![](03-generating-rv_files/figure-html/MVM-performance-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## ====== Scenario B: fixed Sigma, reuse factorization when possible ======\nSigma0 <- Sigma_list[[1]]\n\nrmvn_chol_fixed <- local({\n  R <- chol(Sigma0)\n  function(n, mu) {\n    Z <- matrix(rnorm(n * length(mu)), n)\n    sweep(Z %*% R, 2, mu, `+`)\n  }\n})\n\nrmvn_eigen_fixed <- local({\n  ev <- eigen(Sigma0, symmetric = TRUE)\n  A  <- ev$vectors %*% (sqrt(pmax(ev$values, 0)) * t(ev$vectors))\n  function(n, mu) {\n    Z <- matrix(rnorm(n * length(mu)), n)\n    sweep(Z %*% A, 2, mu, `+`)\n  }\n})\n\nrmvn_svd_fixed <- local({\n  sv <- svd(Sigma0)\n  A  <- sv$u %*% (sqrt(pmax(sv$d, 0)) * t(sv$v))\n  function(n, mu) {\n    Z <- matrix(rnorm(n * length(mu)), n)\n    sweep(Z %*% A, 2, mu, `+`)\n  }\n})\n\nbench_B <- microbenchmark(\n  rmvn_chol_fixed   = rmvn_chol_fixed(n, mu),\n  rmvn_eigen_fixed  = rmvn_eigen_fixed(n, mu),\n  rmvn_svd_fixed    = rmvn_svd_fixed(n, mu),\n  MASS_mvrnorm      = MASS::mvrnorm(n, mu, Sigma0),                   # factorizes internally\n  mvtnorm_rmvnorm   = mvtnorm::rmvnorm(n, mean = mu, sigma = Sigma0), # Cholesky internally\n  times = reps_B\n)\n\nsum_B <- as.data.frame(bench_B) %>%\n  group_by(expr) %>%\n  summarize(median_ms = median(time)/1e6,\n            iqr_ms    = IQR(time)/1e6,\n            .groups = \"drop\") %>%\n  arrange(median_ms) %>%\n  mutate(scenario = \"B: fixed Σ\")\n\nprint(sum_B)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 4\n  expr             median_ms  iqr_ms scenario  \n  <fct>                <dbl>   <dbl> <chr>     \n1 rmvn_chol_fixed      0.117 0.00887 B: fixed Σ\n2 rmvn_eigen_fixed     0.117 0.00828 B: fixed Σ\n3 rmvn_svd_fixed       0.117 0.00965 B: fixed Σ\n4 MASS_mvrnorm         0.182 0.0130  B: fixed Σ\n5 mvtnorm_rmvnorm      0.249 0.0171  B: fixed Σ\n```\n\n\n:::\n\n```{.r .cell-code}\npB <- drop_outliers(as.data.frame(bench_B)) %>%\n  mutate(ms = time/1e6) %>%\n  ggplot(aes(x = reorder(expr, ms, FUN = median), y = ms)) +\n  geom_boxplot() +\n  stat_summary(fun = median, geom = \"point\", shape = 21, size = 2, stroke = 0.6) +\n  coord_flip() +\n  labs(title = \"MVN generators — Scenario B (fixed Σ; outliers removed)\",\n       x = NULL, y = \"Time (ms) per call)\") +\n  theme_minimal(base_size = 12)\n\nprint(pB)\n```\n\n::: {.cell-output-display}\n![](03-generating-rv_files/figure-html/MVM-performance-2.png){width=672}\n:::\n:::\n\n:::\n\n### Mixture of Multivariate Normal\n\nA mixture of multivariate normal distributions is a convex combination of multivariate normal distributions. That is, the density function of a mixture of $k$ multivariate normal distributions is given by $$\n\\alpha N_p(\\mu_1,\\Sigma_1) + (1-\\alpha) N_p(\\mu_2,\\Sigma_2), \\quad 0 < \\alpha < 1.\n$$ Similar to the univariate case, different choice of $\\alpha$ will lead to different degree of departure from the normal.\n\n::: {.callout-algorithm title=\"Mixture of multivariate normal\"}\nGiven a $\\alpha$, to simulatecallout-note a random sample from $\\alpha N_d(\\mu_1, \\Sigma_1) + (1 − p) N_d(\\mu_2, \\Sigma_2)$\n\nThere are two ways.\n\nWay 1:\n\n1.  Generate $U \\sim unif(0,1) \\in \\mathbb{R}^n$.\n\n2.  (Component-wise) If $U_i \\le \\alpha$, generate X from $X_i\\sim N_d(\\mu_1, \\Sigma_1)$, o.w. generate $X_i from \\sim N_d(\\mu_2, \\Sigma_2)$.\n\nEquivalent:\n\nWay 2:\n\n1.  Generate $N \\sim ∼ \\operatorname{Bern}(\\alpha)$.\n\n2.  (Component-wise) If $N_i = 1$ generate $X$ from $X_i \\sim N_d(\\mu_1, \\Sigma_1)$, o.w. generate $X$ from $N_d(\\mu_2, \\Sigma_2)$.\n:::\n\n::: {.callout-example title=\"Mixture of multivariate normal\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(777)\nloc.mix.0 <- function(n, alpha, mu1, mu2, Sigma) {\n  # generate sample from BVN location mixture\n  X <- matrix(0, n, 2)\n  for (i in 1:n) {\n    k <- rbinom(1, size = 1, prob = alpha)\n    if (k) {\n      X[i, ] <- mvrnorm(1, mu = mu1, Sigma)\n    } else {\n      X[i, ] <- mvrnorm(1, mu = mu2, Sigma)\n    }\n  }\n  return(X)\n}\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloc.mix <- function(n, alpha, mu1, mu2, Sigma) {\n  # generate sample from BVN location mixture\n  n1 <- rbinom(1, size = n, prob = alpha)\n  n2 <- n - n1\n  x1 <- mvrnorm(n1, mu = mu1, Sigma)\n  x2 <- mvrnorm(n2, mu = mu2, Sigma)\n  X <- rbind(x1, x2) # combine the samples\n  return(X[sample(1:n), ]) # mix them\n}\n```\n:::\n\n:::\n\n::: {.callout-note title=\"Mixture of multivariate normal\"}\nOne interesting example is when $\\alpha = 1 − 1/2 (1 − \\sqrt{3}/3 )\\approx .\n0.7887$, provides an example of a skewed distribution with normal kurtosis (level of the peak).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(777)\n\n\nn <- 1000\ncomp1 <- MASS::mvrnorm(n, mu = c(0, 0), Sigma = diag(2))\ncomp2 <- MASS::mvrnorm(n, mu = c(3, 2), Sigma = matrix(c(1, 0.5, 0.5, 1), 2))\n\n# Proper mixture membership per observation\nalpha <- 1 - 1/2*(1 - sqrt(3)/3 )   # P(Comp1)\nmemb <- rbinom(n, 1, alpha)         # 1 = Comp1, 0 = Comp2\n\n# Build the mixture matrix row-wise (avoid ifelse on matrices)\nmix <- matrix(NA_real_, nrow = n, ncol = 2)\nid1 <- memb == 1\nid2 <- !id1\nmix[id1, ] <- comp1[sample(n, sum(id1), replace = TRUE), ]\nmix[id2, ] <- comp2[sample(n, sum(id2), replace = TRUE), ]\n\n# Tidy data frames with labels\ndf1 <- data.frame(x = comp1[,1], y = comp1[,2], group = \"Comp1\")\ndf2 <- data.frame(x = comp2[,1], y = comp2[,2], group = \"Comp2\")\ndfm <- data.frame(x = mix[,1],  y = mix[,2],  group = \"Mixture\")\ndf_all <- rbind(df1, df2, dfm)\n\n# Option A: single panel, colored by group\np1 <- ggplot(df_all, aes(x, y, color = group)) +\n  geom_point(alpha = 0.35, size = 1) +\n  stat_ellipse(level = 0.95, linewidth = 0.7) +\n  coord_equal() +\n  theme_minimal(base_size = 12) +\n  labs(title = \"Two Gaussian Components and Their Mixture\")\np1 \n```\n\n::: {.cell-output-display}\n![](03-generating-rv_files/figure-html/MVN-efficient-1.png){width=672}\n:::\n:::\n\n:::\n\n::: {.callout-note title=\"Higher dimensional case\"}\nIt is difficult to visualize data in $R^d$, for $d\\ge 4$, so we display only the histograms of the marginal distributions. All of the one-dimensional marginal distributions are univariate normal location mixtures.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(777)\nlibrary(MASS)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# ---- generate x exactly like your call ----\nx <- loc.mix(1000, .5, rep(0, 4), 2:5, Sigma = diag(4))\ncolnames(x) <- paste0(\"X\", 1:4)\n\n# ---- ggplot histograms ----\nr <- range(x) * 1.2  # global x-limits like your base R code\ndf_long <- as.data.frame(x) |>\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Value\")\n\nggplot(df_long, aes(x = Value)) +\n  geom_histogram(aes(y = ..density..),\n                 breaks = seq(-5, 10, 0.5),\n                 fill = \"red\", color = \"white\") +\n  facet_wrap(~ Variable, ncol = 2) +\n  coord_cartesian(xlim = r, ylim = c(0, 0.3)) +\n  theme_minimal(base_size = 12) +\n  labs(\n    title = \"Histograms of a 4D Gaussian Location Mixture\",\n    subtitle = expression(paste(\"p = 0.5,  \", mu[1], \" = (0,0,0,0),  \",\n                                mu[2], \" = (2,3,4,5),  \",\n                                Sigma, \" = I\")),\n    x = \"Value\", y = \"Density\"\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](03-generating-rv_files/figure-html/MVN-HD-1.png){width=672}\n:::\n:::\n\n:::\n\n### Uniform Distribution on a Sphere\n\nThe uniform distribution on the surface of a $d$-sphere in $\\mathbb{R}^d$ is the distribution that assigns equal probability to equal areas on the surface of the sphere.\n\nThe d-sphere is the set of all points $x \\in R^d$ such that $\\| x \\| = \\sqrt{x^\\top x) = 1$. Random vectors uniformly distributed on the $d$-sphere have equally likely directions. A method of generating this distribution uses a property of the\n\nmultivariate normal distribution (see \\[97, 160\\]). If X1, . . . , Xd are iid N (0, 1), then U = (U1, . . . , Ud) is uniformly distributed on the unit sphere in Rd, where\n\n$$\nU_j=\\frac{X_j}{\\left(X_1^2+\\cdots+X_d^2\\right)^{1 / 2}}, \\quad j=1, \\ldots, d \\tag{*}\n$$\n\nAlgorithm to generate uniform variates on the $d$-Sphere\n\n1.  For each variate $u_i,~ i = 1,\\dots , n$ repeat\n\n<!-- -->\n\n(a) Generate a random sample $x_{i_1}, \\dots , x_{i_d}$ from $N (0, 1)$.\n\n(b) Compute the Euclidean norm $\\|x_i\\| = (x^2_{i1} + \\cdots + x^2_{id})^{1/2}$.\n\n(c) Set $u_{ij} = x_{ij} / \\|xi\\|, j = 1, \\dots , d$.\n\n(d) Deliver $u_i = (u_{i1}, \\dots , u_{id})$.\n\nTo implement these steps efficiently in **R** for a sample size n,\n\n1.  Generate nd univariate normals in $n \\times d$ matrix $M$. The $i$th row of M corresponds to the ith random vector $u_i$.\n\n2.  Compute the denominator of (\\*) for each row, storing the $n$ norms in vector $L$.\n\n3.  Divide each number M\\[i,j\\] by the norm L\\[i\\], to get the matrix U, where $U[i,] = u_i = (u_{i1}, \\dots, u_{id})$.\n\n4.  Deliver matrix $U$ containing $n$ random observations in rows.\n\n::: {.callout-example title=\"Uniform distribution on a sphere\"}\nThis example provides a function to generate random variates uniformly distributed on the unit $d$-sphere.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrunif.sphere <- function(n, d) {\n  # return a random sample uniformly distributed\n  # on the unit sphere in R ^d\n  M <- matrix(rnorm(n * d), nrow = n, ncol = d)\n  L <- apply(M,\n    MARGIN = 1,\n    FUN = function(x) {\n      sqrt(sum(x * x))\n    }\n  )\n  D <- diag(1 / L)\n  U <- D %*% M\n  U\n}\n\n#generate a sample in d=2 and plot\nX_d2 <- runif.sphere(200, 2)\ndf <- data.frame(x1 = X_d2[,1], x2 = X_d2[,2])\n\nggplot(df, aes(x = x1, y = x2)) +\n  geom_point(color = \"steelblue\", size = 2, alpha = 0.7) +\n  coord_equal() +                     \n  labs(\n    x = expression(x[1]),\n    y = expression(x[2]),\n    title = \"Uniform Random Points on the Unit Circle\"\n  ) +\n  ggforce::geom_circle(aes(x0 = 0, y0 = 0, r = 1), inherit.aes = FALSE,\n              color = \"black\", linetype = \"dashed\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in ggforce::geom_circle(aes(x0 = 0, y0 = 0, r = 1), inherit.aes = FALSE, : All aesthetics have length 1, but the data has 200 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](03-generating-rv_files/figure-html/sphere-1.png){width=672}\n:::\n:::\n\n:::\n\n------------------------------------------------------------------------\n\nReference used:\n\n-   Rizzo, M.L. (2007) [*Statistical Computing with R*](https://a-roshani.ir/files/SC/%5B4%5D%20%5BMaria%20L.%20Rizzo%5D%5B2019%5D%20Statistical%20Computing%20%20with%20R,%20Second%20Edition.pdf). CRC Press, Roca Baton.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}