{
  "hash": "744a98693eb8012b764d03ab928767cd",
  "result": {
    "engine": "knitr",
    "markdown": "# Generating Random Variables\n\n\\newcommand{\\unif}{\\operatorname{Unif}}\n\\newcommand{\\geom}{\\operatorname{Geom}}\n\\newcommand{\\beta}{\\operatorname{Beta}}\n\n\n\nOne of the fundamental tools required in computational statistics is the\nability to *simulate random variables* (rvs) from specified probability distributions. \n\n## Overview\n\nIn the simplest case, to simulate drawing an observation at random from\na finite population, a method of generating rvs from the discrete uniform distribution is required. Therefore, a suitable generator of\nuniform pseudo-random numbers is essential. \n\nMethods for generating random\nvariates from other probability distributions all depend on the uniform random number generator (RNG).\n\nIn the Appendices, we have seen that how to use the built-in R functions to generate RVs from some common distributions, such as `runif()`, `rnorm()`, `rbinom()`, etc. In this Section, we will go over some of the common methods to generate RVs from a costume distributions.\n\nExample Theorem\n\n::: {.callout-example title=\"Sampling from a finite population\"}\nIf we already have a finite population of size $N$ with values $x_1, x_2, \\ldots, x_N$ in hand, we can sample from this population *with* and *without* replacement.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(777)\nsample(c(0,1), size = 10, replace = TRUE)  # with replacement\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1 0 1 1 1 1 0 1 1 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Lottery ticket\nsample(1:999999, size = 5, replace = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 567561 203418 450070 287692 435311\n```\n\n\n:::\n\n```{.r .cell-code}\nsample(toupper(letters))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"H\" \"N\" \"J\" \"Y\" \"B\" \"U\" \"F\" \"P\" \"Z\" \"V\" \"W\" \"I\" \"L\" \"A\" \"G\" \"Q\" \"X\" \"D\" \"M\"\n[20] \"R\" \"C\" \"T\" \"O\" \"K\" \"E\" \"S\"\n```\n\n\n:::\n:::\n\n:::\n\n| Distribution       | cdf     | Generator | Parameters           |\n|--------------------|---------|-----------|----------------------|\n| beta               | pbeta   | rbeta     | shape1, shape2       |\n| binomial           | pbinom  | rbinom    | size, prob           |\n| chi-squared        | pchisq  | rchisq    | df                   |\n| exponential        | pexp    | rexp      | rate                 |\n| F                  | pf      | rf        | df1, df2             |\n| gamma              | pgamma  | rgamma    | shape, rate or scale |\n| geometric          | pgeom   | rgeom     | prob                 |\n| lognormal          | plnorm  | rlnorm    | meanlog, sdlog       |\n| negative binomial  | pnbinom | rnbinom   | size, prob           |\n| normal             | pnorm   | rnorm     | mean, sd             |\n| Poisson            | ppois   | rpois     | lambda               |\n| Student's t        | pt      | rt        | df                   |\n| uniform            | punif   | runif     | min, max             |\n: Common probability distributions and their corresponding R functions for cumulative distribution function (CDF) and random number generation (borrowed from Table 3.1 in reference [2]). {#tbl-my-table}\n\n## Inverse Transformation Method\n\nThe first method to simulate rvs is the *inverse transformation method* (ITM).\n\n::: {.callout-theorem title=\"Probability Integral Transformation\"}\nIf $X\\sim F_x(X)$ is a continuous rv, then the rv $U = F_x(X) \\sim \\unif(0,1)$.\n:::\n\nITM of generating rvs applies the\nprobability integral transformation. Define the inverse transformation\n$$ F^{−1}_X(u) = \\inf\\{x : FX (x) = u\\},\\quad  0 < u < 1.$$\nThen, if $U \\sim \\unif(0,1)$, the rv $X = F^{−1}_X(U)$ has the distribution $F_X$. This can be shown as, for all $x \\in R$\n\\begin{align}\nP\\left(F_X^{-1}(U) \\leq x\\right) & =P\\left(\\inf \\left\\{t: F_X(t)=U\\right\\} \\leq x\\right) \\\\\n& =P\\left(U \\leq F_X(x)\\right) \\\\\n& =F_U\\left(F_X(x)\\right)=F_X(x),\n\\end{align}\n\nHence, $F_X^{-1}(U)$ and $X$ have the same distribution. So, in order to generate rv $X$, we can simulate $U\\sim \\unif(0,1)$ first, then apply the inverse $F_X^{-1}(u)$. \n\n::: {.callout-note title=\"Procedure with inverse transformation method\"}\n\nGiven a distribution function $F_X(\\cdot)$, we can simulate/generate a rv $X$ using the ITM in **three** steps:\n\n1. Derive the inverse function $F_X^{-1}(u)$. \n\n2. Write a (**R**) command or function to compute $F_X^{-1}(u)$.\n\n3. For each random variate required:\n    i) Generate a random $u\\sim \\unif(0,1)$.\n    ii) Obtain $x = F_X^{-1}(u)$.\n:::\n\n\n### Continuous case\n\nWhen the distribution function $F_X(\\cdot)$ is continuous, the ITM is straightforward to implement.\n\n::: {.callout-example}\nSuppose we want to use the ITM to simulate $N=1000$ rvs from the density $f_X(x)=3x^2,\\quad x\\in(0,1)$.\n\n1. The cdf is $F_X(x)=x^3$, so the inverse function is $F_X^{-1}(u)=u^{1/3}$.\n\n2. Simulate $N=1000$ rvs from $u\\sum \\unif(0,1)$ and apply the inverse function to obtain the 1000 $x$ values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(777)\nN <- 1000\nuVec <- runif(N)\nxVec <- uVec^(1/3)\n\ndf <- data.frame(x = xVec)\n\n# Density histogram with theoretical density overlay\nggplot(df, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30,\n                 fill = \"lightblue\", color = \"black\") +\n  stat_function(fun = function(x) 3*x^2,\n                color = \"red\", size = 1) +\n  labs(title = expression(f(x) == 3*x^2),\n       y = \"Density\", x = \"x\") + theme_minimal()\n```\n\n::: {.cell-output-display}\n![](03-generating-rv_files/figure-html/example2-1.png){width=672}\n:::\n:::\n\n\n:::\n\n::: {.callout-example title = \"Exponential\"}\n\nSuppose $X\\sim \\exp(\\lambda)$ where $\\lambda$ is the rate parameter. Then $F_X(x) = 1 - e^{-\\lambda x}$, so the inverse function is $F_X^{-1}(u) = -\\frac{1}{\\lambda}\\log(1-u)$. The other fact, is the $U$ and $1-U$ have the same distribution, so we can use either of them, i.e., $x= -\\frac{1}{\\lambda}\\log(u)$ or $x= -\\frac{1}{\\lambda}\\log(1-u)$. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(777)\nN <- 1000\nlambda <- 0.7\nuVec <- runif(N)\nxVec_1 <- - (1/lambda) * log(uVec)\nxVec_2 <- - (1/lambda) * log(1-uVec)\n\n# Put data into long format for ggplot\ndf <- data.frame(\n  value = c(xVec_1, xVec_2),\n  method = rep(c(\"log(U)\", \"log(1-U)\"), each = N)\n)\n\n# Theoretical density function\nexp_density <- function(x) lambda * exp(-lambda * x)\n\n# Plot\nggplot(df, aes(x = value, fill = method)) +\n  geom_histogram(aes(y = ..density..), bins = 40,\n                 position = \"identity\", alpha = 0.4, color = \"black\") +\n  stat_function(fun = exp_density, color = \"red\", size = 1) +\n  labs(title = bquote(\"Exponential(\" ~ lambda == .(lambda) ~ \")\"),\n       x = \"x\", y = \"Density\") \n```\n\n::: {.cell-output-display}\n![](03-generating-rv_files/figure-html/example-exponential-1.png){width=672}\n:::\n:::\n\n\n:::\n\n### Discrete case\n\nAlthough it is slightly more complicated than the continuous case, the ITM can also be applied to *discrete distributions*. Why?\n\nFirst, in the discrete case, the cdf $F_X(x)$ is **NOT continuous**, instead, a step function, so the inverse function $F_X^{-1}(u)$ is not unique.\n\nHere, if we order the random variable \n$$\\cdots < x_{(i-1)} < x_{(i)} < x_{(i+1)}< cdots,$$\nthen the inverse transformation is $F_X^{-1}(u)=x_i$, where $F_X(x_{(i-1)}) < u \\leq F_X(x_{(i)})$.\n\nThen the procedure is:\n\n::: {.callout-note title=\"Procedure with ITM for discrete case\"}\n\n1. Derive the cdf $F_X(x)$ and tabulate the values of $x_i$ and $F_X(x_i)$.\n\n2. Write a (**R**) command or function to compute $F_X^{-1}(u)$.\n\n3. For each random variate required:\n    i) Generate a random $u\\sim \\unif(0,1)$.\n    ii) Find $x_i$ such that $F_X(x_{(i-1)}) < u \\leq F_X(x_{(i)})$ and set $x = x_i$.\n\n:::\n\n::: {.callout-example title=\"Two point distribution\"}\n\nIn this example, $F_X(0) = f_X(0) = 1 - p$ and $F_X(1) = 1$. \n\n\nThus,\n$$\nF_X^{-1}(u) = \n\\begin{cases}\n1, & u > 0.6,\n0, & u \\leq 0.6.\n\\end{cases}\n$$\n\nThe generator should therefore deliver the numerical value of the logical expression $u > 0.6$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(777)\nn <- 1000\np <- 0.4\nu <- runif(n)\nx <- as.integer(u > 0.6)  # (u > 0.6) is a logical vector\n\n(m_x <- mean(x));  (v_x <- var(x))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.381\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2360751\n```\n\n\n:::\n:::\n\n\n\nCompare the sample statistics with the theoretical moments. \nThe sample mean of a generated sample should be approximately $p = 0.4$ \nand the sample variance should be approximately $p(1 - p) = 0.24$, versus our simulated values 0.381 and 0.2360751.\n:::\n\n::: {.callout-example title=\"Geometric distribution\"}\nIn this example, we will use ITM to simulate $X\\sim \\geom(1/4)$. \n\nLet $q:=1-p$. The pmf is $f(x) = p q^x$, $x = 0,1,2,\\ldots$.\nAt the points of discontinuity $x = 0,1,2,\\ldots$, the cdf is\n$$\nF(x) = 1 - q^{x+1}.\n$$\nFor each sample element we need to generate a $u\\sim \\unif(0,1)$ and solve\n$$\n1 - q^x < u \\leq 1 - q^{x+1}.\n$$\n\nWhich is equivalent to \n$x < \\frac{\\log(1 - u)}{\\log(q)} \\leq x+1.$\nThe solution is\n$$\nx + 1 = \\left\\lceil \\frac{\\log(1 - u)}{\\log(q)} \\right\\rceil,\n$$\nwhere $\\lceil \\cdot \\rceil$ denotes the ceiling function (and $\\lfloor \\cdot \\rfloor$ is the floor function). Hence, we have,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(999)\nn <- 1000\np <- 0.25\nu <- runif(n)\nk1 <- ceiling(log(1-u) / log(1-p)) - 1\n```\n:::\n\n\n\nNote again that $U$ and $1 - U$ have the same distribution. Also, the probability that $\\log(1 - u)/\\log(1 - p)$ equals an integer is zero. Thus, we can simplify it to\n\n::: {.cell}\n\n```{.r .cell-code}\nk2 <- floor(log(u) / log(1-p))\ndf <- data.frame(\n  value = c(k1, k2),\n  group = rep(c(\"k1\", \"k2\"), each = length(k1))\n)\n\n# Plot both histograms side by side\nggplot(df, aes(x = value, fill = group)) +\n  geom_histogram(alpha = 0.6, position = \"identity\", bins = 40, color = \"black\") +\n  labs(title = \"Histograms of k1 and k2\", x = \"Value\", y = \"Count\") + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](03-generating-rv_files/figure-html/geom2-1.png){width=672}\n:::\n:::\n\n\nThe geometric distribution was particularly easy to simulate by the inverse transform method because it was easy to solve the inequality\n\\[\nF(x-1) < u \\leq F(x)\n\\]\nrather than compare each $u$ to all the possible values $F(x)$.\n\\medskip\n\nThe same method applied to the Poisson distribution is more complicated because we do not have an explicit formula for the value of $x$ such that\n$$\nF(x-1) < u \\leq F(x).\n$$\n\nThe R function \\texttt{rpois} generates random Poisson samples. The basic method to generate a Poisson($\\lambda$) variate is to generate and store the cdf via the recursive formula\n$$\nf(x+1) = \\frac{\\lambda f(x)}{x+1}, \n\\qquad \nF(x+1) = F(x) + f(x+1).\n$$\n:::\n\n## Acceptance-Rejection Method\n\nIn the previous section, we have seen that the ITM is straightforward to implement when the inverse cdf is available in closed form. However, for many distributions, the *inverse cdf is not available in closed form or is difficult to compute*. In those cases, we need to have other strategies!\n\n\nThe *acceptance-rejection method* (ARM) is a general method for generating rvs from a distribution with pdf $f_X(x)$, when the inverse cdf is not available in closed form or is difficult to compute.\n\n::: {.callout-definition title=\"The Acceptance--Rejection Method\"}\n\nSuppose $X$ and $Y$ are rvs with pdfs/pmds $f_X(x)$ and $g_Y(y)$, respectively. Further we suppose there is a constant $k$ such that \n$$\n\\frac{f_X(t)}{g_Y(t)} \\leq k,\n$$\nfor all $t$ such that $f_X(t) > 0$.\n\nThen we can simulate $X$ using the following procedure:\n\n1. Find a rv $Y$ with density $g_Y(\\cdot)$ satisfying \n$f_X(t)/g_Y(t) \\le k,$ for all $t$ such that $f(t) > 0$.\n\n2. For each rv, required:\n    (i) Generate a random $y$ from the distribution with density $g_Y$.\n    (ii) Generate a random $u\\sim \\unif(0,1)$.\n    (iii) If $u < f_X(y)/(k g_Y(y))$, accept $y$ and set $x = y$; o.w. reject $y$ and jump back to (i)\n:::\n\nWhy it work?Note that in Step 2c,\n$$\nP(\\text{accept} \\mid Y) \n= P\\!\\left(U < \\frac{f(Y)}{k g(Y)} \\,\\Big|\\, Y\\right) \n= \\frac{f_X(Y)}{k g_X(Y)}.\n$$\n\nThe total probability of acceptance for any iteration is therefore\n$$\n\\sum_y P(\\text{accept} \\mid y) P(Y = y) \n= \\sum_y \\frac{f(y)}{k g(y)} g(y) \n= \\frac{1}{k},\n$$\nand the number of iterations until acceptance has the geometric distribution\nwith mean $k$. That means, in order to sample $X$, in average, we need $k$ iterations.\n\nNote: The choice of $Y$ and $k$ is crucial for the efficiency of the ARM. A poor choice can lead to a large $k$, resulting in many rejections and inefficiency. We want $Y$ to be easy to simulate, and $k$ to be as small as possible.\n\nDoes this have anything to do with $X$?\n\nTo see that the accepted sample has the same distribution as $X$, apply Bayes' Theorem. \nIn the discrete case, for each $\\ell$ such that $f(\\ell) > 0$,\n$$\nP(\\ell \\mid \\text{accepted}) \n= \\frac{P(\\text{accepted} \\mid \\ell) g(\\ell)}{P(\\text{accepted})} \n= \\frac{\\big[f(\\ell)/(k g(\\ell))\\big] g(\\ell)}{1/k} \n= f(\\ell).\n$$\n\n::: {.callout-example title=\"ARM for beta\" }\nThis example illustrates the acceptance--rejection method for the beta distribution.\n\nQ:  On average, how many random numbers must be simulated to generate $N=1000$ samples from the $\\beta$(\\alpha=2,\\beta=2)$ distribution by ARM? \n\nA: Depends on the upper bound $k$ of $f_X(t)/_Yg(t)$, which depends on the choice of the function $g_Y(\\cdot)$.\n\nRecall that the $\\beta(2,2)$ density is \n$$\nf(t) = 6t(1-t), \\quad 0 < t < 1.\n$$ \nLet $g(\\cdot)$ be the Uniform(0,1) density. Then\n$$\n\\frac{f(t)}{g(t)} = \\frac{6t(1-t)}{(1)} = 6t(1-t) \\leq k \\quad \\text{for all } 0 < t < 1.\n$$\nIt is easy to see that $k = 6$. A random $x$ from $g(x)$ is accepted if\n$$\n\\frac{f(x)}{kg(x)} = \\frac{6x(1-x)}{6(1)} = x(1-x) > u.\n$$\n\nOn average, $kN = 6\\cdot 1000 =6000$ iterations (12000 random numbers as we need $X$ and $Y$) will be required for $N=1000$. In the following simulation, the counter $\\operatorname{iter}$ for iterations is not necessary, but included to record how many iterations were actually needed to generate the 1000 beta rvs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(7777)\nN <- 1000\nell_accept <- 0       # counter for accepted\niter <- 0       # iterations\ny <- rep(0, N)\n\nwhile (ell_accept < N) {\n  u <- runif(1)\n  iter <- iter + 1\n  x <- runif(1)   # random variate from g\n  if (x * (1-x) > u) {\n    # we accept x\n    ell_accept <- ell_accept + 1\n    y[ell_accept] <- x\n  }\n}\n\niter\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5972\n```\n\n\n:::\n:::\n\n\nIn this simulation, 5972 iterations ( 1.1944\\times 10^{4} random numbers) were required to generate the 1000 beta samples.\n:::\n\n## Using known probability distribution theory\n\nMany types of transformations other than the probability inverse transformation can be applied to simulate random variables. Some examples are\n\n1). If $Z \\sim N(0,1)$, then $V = Z^2 \\sim \\chi^2(1)$.\n\n2). If $Z_1,\\ldots,Z_n \\sim N(0,1)$ are independent, then \n  $$\n  U = \\sum_{i=1}^n Z_i^2 \\sim \\chi^2(n).\n  $$\n  \n3). If $U \\sim \\chi^2(m)$ and $V \\sim \\chi^2(n)$ are independent, then \n  $$\n  F = \\frac{U/m}{V/n}\n  $$\n  has the $F$ distribution with $(m,n)$ degrees of freedom.\n  \n4). If $Z \\sim N(0,1)$ and $V \\sim \\chi^2(n)$ are independent, then \n  $$\n  T = \\frac{Z}{\\sqrt{V/n}}\n  $$\n  has the Student $t$ distribution with $n$ degrees of freedom.\n  \n5). If $U,V \\sim \\text{Unif}(0,1)$ are independent, then\n  $$\n  Z_1 = \\sqrt{-2 \\log U}\\, \\cos(2\\pi V), \n  \\qquad\n  Z_2 = \\sqrt{-2 \\log U}\\, \\sin(2\\pi V)\n  $$\n  are independent standard normal variables.\n  \n6). If $U \\sim \\text{Gamma}(r,\\lambda)$ and $V \\sim \\text{Gamma}(s,\\lambda)$ are independent, then \n  $$\n  X = \\frac{U}{U+V}\n  $$\n  has the $\\text{Beta}(r,s)$ distribution.\n  \n7). If $U,V \\sim \\text{Unif}(0,1)$ are independent, then\n  $$\n  X = \\left\\lfloor 1 + \\frac{\\log(V)}{\\log\\big(1 - (1-\\theta)U\\big)} \\right\\rfloor.\n  $$\n  has logarithmic distribution with parameter $\\theta$. \n  \n::: {.callout-example title=\"Beta distribution\"}\nUsing the distribution theory, we recall the relationship between beta and gamma distributions provides another beta generator.\n\nIf $U \\sim \\mathrm{Gamma}(r,\\lambda)$ and $V \\sim \\mathrm{Gamma}(s,\\lambda)$ are independent, then\n$$\nX=\\frac{U}{U+V}\n$$\nhas the $\\mathrm{Beta}(r,s)$ distribution. This transformation determines an algorithm for generating random $\\mathrm{Beta}(a,b)$ variates.\n\n1. Generate a random $u$ from $\\mathrm{Gamma}(a,1)$.\n\n2. Generate a random $v$ from $\\mathrm{Gamma}(b,1)$.\n\n3. Obtain $x=\\dfrac{u}{u+v}$.\n\nThis method is applied below to generate a random $\\mathrm{Beta}(3,2)$ sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(777)\nn <- 1000\na <- 3\nb <- 2\nu <- rgamma(n, shape = a, rate = 1)\nv <- rgamma(n, shape = b, rate = 1)\nx <- u / (u + v)\n```\n:::\n\n\nThe sample data can be compared with the Beta$(3,2)$ distribution using a quantile--quantile (QQ) plot. If the sampled distribution is Beta$(3,2)$, the QQ plot should be nearly linear.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq <- qbeta(ppoints(n), a, b)\nqqplot(q, x, cex = 0.25, xlab = \"Beta(3, 2)\", ylab = \"Sample\")\nabline(0, 1)\n```\n\n::: {.cell-output-display}\n![](03-generating-rv_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n:::\n\n------------------------------------------------------------------------\n\nReference used:\n\n-  Reference book [2]\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}