{
  "hash": "1520b7e243c09b44f85fa0dd024bd3b2",
  "result": {
    "engine": "knitr",
    "markdown": "# Resampling, Jackknife and Bootstrap\n\n\\newcommand{\\E}{\\mathbb E}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\var}{\\mathbb{V}ar}\n\\newcommand{\\bx}{\\mathbf{x}}\n\\newcommand{\\bX}{\\mathbf{X}}\n\\newcommand{\\cov}{\\mathbb{C}ov}\n\\newcommand{\\mse}{\\mathrm{MSE}}\n\\newcommand{\\corr}{\\mathbb{C}orr}\n\\newcommand{\\unif}{\\operatorname{Unif}}\n\\newcommand{\\geom}{\\operatorname{Geom}}\n\\newcommand{\\bet}{\\operatorname{Beta}}\n\\newcommand{\\bern}{\\operatorname{Bern}}\n\\newcommand{\\iid}{\\overset{iid}{\\sim}}\n\\newcommand{\\ef}{\\operatorname{Eff}}\n\\newcommand{\\htt}{\\hat \\theta}\n\\newcommand{\\b}{\\mathbb b}\n\n\n\n## Introduction\n\nThis chapter continues from the previous chapter, introducing more resampling methods; jackknife and bootstrap techniques.\n\n## Disclaimer\n\nMost of the contents of this chapter are mainly from the reference book, Chapter 8 of Rizzo, M.L. (2007). *Statistical Computing with R*. CRC Press, Roca Baton.\n\n## Resampling methods\n\n*Resampling methods* is a way to use the observed data to generate hypothetical samples. It treats an observed sample as a *finite population*, and random samples are generated/resampled from it to estimate population characteristics and make inferences about the sampled population. It is useful when:\n\n-   Do not know the underlying distribution of a population\n\n-   The formula may be difficult to be calculated.\n\nSome commonly used resampling methods include:\n\n-   **Cross-validation**: Cross-validation is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.\n\n-   **Bootstrap**: Bootstrap methods are often used when the distribution of the target population is not specified; the sample is the only information available.\n\n-   **Jackknife**: The jackknife is a resampling technique used to estimate the bias and variance of a statistic. It is like a leave-one-out (LOO) cross-validation.\n\n-   **Permutation tests**: Permutation tests are a type of non-parametric statistical test that involves rearranging the data points to test a hypothesis. They are used to determine whether the observed effect is statistically significant by comparing it to the distribution of effects obtained through random permutations of the data.\n\n## Estimators\n\n### Bias-Variance Tradeoff of an estimator\n\nIn statistics, the bias-variance tradeoff is the property of a model that the variance of the parameter estimated across samples can be reduced by increasing the bias in the estimated parameters. If we look at the Mean square error (MSE) of an estimator $\\hat \\theta$ for a parameter $\\theta$:\n\n::: {.callout-definition title=\"Mean Square Errors\"}\nSuppose we have a parameter $\\theta$ and an estimator $\\hat \\theta$. The mean square error (MSE) of the estimator is defined as $$\\mathrm{MSE}_{\\theta}(\\hat \\theta)=\\mathbb E[(\\hat \\theta-\\theta )^2 ]= \\mathbb{V}ar(\\hat \\theta) + [\\mathbb b(\\hat \\theta)]^2,$$ where $\\mathbb b$ is the bias of the estimator.\n:::\n\n::: {.callout-note title=\"Derivation\"}\nWe start with the definition of the mean squared error (MSE):\n\n$$\n\\mathrm{MSE}_\\theta(\\hat \\theta)\n= \\mathbb E\\big[(\\hat \\theta- \\theta)^2\\big].\n$$\n\nAdd and subtract $\\mathbb E[\\hat \\theta]$ inside the square: $$\n\\mathbb E\\big[(\\hat \\theta- \\theta)^2\\big]\n= \\mathbb E\\big[(\\hat \\theta- \\mathbb E[\\hat \\theta] + \\mathbb E[\\hat \\theta] - \\theta)^2\\big].\n$$\n\nExpand the square: $$\n\\mathbb E\\big[(\\hat \\theta- \\mathbb E[\\hat \\theta])^2\\big]\n+ 2\\,\\mathbb E\\big[(\\hat \\theta- \\mathbb E[\\hat \\theta])(\\mathbb E[\\hat \\theta] - \\theta)\\big]\n+ (\\mathbb E[\\hat \\theta] - \\theta)^2.\n$$\n\nThe middle term vanishes because $$\n\\mathbb E\\big[\\hat \\theta- \\mathbb E[\\hat \\theta]\\big] = 0.\n$$\n\nHence, $$\n\\mathbb E\\big[(\\hat \\theta- \\theta)^2\\big]\n= \\mathbb E\\big[(\\hat \\theta- \\mathbb E[\\hat \\theta])^2\\big]\n+ (\\mathbb E[\\hat \\theta] - \\theta)^2.\n$$\n\nRecognizing that $$\n\\mathbb{V}ar(\\hat \\theta) = \\mathbb E\\big[(\\hat \\theta- \\mathbb E[\\hat \\theta])^2\\big]\n\\quad \\text{and} \\quad\nb(\\hat \\theta) = \\mathbb E[\\hat \\theta] - \\theta,\n$$ we obtain the biasâ€“variance decomposition: $$\n\\boxed{\n\\mathbb E\\big[(\\hat \\theta- \\theta)^2\\big]\n= \\mathbb{V}ar(\\hat \\theta) + \\mathbb b^2(\\hat \\theta)^2.}\n$$\n:::\n\n**Questions**: Why do we care about the bias? Can we always find an unbiased estimator? What does it mean?\n\nFor most of the course, we focus on the *unbiased estimator*. This may often be obtained from using the LLN. Then, such as in the MC Chapter, we can compare the (relative) efficiency between the estimators, and discuss about the variance reduction.\n\n## Bootstrap\n\nBootstrap estimates of a sampling distribution are analogous to the idea of *density estimation* (will be introduced in a later chapter). We construct a histogram of a sample to obtain an estimate of the shape of the density function. The histogram is not the density, but in a nonparametric problem, can be viewed as a reasonable estimate of the density. We have methods to generate random samples from completely specified densities; bootstrap generates random samples from the empirical distribution of the sample.\n\nSuppose we have an observed random sample $x=(x_1,\\dots,x_n)$ from a distribution $F$. If $X^\\ast$ is selected at random from $x$, then $$P(X^\\ast=x_i)=1/n,\\quad i=1,\\dots,n.$$\n\nResampling generates a random sample $X^\\ast_1,\\dots,X_n^\\ast$ by *sampling with replacement* from the observed sample $x$. Then the RVs $X_i^\\ast$ are i.i.d. and uniformly distributed on the observed data points $\\{x_1,\\dots,x_n\\}$.\n\nThe empirical distribution function (ecdf) $F_n(x)$ is an estimator of $F(x)$. It can be shown that $F_n(x)$ is a sufficient statistic for $F(x)$; that is, all the information about $F(x)$ that is contained in the sample is also contained in $F_n(x)$. Moreover, $F_n(x)$ is itself the distribution function of a random variable; namely the random variable that is uniformly distributed on the set $\\{x_1, \\dots , x_n\\}$. Hence the empirical cdf $F_n$ is the cdf of $X^\\ast$. Thus in bootstrap, there are two approximations. The ecdf $F_n$ is an approximation to the cdf $F_X$. The ecdf $F_m^\\ast$ of the bootstrap replicates is an approximation to the ecdf $F_n$. Resampling from the sample $x$ is equivalent to generating random samples from the distribution $F_n(x)$. The two approximations can be represented by the diagram\n\n$$\n\\begin{aligned}\nF & \\rightarrow X \\rightarrow F_n \\\\\nF_n & \\rightarrow X^* \\rightarrow F_n^* .\n\\end{aligned}\n$$\n\nTo generate a bootstrap random sample by resampling $x$, generate $n$ random integers $\\{i_1,\\dots, i_n\\}$ uniformly distributed on $\\{1,\\dots , n\\}$ and select the bootstrap sample $x^\\ast = (x_{i_1} ,\\dots , x_{i_n} )$.\n\n------------------------------------------------------------------------\n\nLet $\\theta$ be the parameter of interest (could be a vector), and $\\htt$ be its estimator. Then the bootstrap estimate of the distribution of $\\htt$ is obtained as follows.\n\n::: {.callout-algorithm title = \"Procedure of Bootstrap\"}\n\n1.  For each bootstrap replicate, indexed $b = 1, \\dots, B$:\n    (a) Generate sample $x^{\\ast (b)} = x_1^\\ast,\\dots,x_n^\\ast$ by sampling with replacement from the observed sample $x_1,\\dots,x_n$.\n    (b) Compute the $b$th replicate $\\hat \\theta^{(b)}$ from the $b$th bootstrap sample.\n2.  The bootstrap estimate of $F_{\\hat \\theta}(\\cdot)$ is the empirical distribution of the replicates $\\hat \\theta^{(1)},\\dots ,\\hat \\theta^{(B)}$.\n\n:::\n\n::: {.callout-example title=\"Empirical CDF and Bootstrap Distribution\"}\nSuppose that we have observed the sample\n\n$$x = \\{2, 2, 1, 1, 5, 4, 4, 3, 1, 2\\}.$$\n\nResampling from $x$ we select $1, 2, 3, 4$, or $5$ with probabilities $0.3, 0.3, 0.1, 0.2$, and $0.1$, respectively, so the cdf $F_{X^\\ast}$ of a randomly selected replicate is exactly the ecdf $F_n(x)$:\n\n$$\nF_{X *}(x)=F_n(x)= \\begin{cases}0, & x<1 ; \\\\ 0.3, & 1 \\leq x<2 ; \\\\ 0.6, & 2 \\leq x<3 ; \\\\ 0.7, & 3 \\leq x<4 ; \\\\ 0.9, & 4 \\leq x<5 ; \\\\ 1, & x \\geq 5 .\\end{cases}\n$$\n\nNote that if $F_n$ is not close to $F_X$ then the distribution of the replicates will not be close to $F_X$. The sample $x$ above is actually a sample from a Poisson(2) distribution. Resampling from $x$ a large number of replicates produces a good estimate of $F_n$ but not a good estimate of $F_X$ , because regardless of how many replicates are drawn, the bootstrap samples will never include $0$.\n:::\n\n### Bootstrap Estimation of Standard Error\n\nRecall that, in normal approximate, the $(100-\\alpha)$% confidence interval (for $\\mu$) is given as $$\n  \\bar{x}_n \\pm z_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}.\n$$ In the above formula, we refer\n\n-   $\\sigma/\\sqrt{n}$: as the **standard error** (se).\n-   $z_{1-\\alpha/2}  \\sigma/\\sqrt{n}$: as the **margin of error**.\n\n::: {.callout-definition title=\"Standard Error\"}\nThe standard error (SE) of a statistic is the standard deviation of its sampling distribution. The se is often used in calculations of confidence intervals.\n:::\n\nTo estimate the se of an estimator $\\hat \\theta$, we can use the bootstrap method. Let $\\hat \\theta^{(1)},\\dots,\\hat \\theta^{(B)}$ be the bootstrap replicates. The bootstrap estimate of the standard error of $\\hat \\theta$ is given by the sample standard deviation of the replicates:\n\n$$\n\\widehat{s e}\\left(\\hat{\\theta}^*\\right)=\\sqrt{\\frac{1}{B-1} \\sum_{b=1}^B\\left(\\hat{\\theta}^{(b)}-\\overline{\\hat{\\theta}^*}\\right)^2},\n$$ where $\\overline{\\hat{\\theta}^*}=\\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}^{(b)}$.\n\n::: {.callout-note title = \"Remark for estimating se\"}\n\nThe number of replicates needed for good estimates of standard error is not large; $B = 50$ is usually large enough, and rarely is $B > 200$ necessary.\n\n:::\n\n::: {.callout-example title=\"Bootstrap Estimate of Standard Error\"}\nExample 8.2 (Bootstrap estimate of standard error). The law school data set law in the bootstrap \\[286\\] package is from Efron and Tibshirani \\[91\\]. The data frame contains LSAT (average score on law school admission test score) and GPA (average undergraduate grade point average) for 15 law schools.\n\n| Observation | LSAT | GPA |\n|-------------|------|-----|\n| 1           | 576  | 339 |\n| 2           | 635  | 330 |\n| 3           | 558  | 281 |\n| 4           | 578  | 303 |\n| 5           | 666  | 344 |\n| 6           | 580  | 307 |\n| 7           | 555  | 300 |\n| 8           | 661  | 343 |\n| 9           | 651  | 336 |\n| 10          | 605  | 313 |\n| 11          | 653  | 312 |\n| 12          | 575  | 274 |\n| 13          | 545  | 276 |\n| 14          | 572  | 288 |\n| 15          | 594  | 296 |\n\nThis data set is a random sample from the universe of 82 law schools in law82 (bootstrap). Estimate the correlation between LSAT and GPA scores, and compute the bootstrap estimate of the standard error of the sample correlation.\n\nThe bootstrap procedure is\n\n1.  For each bootstrap replicate, indexed $b = 1,\\dots , B$\n    (a) Generate sample $(\\text{LSAT}^{\\ast (b)}_i, \\text{GPA}^{\\ast (b)}_i), i = 1,\\dots , n$ by sampling with replacement from the observed sample.\n    (b) Compute the $b$th replicate of the correlation $\\hat \\theta^{(b)} = \\cor(\\text{LSAT}^{\\ast (b)}, \\text{GPA}^{\\ast (b)})$ from the $b$th bootstrap sample.\n2.  The bootstrap estimate of the standard error of $\\hat \\theta$ is the sample standard deviation of the replicates $\\hat \\theta^{(1)},\\dots ,\\hat \\theta^{(B)}$, $\\{R^{(i)}\\}_{i=1}^B$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bootstrap) #for the law data\nprint(cor(law$LSAT, law$GPA))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7763745\n```\n\n\n:::\n\n```{.r .cell-code}\n# [1] 0.7763745\nprint(cor(law82$LSAT, law82$GPA))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7599979\n```\n\n\n:::\n\n```{.r .cell-code}\n# [1] 0.7599979\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#set up the bootstrap\nB <- 200 #number of replicates\nn <- nrow(law) #sample size\nR <- numeric(B) #storage for replicates\n#bootstrap estimate of standard error of R\nfor (b in 1:B) {\n  # randomly select the indices\n  i <- sample(1:n, size = n, replace = TRUE)\n  LSAT <- law$LSAT[i] # i is a vector of indices\n  GPA <- law$GPA[i]\n  R[b] <- cor(LSAT, GPA)\n}\n#output\n(se.R <- sd(R))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1335692\n```\n\n\n:::\n\n```{.r .cell-code}\n# [1] 0.1358393\nhist(R, prob = TRUE)\n```\n\n::: {.cell-output-display}\n![](06-resample_files/figure-pdf/unnamed-chunk-2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThe bootstrap estimate of se(R) is 0.1335692. The normal theory estimate for standard error of R is 0.115.\n:::\n\n::: {.callout-example title=\"Bootstrap Estimate of Standard Error using *boot* function\"}\nWe can also use the `boot(.)` function in the `boot` package to compute the bootstrap estimate of standard error.\n\nFirst, write a function that returns $\\hat \\theta(b)$, where the first argument to the function is the sample data, and the second argument is the vector $\\{i_1, \\dots , i_n\\}$ of indices. If the data is $x$ and the vector of indices is i, we need $x[i,1]$ to extract the first resampled variable, and x\\[i,2\\] to extract the second resampled variable. The code and output is shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_r <- function(x, i) {\n  # want correlation of columns 1 and 2\n  cor(x[i, 1], x[i, 2])\n}\n```\n:::\n\n\nThe printed summary of output from the boot function is obtained by the command boot or the result can be saved in an object for further analysis. Here we save the result in obj and print the summary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(boot) #for boot function\nset.seed(777)\n(obj <- boot(data = bootstrap::law, statistic = my_r, R = 2000))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = bootstrap::law, statistic = my_r, R = 2000)\n\n\nBootstrap Statistics :\n     original      bias    std. error\nt1* 0.7763745 -0.01075929   0.1364617\n```\n\n\n:::\n:::\n\n\nThe observed value $\\hat \\theta$ of the correlation statistic is labeled t1\\*. The bootstrap estimate of standard error of the estimate is se(Ë†Î¸) . = 0.13, based on 2000 replicates. To compare with formula (8.1), extract the replicates in \\$t, so that the standard deviation of the replicates can be computed directly\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsd(obj$t)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1364617\n```\n\n\n:::\n:::\n\n:::\n\n### Bootstrap Estimation of Bias\n\nRecall that a bias of an estimator $\\hat \\theta$ is defined as $$\n\\mathbb b(\\hat \\theta) = \\mathbb E[\\hat \\theta] - \\theta.\n$$\n\nAn example of a biased estimator is the MLE of variance, $\\hat{\\sigma}^2 = \\sum (X_i-\\bar{X}_n)^2/n$ of $\\sigma^2 = \\sum (X_i-\\bar{X}_n)^2/(n-1)$, which has expected value $(1 âˆ’ 1/n)\\sigma^2$. Here, the bias is $âˆ’\\sigma^2/n$.\n\nThe bootstrap estimation of bias uses the bootstrap replicates of $\\hat \\theta$ to estimate the sampling distribution of $\\hat \\theta$. For the finite population $x = (x_1,\\dots , x_n)$, the parameter is $\\hat \\theta(x)$ and there are $B$ i.i.d. estimators $\\hat \\theta^{(b)}$. The sample mean of the replicates {Ë†Î¸(b)} is unbiased for its expected value $\\mathbb E[\\hat \\theta^\\ast]$, so the bootstrap estimate of bias is\\\n$$bias(\\hat \\theta) = \\hat \\theta^\\ast âˆ’ \\hat \\theta$$, where $\\hat \\theta^\\ast  = \\sum_{b=1}^B \\hat \\theta^{(b)}$, and $\\hat \\theta= \\hat \\theta(x)$ is the estimate computed from the original observed sample. (In bootstrap $F_n$ is sampled in place of $F_X$ , so we replace $\\theta$ with $\\hat \\theta$ to estimate the bias.)\n\n::: {.callout-note title = \"Remark for estimating bias\"}\n\n-   $\\oplus$ Positive bias indicates that $\\hat \\theta$ on average tends to overestimate $\\theta$,\n\n-   $\\ominus$ Negative bias is on average underestimate $\\theta$\n\n:::\n\n::: {.callout-example title=\"Bootstrap Estimate of Bias\"}\nIn the law data of Example 8.2, compute the bootstrap estimate of bias in the sample correlation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# sample estimate for n=15\ntheta.hat <- cor(law$LSAT, law$GPA)\n# bootstrap estimate of bias\nB <- 2000 # larger for estimating bias\nn <- nrow(law)\ntheta.b <- numeric(B)\nfor (b in 1:B) {\n  i <- sample(1:n, size = n, replace = TRUE)\n  LSAT <- law$LSAT[i]\n  GPA <- law$GPA[i]\n  theta.b[b] <- cor(LSAT, GPA)\n}\nbias <- mean(theta.b - theta.hat)\nbias\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.001063244\n```\n\n\n:::\n\n```{.r .cell-code}\n# [1] -0.005797944\n```\n:::\n\n\nThe estimate of bias is -0.005797944. Note that this is close to the estimate of bias returned by the boot function in Example 8.3.\n:::\n\n::: {.callout-example title=\"One more example of Bootstrap estimation of bias\"}\nThe `patch` (bootstrap) data contains measurements of a certain hormone in the bloodstream of 8 subjects after wearing a medical patch. The parameter of interest is\n\n$$\n\\theta\\coloneqq\\frac{\\mathbb E(\\text { new })-\\mathbb E(\\text { old })}{\\mathbb E(\\text { old })-\\mathbb E(\\text { placebo })}\n$$\n\nIf $|\\theta| \\ge 0.20$, this indicates bioequivalence of the old and new patches. The statistic is $\\bar{Y} /\\bar{Z}$. Compute a bootstrap estimate of bias in the bioequivalence ratio statistic.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(patch, package = \"bootstrap\")\nhead(patch)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  subject placebo oldpatch newpatch    z     y\n1       1    9243    17649    16449 8406 -1200\n2       2    9671    12013    14614 2342  2601\n3       3   11792    19979    17274 8187 -2705\n4       4   13357    21816    23798 8459  1982\n5       5    9055    13850    12560 4795 -1290\n6       6    6290     9806    10157 3516   351\n```\n\n\n:::\n\n```{.r .cell-code}\nn <- nrow(patch) #in bootstrap package\nB <- 2000\ntheta.b <- numeric(B)\ntheta.hat <- mean(patch$y) / mean(patch$z)\n\nfor (b in 1:B) {\n  i <- sample(1:n, size = n, replace = TRUE)\n  y <- patch$y[i]\n  z <- patch$z[i]\n  theta.b[b] <- mean(y) / mean(z)\n}\nbias <- mean(theta.b) - theta.hat\nse <- sd(theta.b)\nresult <- data.frame(\n  Estimate = theta.hat,\n  Bias = bias,\n  SE = se,\n  CV = bias / se\n)\n\nknitr::kable(result, digits = 3, caption = \"Bootstrap results for correlation estimate\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Bootstrap results for correlation estimate\n\n| Estimate|  Bias|    SE|    CV|\n|--------:|-----:|-----:|-----:|\n|   -0.071| 0.006| 0.101| 0.061|\n\n\n:::\n:::\n\n\nIf $|\\mathbb b|/se \\le 0.25$, it is not usually necessary to adjust for bias. The $\\mathbb b$ is small relative to standard error ( $\\mathbb b/se < 0.08$), so in this example it is not necessary to adjust for bias.\n:::\n\n## Jackknife\n\nThe jackknife is another resampling technique used to estimate the bias and variance of a statistic.\n\nJackknife is like a **leave-one-out cross-validation** (LOOCV). Let $\\mathbf{x}= (x_1,\\dots,x_n)$ be an observed random sample, and denote the $i$th jackknife sample by $\\mathbf{x}_{-i} = (x_1,\\dots,x_{i-1},x_{i+1},\\dots,x_n)$, that is, a subset of $\\mathbf{x}$.\n\nFor the parameter of interest $\\theta$, if the statistics is $T(\\mathbf{x})=:\\hat \\theta$ is computed on the full\n\n### When does jackknife not work?\n\nJackknife does not work when the function $T(\\cdot)$ is **not a smooth** functional!\n\n------------------------------------------------------------------------\n\nThe jackknife is another resampling method, proposed by Quenouille \\[225, 224\\] for estimating bias, and by Tukey \\[289\\] for estimating standard error, a few decades earlier than the bootstrap. Efron \\[88\\] is a good introduction to the jackknife.\n\nJackknife is a special kind of *Cross-validation* where we *leave-one-out* (LOO) the observation, and calculate the quantities on the remaining data. To fix the idea, let $x=(x_1,\\dots,x_n)$ be the observed data of samele size $n$. The $i$th jackknife sample is defined as $x_{-i}=(x_1,\\dots,x_{i-1},x_{i+1},\\dots,x_n)$, that is, the sample with the $i$th observation removed. Let $\\hat \\theta=T(x)$ be the estimator of the parameter of interest $\\theta$. The $i$th jackknife replicate is defined as $\\hat \\theta_{-i}=T(x_{-i})$, that is, the estimate computed from the $i$th jackknife sample. The jackknife estimate of bias is defined as\n\n### Jackknife Estimate of Bias\n\nIf Ë†Î¸ is a smooth (plug-in) statistic, then $\\hat \\theta_{(\\cdot)} = t\\{F_{nâˆ’1}(x(i))\\}$, and the jackknife estimate of bias is $$\n\\hat{b}_{jack} = (n âˆ’ 1)(\\hat \\theta_{(\\cdot)} âˆ’ \\hat \\theta), \n$$ where $\\overline{\\hat \\theta_{(\\cdot)}}=\\frac{1}{n} \\sum_{i=1}^n \\hat \\theta_{(i)}$ is the average of the estimate from LOO samples, and $\\hat \\theta=\\hat \\theta(x)$ is the estimate from the original observed sample.\n\n::: {.callout-example title=\"Jackknife Estimate of Bias\"}\nCompute the jackknife estimate of bias for the patch data in the `bootstrap` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(patch, package = \"bootstrap\")\nn <- nrow(patch)\ny <- patch$y\nz <- patch$z\ntheta.hat <- mean(y) / mean(z)\nprint (theta.hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.0713061\n```\n\n\n:::\n\n```{.r .cell-code}\n#compute the jackknife replicates, leave-one-out estimates\ntheta.jack <- numeric(n)\nfor (i in 1:n) {\n  theta.jack[i] <- mean(y[-i]) / mean(z[-i])\n}\n(bias <- (n - 1) * (mean(theta.jack) - theta.hat) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.008002488\n```\n\n\n:::\n:::\n\n:::\n\n## Others\n\nThe distribution of $X^\\ast$ is the empirical distribution $F_n$ of the sample $x$. The empirical distribution places mass $1/n$ at each observed data point. The bootstrap uses the empirical distribution as an estimate of the true but unknown distribution $F$.\n\nThe bootstrap is a resampling method that allows estimation of the *sampling distribution* of almost any statistic using random sampling methods.\n\n-   Parametric bootstrap\n\n-   Nonparametric bootstrap: In nonparametric bootstrap, the distribution is not specified.\n\n::: {.callout-example title=\"Simple example of Bootstrap\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(333)\nn <- 1E4\nB <- 1E4\nx <- rnorm(n, mean = 5, sd = 2) #original sample\n\n\nboot_means <- pbapply::pbsapply(1:B, function(i){\n  indices <- sample(1:n, size = n, replace = TRUE) #resample\n  boot.sample <- x[indices]\n  mean(boot.sample)\n}) \nmean(boot_means)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.999525\n```\n\n\n:::\n\n```{.r .cell-code}\nhist(boot_means, breaks = 50, main = \"Bootstrap distribution of the mean\", xlab = \"Mean\")\n```\n\n::: {.cell-output-display}\n![](06-resample_files/figure-pdf/bootstrap-simple-example-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Compute statistics\ntrue_mean <- 5\nboot_mean <- mean(boot_means)\ndf <- tibble(x = boot_means)\n# Plot\nggplot(df, aes(x = x)) +\n  geom_histogram(aes(y = ..density..), bins = 50, \n                 fill = \"lightgreen\", color = \"black\", alpha = 0.7) +\n  geom_density(color = \"darkgreen\", size = 1) +\n  geom_vline(xintercept = true_mean, color = \"red\", linetype = \"dashed\", size = 1.1) +\n  geom_vline(xintercept = boot_mean, color = \"blue\", linetype = \"solid\", size = 1.1) +\n  labs(\n    title = \"Bootstrap Distribution of the Sample Mean\",\n    x = \"Bootstrap Mean\",\n    y = \"Density\"\n  ) +\n  annotate(\"text\", x = true_mean, y = 0.3, label = \"True mean = 5\", color = \"red\", hjust = -0.1) +\n  annotate(\"text\", x = boot_mean, y = 0.25, label = sprintf(\"Bootstrap mean = %.2f\", boot_mean),\n           color = \"blue\", hjust = -0.1) +\n  theme_minimal(base_size = 14)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\ni Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\ni Please use `after_stat(density)` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](06-resample_files/figure-pdf/bootstrap-simple-example-2.pdf){fig-pos='H'}\n:::\n:::\n\n:::\n\nThe bootstrap is a general tool for assessing statistical accuracy. First we describe the bootstrap in general, and then show how it can be used to estimate extra-sample prediction error. As with cross-validation, the boot- strap seeks to estimate the conditional error ErrT , but typically estimates well only the expected prediction error Err.\n\nThe distribution of the finite population represented by the sample can be regarded as a pseudo-population with similar characteristics as the true population. By repeatedly generating random samples from this pseudo-population (resampling), the sampling distribution of a statistic can be estimated. Properties of an estimator such as *bias* or *standard error* can be estimated by resampling.\n\nBootstrap estimates of a sampling distribution are analogous to the idea of density estimation. We construct a histogram of a sample to obtain an estimate of the shape of the density function. The histogram is not the density, but in a nonparametric problem, can be viewed as a reasonable estimate of the density. We have methods to generate random samples from completely specified densities; bootstrap generates random samples from the empirical distribution of the sample.\n\nThe term *bootstrap* can refer to nonparametric bootstrap or parametric bootstrap. Monte Carlo methods that involve sampling from a fully specified probability distribution, such as methods of Chapter 7 are sometimes called parametric bootstrap. Nonparametric bootstrap is the subject of this chapter. In nonparametric bootstrap, the distribution is not specified.\n\nTo generate a bootstrap random sample by resampling $x$, generate n random integers $\\{i_1,\\dots, i_n\\}$ uniformly distributed on $\\{1,\\dots , n\\}$ and select the bootstrap sample $x^âˆ— = (x_{i_1} ,\\dots, x_{i_n} )$. Suppose $\\theta$ is the parameter of interest ($\\theta$ could be a vector), and $\\hat \\theta$ is an estimator of $\\theta$. Then the bootstrap estimate of the distribution of $\\hat \\theta$ is obtained as follows.\n\n## Applications\n\nThese methods are widely used in statistical inference and have applications in various fields.\n\n## Side reading (Won't be on the exam)\n\n::: {.callout-note title=\"Normal theory for SE (will not be on exam)\"}\nNormal Approximation for Correlation SE\n\nFor a bivariate normal population, the sampling distribution of the sample correlation $r$ is approximately normal with variance\n\n$$\n\\operatorname{Var}(r) \\approx \\frac{(1 - \\rho^2)^2}{n - 1}.\n$$\n\nThus, the **standard error (SE)** under the normal approximation is\n\n$$\n\\operatorname{se}_{\\text{normal}}(r) = \\sqrt{\\frac{(1 - r^2)^2}{n - 1}}.\n$$\n\nIn the Law school example\n\nGiven: - $r = 0.7763745$ - $n = 15$\n\nWe can compute:\n\n$$\n1 - r^2 = 1 - (0.776)^2 = 1 - 0.602 = 0.398,\n$$ $$\n\\operatorname{Var}(r) \\approx \\frac{(0.398)^2}{14} = \\frac{0.158}{14} = 0.0113,\n$$ $$\n\\operatorname{se}(r) = \\sqrt{0.0113} = 0.106.\n$$\n\nThis gives a rough normal-theory estimate of **0.106**.\n\n-   For Fisher z-Transformation Approximation\n\nA more accurate approximation uses the **Fisher z-transform**:\n\n$$\nz = \\frac{1}{2}\\log\\!\\left(\\frac{1 + r}{1 - r}\\right),\n$$\n\nwhich is approximately normal with\n\n$$\n\\operatorname{Var}(z) = \\frac{1}{n - 3}.\n$$\n\nApplying the delta method:\n\n$$\n\\operatorname{se}(r) \\approx (1 - r^2)\\sqrt{\\frac{1}{n - 3}}.\n$$\n\nPlug in the values:\n\n$$\n\\operatorname{se}(r) = (1 - 0.776^2)\\sqrt{\\frac{1}{12}} = 0.398 \\times 0.289 = 0.115.\n$$\n\nâœ… **Normal approximation (Fisher z) SE = 0.115**\n\nAlternatively, we may use the **Bootstrap Estimate** to compare with:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bootstrap) # for the law data\nset.seed(123)\n\n# Sample data\nlaw <- data.frame(\n  LSAT = c(576, 635, 558, 578, 666, 580, 555, 661, 651, 605, 653, 575, 545, 572, 594),\n  GPA  = c(339, 330, 281, 303, 344, 307, 300, 343, 336, 313, 312, 274, 276, 288, 296)\n)\n\n# Sample correlation\nr <- cor(law$LSAT, law$GPA)\nn <- nrow(law)\n\n# Bootstrap estimate of SE\nB <- 200\nR <- replicate(B, {\n  i <- sample(1:n, size = n, replace = TRUE)\n  cor(law$LSAT[i], law$GPA[i])\n})\nse_boot <- sd(R)\n\n# Normal approximation\nse_normal <- sqrt((1 - r^2)^2 / (n - 1))\n\n# Fisher z-approximation\nse_fisher <- (1 - r^2) / sqrt(n - 3)\n\n# Display results\ndata.frame(\n  Method = c(\"Bootstrap\", \"Normal theory\", \"Fisher z-approximation\"),\n  SE = c(se_boot, se_normal, se_fisher)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  Method        SE\n1              Bootstrap 0.1200175\n2          Normal theory 0.1061676\n3 Fisher z-approximation 0.1146741\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Reference:\n\n-   Section 7.11 in Hastie, T., Tibshirani R. and Friedman, J. (2008). [The Elements of Statistical Learning](https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf). Springer, 2nd edition.\n\n-   Chapter 8 of Rizzo, M.L. (2007). *Statistical Computing with R*. CRC Press, Roca Baton.\n\n-   Wu, C.F.J. (1986).[Bootstrap and Other Resampling Methods in Regression Analysis](https://projecteuclid.org/journals/annals-of-statistics/volume-14/issue-4/Jackknife-Bootstrap-and-Other-Resampling-Methods-in-Regression-Analysis/10.1214/aos/1176350142.full). *The Annals of Statistics*.\n\n------------------------------------------------------------------------\n\n## R Functions\n\nSome useful R functions for resampling methods:\n\n-   `sample()`: Generate random samples from a specified set of data points.\n\n-   `boot::boot()`: Perform bootstrap resampling and compute statistics.\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}