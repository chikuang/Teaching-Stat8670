{
  "hash": "1e0f8d0721c4744b2c8ee5416957bb52",
  "result": {
    "engine": "knitr",
    "markdown": "# Monte Carlo Simulation and Variance Reduction\n\n\\newcommand{\\unif}{\\operatorname{Unif}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\var}{\\mathbb{V}ar}\n\\newcommand{\\geom}{\\operatorname{Geom}}\n\\newcommand{\\beta}{\\operatorname{Beta}}\n\\newcommand{\\bern}{\\operatorname{Bern}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\iid}{\\overset{iid}{\\sim}}\n\\newcommand{\\cov}{\\mathbb{C}ov}\n\n\n\n\nMonte Carlo (MC) integration is a simulation-based method for *approximating integrals* using random sampling.\n\nIn numerical integration, methods such as the trapezoidal rule use a *deterministic approach*. MC integration, on the other hand, employs a *non-deterministic* approach: each realization provides a different outcome.\n\n## What is Monte Carlo Simulation?\n\n###  History\n\nMonte Carlo (MC) is a casino in Monaco, famous for its gambling and games of chance. The term \"Monte Carlo\" was coined by physicists Stanislaw Ulam in the 1940s while working on nuclear weapon projects at Los Almos. \n\n\n![Monte Carlo Casino, Picture borrowed from Wikipedia](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c2/Table_de_Roulette%2C_Casino_de_Monte_Carlo_%28Carte_Postale_Ancienne%29.jpg/640px-Table_de_Roulette%2C_Casino_de_Monte_Carlo_%28Carte_Postale_Ancienne%29.jpg\n \"Monte Carlo Casino, Picture borrowed from Wikipedia\")\n\n\n\nMonte Carlo methods are mainly used in three distinct problem classes:\n\n1. optimization\n\n2. numerical integration \n\n3. generating draws from a probability distribution. \n\nThey can also be used to model phenomena with significant uncertainty in inputs, such as calculating the risk of a nuclear power plant failure. Monte Carlo methods are often implemented using computer simulations, and they can provide approximate solutions to problems that are otherwise intractable or too complex to analyze mathematically. \n\n\n### Key Steps of MC methods\n\n::: {.callout-algorithm title=\"Monte Carlo Pattern\"}\n\nMonte Carlo methods vary, but tend to follow a particular pattern:    \n    \n1. Define a domain of possible inputs.\n\n2. Generate inputs randomly from a probability distribution over the domain.\n\n3. Perform a deterministic computation of the outputs.\n\n4. Aggregate the results.\n:::\n\n::: {.callout-example title=\"Calculating the quadrant\"}\n\n\nFor example, consider a quadrant (circular sector) inscribed in a unit square. Given that the ratio of their areas is $\\pi/4$4⁠, the value o f$\\piπ can be approximated using the Monte Carlo method:\n\n1. Draw a square, then inscribe a quadrant within it.\n\n2. Uniformly scatter a given number of points over the square.\n\n3. Count the number of points inside the quadrant, i.e. having a distance from the origin of less than 1.\n\n4. The ratio of the inside-count and the total-sample-count is an estimate of the ratio of the two areas, ⁠π/4⁠. Multiply the result by 4 to estimate π.\n\n![Picture borrowed from Wikipedia](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Pi_monte_carlo_all.gif/660px-Pi_monte_carlo_all.gif \"borrowed from Wikipedia\")\n\n\n:::\n\n## Basic Monte Carlo Integration\n\nTo approximate the integral of a function $f(x)$ over the interval $[a, b]$, we can use the following formula:\n\nConsider the problem of estimating $\\theta = \\int_0^1 g(x)dx$. If $X_1,\\dots , X_m\\sim\\operatorname{Unif}(0,1)$, then the MC estimator is given by:\n\n$$\\hat{\\theta}=\\bar{g}_m(X)=\\frac{1}{m}\\sum_{i=1}^m g(X_i)$$ converges to $\\mathbb{E}[g(X)]$ as $m\\to\\infty$ with probability 1, by *Strong law of Large Number* (SLLN). The simple MC estimator is unbiased, i.e., $\\bar{g}_m(X)$.\n\n::: {.callout-example title=\"Sample MC Integration\"}\nCompute a MC estimate \n$$\n\\theta = \\int_0^1 \\exp(-x)dx,\n$$\nand compare the estimate with the theoretical value\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(777)\nn <- 1E3\nx <- runif(n)\n# simulated estimator\ntheta_hat <- exp(-x) |> mean()\n\n# theoretical value\ntheta_true <- 1 - exp(-1)\n\n# put them in a tibble\n(results <- tibble(\n  Method = c(\"Simulated\", \"Theoretical\"),\n  Value  = c(theta_hat, theta_true)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  Method      Value\n  <chr>       <dbl>\n1 Simulated   0.641\n2 Theoretical 0.632\n```\n\n\n:::\n:::\n\n\n:::\n\n\nTo simulate $\\int_a^b g(t)dt$, use change of variable so the limit becomes from $0$ to $1$. This can be done through a *linear transformation* of the variable $t$: $y:=\\frac{t-a}{b-a}$. Then, $t=a+(b-a)y$ and $dt=(b-a)dy$. Thus, we have\n$$\n\\int_a^b g(t)dt = (b-a)\\int_0^1 g(a+(b-a)y)dy.\n$$\nAlternatively, instead of using $\\unif(0,1)$, we can replace it with other densities with supports between $a$ and $b$. One instance is, \n$$\n\\int_a^b g(t)dt = \\int_a^b \\frac{g(t)}{f}f dt = (b-a) \\int_a^b  \\frac{g(t)}{b-a} dt.\n$$\nThis is a $b-a$ times the expectation of $g(X)$ where $X\\sim \\unif(a,b)$. Therefore, this integral can be estimated by averaging through the function $g(\\cdot)$ over the interval from $a$ to $b$ multiply by $b-a$.\n\n::: {.callout-example title=\"MC Integration with different limits\"}\n\nCompute a MC estimate of \n$$\n\\theta = \\int_2^4 \\exp(-x)dx,\n$$\nand compare the estimate with the exact value of the integral.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tibble)\n\nset.seed(777)\nm <- 1E3\nx <- runif(m, min = 2, max = 4)\n\n# simulated estimator\ntheta_hat <- exp(-x) |> mean() * (4 - 2)\n\n# theoretical value\ntheta_true <- exp(-2) - exp(-4)\n\n# put into tibble\n(results <- tibble(\n  Method = c(\"Simulated\", \"Theoretical\"),\n  Value  = c(theta_hat, theta_true)\n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  Method      Value\n  <chr>       <dbl>\n1 Simulated   0.120\n2 Theoretical 0.117\n```\n\n\n:::\n:::\n\n:::\n\n::: {.callout-algorithm}\nTo summarize, the simple Monte Carlo estimator of the integral $\\theta = \\int_a^b g(x)dx$ is computed as follows.\n\n1. Generate $X_1, \\dots , X_m\\iid \\unif(a,b)$,\n\n2. Compute $\\bar{g}(X) = \\frac{1}{m} g(X_i)$.\n\n3. $\\hat{\\theta} = (b − a)\\bar{g}(X)$.\n:::\n\n\n::: {.callout-example title=\"MC integration with unbounded inteval\"}\n\nCompute a MC estimate of a standard normal cdf\n\n$$\n\\Phi(x)=\\int_{-\\infty}^x \\frac{1}{\\sqrt{2 \\pi}} e^{-t^2 / 2} d t\n$$\n\n1. Note, we cannot apply the algorithm above directly because\nthe limits of integration cover an *unbounded interval*. However, we can break this problem into two cases: (i) $x \\ge 0$ and (ii) $x < 0$, and use the *symmetry* of the normal density to handle the second case. Then the problem is to estimate $\\theta =\\int_0^x \\theta = \\int_0^x \\exp(−t^2/2) dt$ for $x > 0$. This can be done by generating random\n$\\unif(0,x)$ numbers, but it would mean changing the parameters of the\nuniform distribution for each different value of the cdf required. Suppose that\nwe prefer an algorithm that always samples from $\\unif(0,1)$.\nThis can be accomplished by a change of variables. Making the substitution\n$y = t/x$, we have $dt = x dy$ and\n\n$$\n\\theta=\\int_0^1 x e^{-(x y)^2 / 2} d y\n$$\nThus, $\\theta = \\E_Y[x\\exp(-(xY)^2/2)]$, where the rv $Y\\sim \\unif(0,1)$. Generate iid $\\unif(0,1)$ random numbers $u_1,\\dots,u_m$, and compute\n$$\\hat{\\theta}=\\overline{g_m(u)}=\\frac{1}{m} \\sum_{i=1}^m x e^{-\\left(u_i x\\right)^2 / 2}.\n$$\n\nThe sample mean $\\hat{\\theta}$ converges to $\\E \\hat{\\theta} = \\theta$ as $m\\to \\infty$. If $x > 0$, the estimate\nof $\\Phi(x) = 1/2 + \\hat{\\theta}/\\sqrt{2\\pi}$. If $x < 0$ compute $\\Phi(x) = 1 − \\Phi(−x)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(.1, 2.5, length = 10)\nm <- 10000\nu <- runif(m)\ncdf <- numeric(length(x))\nfor (i in 1:length(x)) {\n  g <- x[i] * exp(-(u * x[i])^2 / 2)\n  cdf[i] <- mean(g) / sqrt(2 * pi) + 0.5\n}\nPhi <- pnorm(x)\nprint(round(rbind(x, cdf, Phi), 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\nx   0.10 0.367 0.633 0.900 1.167 1.433 1.700 1.967 2.233 2.500\ncdf 0.54 0.643 0.737 0.816 0.878 0.924 0.956 0.976 0.987 0.994\nPhi 0.54 0.643 0.737 0.816 0.878 0.924 0.955 0.975 0.987 0.994\n```\n\n\n:::\n:::\n\n\nNotice that it would have been simpler to generate random Uniform(0, x)\nrandom variables and skip the transformation. In\nfact, the integrand of the previous example is itself a density function, and we can generate random variables from this density. This provides a more direct approach to estimating the integral.\n\n:::\n\n::: {.callout-example title=\"MC integration with indicator function\"}\n\nLet $I(\\cdot)$ be the indicator function, and $Z\\sim N(0,1)$. Then for any constant $x$ we have $\\E[I(Z ≤ x)] = P (Z ≤ x) =\\Phi(x)$, the standard normal cdf evaluated at $x$.\n\nGenerate a random sample $z_1, \\dots , z_m\\sim N(0,1)$. Then the theoretical mean and sample mean are \n$$\\hat{\\theta} = \\frac{1}{m} \\sum_{i=1}^m I(z_i \\le x),$$\nand \n$$\\E[\\hat{\\theta}] = P(Z \\le x) = \\Phi(x).$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(777)\nx <- seq(0.1, 2.5, length = 10)\nm <- 1E4\nz <- rnorm(m)\ndim(x) <- length(x)\np <- apply(x, MARGIN = 1,\n  FUN = function(x, z) {mean(z < x)}, z = z)\nPhi <- pnorm(x)\n\nrbind(x, p, Phi) |> round(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\nx   0.100 0.367 0.633 0.900 1.167 1.433 1.700 1.967 2.233 2.500\np   0.544 0.645 0.740 0.818 0.881 0.926 0.957 0.976 0.986 0.993\nPhi 0.540 0.643 0.737 0.816 0.878 0.924 0.955 0.975 0.987 0.994\n```\n\n\n:::\n:::\n\n\n\nIn this example, compared with the previous example, it appears that\nwe have better agreement with `pnorm()` in the upper tail, but worse agreement\nnear the center.\n\n:::\n\n::: {.callout-note}\nSummarizing, if $f (x)$ is a probability density function supported on a set\nA, (that is, $f (x) \\ge 0$ for all $x\\in\\R$ and $\\int_A f (x) = 1$), to estimate the integral\n$$\\theta = \\int_A g(x)f (x)dx,$$\ngenerate a random sample $x_1,\\dots,x_m$ from the distribution $f (x)$, and compute the sample mean\n$$\\hat{\\theta} = \\frac{1}{m}\\sum_{i=1}^mg(x_i).$$ Then  $\\hat{\\theta}\\overset{p}{\\to}\\theta$ \n:::\n\n\nThe standard error of $\\hat{\\theta} = \\frac{1}{m}\\sum_{i=1}^m g(x_i)$.\n\nRecall that, $\\var{\\hat{\\theta}}=\\sigma^2/m$ where $\\sigma^2 = \\var{g(X)}$. When the distribution of $X$ is unknown, we substitute for $F_X$ the empirical distribution $F_m$ of the\nsample $x_1, \\dots , x_m$. The variance of $\\hat{\\theta}$ can be estimated by\n$$\n\\frac{\\hat{\\sigma}}{m} = \\frac{1}{m^2}\\sum_{i=1}^m [g(x_i) - \\bar{g}(x)]^2.\n$$\n\nNote that\n$$\\frac{1}{m}\\sum_{i=1}^m[g(x_i)-\\bar{g}(x_i)],$$\nis the *plug-in estimate* of $\\var\\{g(X)\\}$. That is, it is the variance of $U$ , where\n$U$ is uniformly distributed on the set of replicates $\\{g(xi)\\}$. The corresponding estimated standard error of $\\hat{\\theta}$ is \n$$\n\\widehat{se}(\\hat{\\theta}) = \\frac{\\hat{\\sigma}}{\\sqrt{m}} = \\frac{1}{m}\\left\\{\\sum_{i=1}^m [ g(x_i)- \\bar{g}(x)]^2\\right\\}.\n$$\nThe CLT implies \n$$\n\\frac{\\hat{\\theta}-E[\\hat{\\theta}]}{\\sqrt{\\operatorname{Var} \\hat{\\theta}}} \\overset{D}{\\to} N(0,1),\n$$\nas $m\\to\\infty$.Hence, if $m$ is sufficiently large, $\\hat{\\theta}$\nis approximately normal with mean $\\theta$. The large-sample, approximately normal distribution of $\\hat{\\theta}$ can be applied to put confidence limits or error bounds\non the MC estimate of the integral, and check for convergence\n\n::: {.callout-example title = \"Error bound of MC integration\"}\n\nCompute the 95% confidence interval for $\\Phi(2)$ and $\\Phi(2.5)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(777)\nx <- 2\nm <- 10000\nz <- rnorm(m)\ng <- (z < x) #the indicator function\nv <- mean((g - mean(g))^2) / m\ncdf <- mean(g)\nc(cdf, v)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9.771000e-01 2.237559e-06\n```\n\n\n:::\n\n```{.r .cell-code}\nc(cdf - 1.96 * sqrt(v), cdf + 1.96 * sqrt(v))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9741681 0.9800319\n```\n\n\n:::\n:::\n\n\nThe interpretation is:\n\n\nthe probability $P (I(Z < x) = 1)$ is $\\Phi(2)\\approx 0.977$. Here $g(X)$ has the distribution of the sample proportion of 1’s in $m = 10000$ Bernoulli trials with\n$p\\approx 0.977$, and the variance of $g(X)$ is therefore $(0.977)(1 − 0.977)/10000 =2.223e-06$. The MC estimate $2.228e-06$ of variance is quite close to this value.\n\nQ: What about $\\Phi(2.5)$?\n:::\n\n## Variance and Efficiency\n\n\n### Efficiency\nSuppose we have two (unbiased) estimators $\\hat{\\theta}_1$ and $\\hat{\\theta}_2$ for θ. We say $\\hat{\\theta}_1$ is *statistically* more efficient than $\\hat{\\theta}_2$ if\n$$\\var(\\hat{\\theta}_1) < \\var(\\hat{\\theta}_2).$$\nWhat is the variance of $hat{\\theta}_i$ is unknown or hard to be calculated?\n\ncan substitute it by sample estimate of the variance for each estimator.\n\nNote, the variance can *always be reduced by increasing the number of\nreplicates*, so computational efficiency is also relevant.\n\n------------------------------------------------------------------------\n\nReference used:\n\n- Chapter 6 of Rizzo, M.L. (2007). *Statistical Computing with R*. CRC Press, Roca Baton.\n\n- Monte Carlo Method page on Wikiepdia, \n[Link](https://en.wikipedia.org/wiki/Monte_Carlo_method).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}