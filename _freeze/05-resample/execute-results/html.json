{
  "hash": "f4337e7aebaeb3e9057f07fea689345a",
  "result": {
    "engine": "knitr",
    "markdown": "# Resampling, Jackknife and Bootstrap\n\n\\newcommand{\\htt}{\\hat{\\theta}}\n\n## Introduction\n\nThis chapter covers resampling methods including the jackknife and bootstrap techniques.\n\n## Jackknife\n\nThe jackknife is a resampling technique used to estimate the bias and variance of a statistic.\n\nJackknife is like a **leave-one-out cross-validation**. Let $\\mathbf{x}= (x_1,\\dots,x_n)$ be an observed random sample, and denote the $i$th jackknife sample by $\\mathbf{x}_{-i} = (x_1,\\dots,x_{i-1},x_{i+1},\\dots,x_n)$, that is, a subset of $\\mathbf{x}$.\n\nFor the parameter of interest $\\theta$, if the statistics is $T(\\mathbf{x})=:\\hat{\\theta}$ is computed on the full\n\n### When does jackknife not work?\n\nJackknife does not work when the function $T(\\cdot)$ is **not a smooth** functional!\n\n## Bootstrap\n\nThe bootstrap is a resampling method that allows estimation of the sampling distribution of almost any statistic using random sampling methods.\n\n## Applications\n\nThese methods are widely used in statistical inference and have applications in various fields.\n\n## Bootstrap\n\nThe term *bootstrap* can refer to nonparametric bootstrap or parametric bootstrap. Monte Carlo methods that involve sampling from a fully specified probability distribution, such as methods of Chapter 7 are sometimes called parametric bootstrap. Nonparametric bootstrap is the subject of this chapter. In nonparametric bootstrap, the distribution is not specified.\n\nTo generate a bootstrap random sample by resampling x, generate n random integers $\\{i_1,\\dots, i_n\\}$ uniformly distributed on $\\{1,\\dots , n\\}$ and select the bootstrap sample $x^∗ = (x_{i_1} ,\\dots, x_{i_n} )$. Suppose θ is the parameter of interest (θ could be a vector), and ˆθ is an estimator of θ. Then the bootstrap estimate of the distribution of ˆθ is obtained as follows.\n\n1.  For each bootstrap replicate, indexed b = 1, . . . , B:\n    (a) Generate sample x∗(b) = x∗ 1, . . . , x∗ n by sampling with replacement from the observed sample x1, . . . , xn.\n    (b) Compute the bth replicate ˆθ(b) from the bth bootstrap sample.\n2.  The bootstrap estimate of Fˆθ (·) is the empirical distribution of the repli- cates ˆθ(1), . . . , ˆθ(B).\n\n## Jackknife\n\nThe jackknife is another resampling method, proposed by Quenouille [225, 224] for estimating bias, and by Tukey [289] for estimating standard error, a\nfew decades earlier than the bootstrap. Efron [88] is a good introduction to the jackknife.\n\n\nJackknife is a special kind of *Cross-validation* where we *leave-one-out* (LOO) the observation, and calculate the quantities on the remaining data. To fix the idea, let $x=(x_1,\\dots,x_n)$ be the observed data of samele size $n$. The $i$th jackknife sample is defined as $x_{-i}=(x_1,\\dots,x_{i-1},x_{i+1},\\dots,x_n)$, that is, the sample with the $i$th observation removed. Let $\\htt=T(x)$ be the estimator of the parameter of interest $\\theta$. The $i$th jackknife replicate is defined as $\\htt_{-i}=T(x_{-i})$, that is, the estimate computed from the $i$th jackknife sample. The jackknife estimate of bias is defined as\n\n\n###  Jackknife Estimate of Bias\n\nIf ˆθ is a smooth (plug-in) statistic, then $\\htt_{(\\cdot)} = t\\{F_{n−1}(x(i))\\}$, and the jackknife estimate of bias is \n$$\n\\hat{b}_{jack} = (n − 1)(\\htt_{(\\cdot)} − \\htt ), \n$$\nwhere $\\overline{\\hat{\\theta}_{(\\cdot)}}=\\frac{1}{n} \\sum_{i=1}^n \\hat{\\theta}_{(i)}$ is the average of the estimate from LOO samples, and $\\htt=\\htt(x)$ is the estimate from the original observed sample.\n\n::: {.callout-example title=\"Jackknife Estimate of Bias\"}\nCompute the jackknife estimate of bias for the patch data in the `bootstrap` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(patch, package = \"bootstrap\")\nn <- nrow(patch)\ny <- patch$y\nz <- patch$z\ntheta.hat <- mean(y) / mean(z)\nprint (theta.hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.0713061\n```\n\n\n:::\n\n```{.r .cell-code}\n#compute the jackknife replicates, leave-one-out estimates\ntheta.jack <- numeric(n)\nfor (i in 1:n) {\n  theta.jack[i] <- mean(y[-i]) / mean(z[-i])\n}\nbias <- (n - 1) * (mean(theta.jack) - theta.hat)  \nbias #jackknife estimate of bias\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.008002488\n```\n\n\n:::\n:::\n\n:::\n\n------------------------------------------------------------------------\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}