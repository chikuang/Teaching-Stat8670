{
  "hash": "1af3a1b9ba3ec1b7fe269b965cf2ede3",
  "result": {
    "engine": "knitr",
    "markdown": "# Resampling, Jackknife and Bootstrap\n\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\var}{\\mathbb{V}ar}\n\\newcommand{\\cov}{\\mathbb{C}ov}\n\\newcommand{\\corr}{\\mathbb{C}orr}\n\\newcommand{\\unif}{\\operatorname{Unif}}\n\\newcommand{\\geom}{\\operatorname{Geom}}\n\\newcommand{\\bet}{\\operatorname{Beta}}\n\\newcommand{\\bern}{\\operatorname{Bern}}\n\\newcommand{\\iid}{\\overset{iid}{\\sim}}\n\\newcommand{\\ef}{\\operatorname{Eff}}\n\\newcommand{\\htt}{\\hat{\\theta}}\n\\newcommand{\\b}{\\mathbb b}\n\n\n\n## Introduction\n\nThis chapter covers resampling methods including the jackknife and bootstrap techniques.\n\n## Resampling methods\n\n*Resampling methods* is a way to use the observed data to generate hypothetical samples. It treats an observed sample as a *finite population*, and random samples are generated/resampled from it to estimate population characteristics and make inferences about the sampled population. It is useful when:\n\n* Do not know the underlying distribution of a population\n\n* The formula may be difficult to be calculated.\n\nSome commonly used resampling methods include:\n\n-   **Bootstrap**: Bootstrap methods are often used when the distribution of the target population is not specified; the sample is the only information available.\n\n-   **Jackknife**: The jackknife is a resampling technique used to estimate the bias and variance of a statistic. It is like a leave-one-out (LOO) cross-validation.\n\n-   **Cross-validation**: Cross-validation is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.\n\n-   **Permutation tests**: Permutation tests are a type of non-parametric statistical test that involves rearranging the data points to test a hypothesis. They are used to determine whether the observed effect is statistically significant by comparing it to the distribution of effects obtained through random permutations of the data.\n\n## Estimators\n\n### Bias-Variance Tradeoff\n\nIn statistics, the bias-variance tradeoff is the property of a model that the variance of the parameter estimated across samples can be reduced by increasing the bias in the estimated parameters. If we look at the Mean square error (MSE) of an estimator $\\hat{\\theta}$ for a parameter $\\theta$:\n\n::: {.callout-definition title=\"Mean Square Errors \"}\nSuppose we have a parameter $\\theta$ and an estimator $\\hat{\\theta}$. The mean square error (MSE) of the estimator is defined as $$\\mathbb{E}[(\\hat{\\theta}-\\theta )^2 ]= \\var(\\hat{\\theta}) + [\\mathbb b(\\hat{\\theta})]^2,$$ where $\\b$ is the bias of the estimator.\n:::\n\n**Questions**: Why do we care about the bias? Can we always find an unbiased estimator? What does it mean?\n\nFor most of the course, we focus on the *unbiased estimator*. This may often be obtained from using the LLN. Then, such as in the MC Chapter, we can compare the (relative) efficiency between the estimators, and discuss about the variance reduction.\n\n## Bootstrap\n\nThe bootstrap is a resampling method that allows estimation of the *sampling distribution* of almost any statistic using random sampling methods.\n\n-   Parametric bootstrap\n\n-   Nonparametric bootstrap: In nonparametric bootstrap, the distribution is not specified.\n\n::: { .callout-note title=\"Simple example of Bootstrap\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(333)\nn <- 1E4\nB <- 1E4\nx <- rnorm(n, mean = 5, sd = 2) #original sample\n\n\nboot_means <- pbapply::pbsapply(1:B, function(i){\n  indices <- sample(1:n, size = n, replace = TRUE) #resample\n  boot.sample <- x[indices]\n  mean(boot.sample)\n}) \nmean(boot_means)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.999525\n```\n\n\n:::\n\n```{.r .cell-code}\nhist(boot_means, breaks = 50, main = \"Bootstrap distribution of the mean\", xlab = \"Mean\")\n```\n\n::: {.cell-output-display}\n![](05-resample_files/figure-html/bootstrap-simple-example-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Compute statistics\ntrue_mean <- 5\nboot_mean <- mean(boot_means)\ndf <- tibble(x = boot_means)\n# Plot\nggplot(df, aes(x = x)) +\n  geom_histogram(aes(y = ..density..), bins = 50, \n                 fill = \"lightgreen\", color = \"black\", alpha = 0.7) +\n  geom_density(color = \"darkgreen\", size = 1) +\n  geom_vline(xintercept = true_mean, color = \"red\", linetype = \"dashed\", size = 1.1) +\n  geom_vline(xintercept = boot_mean, color = \"blue\", linetype = \"solid\", size = 1.1) +\n  labs(\n    title = \"Bootstrap Distribution of the Sample Mean\",\n    x = \"Bootstrap Mean\",\n    y = \"Density\"\n  ) +\n  annotate(\"text\", x = true_mean, y = 0.3, label = \"True mean = 5\", color = \"red\", hjust = -0.1) +\n  annotate(\"text\", x = boot_mean, y = 0.25, label = sprintf(\"Bootstrap mean = %.2f\", boot_mean),\n           color = \"blue\", hjust = -0.1) +\n  theme_minimal(base_size = 14)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](05-resample_files/figure-html/bootstrap-simple-example-2.png){width=672}\n:::\n:::\n\n:::\n\nThe bootstrap is a general tool for assessing statistical accuracy. First we describe the bootstrap in general, and then show how it can be used to estimate extra-sample prediction error. As with cross-validation, the boot- strap seeks to estimate the conditional error ErrT , but typically estimates\nwell only the expected prediction error Err.\n\n\nThe distribution of the finite population represented by the sample can be regarded as a pseudo-population with similar characteristics as the true population. By repeatedly generating random samples from this pseudo-population (resampling), the sampling distribution of a statistic can be estimated. Properties of an estimator such as *bias* or *standard error* can be estimated by resampling.\n\nBootstrap estimates of a sampling distribution are analogous to the idea of density estimation. We construct a histogram of a sample to obtain an estimate of the shape of the density function. The histogram is not the density, but in a nonparametric problem, can be viewed as a reasonable estimate of the density. We have methods to generate random samples from completely specified densities; bootstrap generates random samples from the empirical distribution of the sample.\n\nThe term *bootstrap* can refer to nonparametric bootstrap or parametric bootstrap. Monte Carlo methods that involve sampling from a fully specified probability distribution, such as methods of Chapter 7 are sometimes called parametric bootstrap. Nonparametric bootstrap is the subject of this chapter. In nonparametric bootstrap, the distribution is not specified.\n\nTo generate a bootstrap random sample by resampling x, generate n random integers $\\{i_1,\\dots, i_n\\}$ uniformly distributed on $\\{1,\\dots , n\\}$ and select the bootstrap sample $x^∗ = (x_{i_1} ,\\dots, x_{i_n} )$. Suppose θ is the parameter of interest (θ could be a vector), and ˆθ is an estimator of θ. Then the bootstrap estimate of the distribution of ˆθ is obtained as follows.\n\n1.  For each bootstrap replicate, indexed $b = 1, \\dots, B$:\n    (a) Generate sample x∗(b) = x∗ 1, . . . , x∗ n by sampling with replacement from the observed sample x1, . . . , xn.\n    (b) Compute the bth replicate ˆθ(b) from the bth bootstrap sample.\n2.  The bootstrap estimate of Fˆθ (·) is the empirical distribution of the repli- cates ˆθ(1), . . . , ˆθ(B).\n\n## Jackknife\n\nThe jackknife is a resampling technique used to estimate the bias and variance of a statistic.\n\nJackknife is like a **leave-one-out cross-validation**. Let $\\mathbf{x}= (x_1,\\dots,x_n)$ be an observed random sample, and denote the $i$th jackknife sample by $\\mathbf{x}_{-i} = (x_1,\\dots,x_{i-1},x_{i+1},\\dots,x_n)$, that is, a subset of $\\mathbf{x}$.\n\nFor the parameter of interest $\\theta$, if the statistics is $T(\\mathbf{x})=:\\hat{\\theta}$ is computed on the full\n\n### When does jackknife not work?\n\nJackknife does not work when the function $T(\\cdot)$ is **not a smooth** functional!\n\n## Jackknife\n\nThe jackknife is another resampling method, proposed by Quenouille \\[225, 224\\] for estimating bias, and by Tukey \\[289\\] for estimating standard error, a few decades earlier than the bootstrap. Efron \\[88\\] is a good introduction to the jackknife.\n\nJackknife is a special kind of *Cross-validation* where we *leave-one-out* (LOO) the observation, and calculate the quantities on the remaining data. To fix the idea, let $x=(x_1,\\dots,x_n)$ be the observed data of samele size $n$. The $i$th jackknife sample is defined as $x_{-i}=(x_1,\\dots,x_{i-1},x_{i+1},\\dots,x_n)$, that is, the sample with the $i$th observation removed. Let $\\hat{\\theta}=T(x)$ be the estimator of the parameter of interest $\\theta$. The $i$th jackknife replicate is defined as $\\hat{\\theta}_{-i}=T(x_{-i})$, that is, the estimate computed from the $i$th jackknife sample. The jackknife estimate of bias is defined as\n\n### Jackknife Estimate of Bias\n\nIf ˆθ is a smooth (plug-in) statistic, then $\\hat{\\theta}_{(\\cdot)} = t\\{F_{n−1}(x(i))\\}$, and the jackknife estimate of bias is $$\n\\hat{b}_{jack} = (n − 1)(\\hat{\\theta}_{(\\cdot)} − \\hat{\\theta}), \n$$ where $\\overline{\\hat{\\theta}_{(\\cdot)}}=\\frac{1}{n} \\sum_{i=1}^n \\hat{\\theta}_{(i)}$ is the average of the estimate from LOO samples, and $\\hat{\\theta}=\\hat{\\theta}(x)$ is the estimate from the original observed sample.\n\n::: {.callout-example title=\"Jackknife Estimate of Bias\"}\nCompute the jackknife estimate of bias for the patch data in the `bootstrap` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(patch, package = \"bootstrap\")\nn <- nrow(patch)\ny <- patch$y\nz <- patch$z\ntheta.hat <- mean(y) / mean(z)\nprint (theta.hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.0713061\n```\n\n\n:::\n\n```{.r .cell-code}\n#compute the jackknife replicates, leave-one-out estimates\ntheta.jack <- numeric(n)\nfor (i in 1:n) {\n  theta.jack[i] <- mean(y[-i]) / mean(z[-i])\n}\nbias <- (n - 1) * (mean(theta.jack) - theta.hat)  \nbias #jackknife estimate of bias\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.008002488\n```\n\n\n:::\n:::\n\n:::\n\n## Applications\n\nThese methods are widely used in statistical inference and have applications in various fields.\n\n------------------------------------------------------------------------\n\nReference:\n\n-   Section 7.11 in Hastie, T., Tibshirani R. and Friedman, J. (2008). [The Elements of Statistical Learning](https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf). Springer, 2nd edition.\n\n-   Chapter 8 of Rizzo, M.L. (2007). *Statistical Computing with R*. CRC Press, Roca Baton.\n\n-   Wu, C.F.J. (1986).[Bootstrap and Other Resampling Methods in Regression Analysis](https://projecteuclid.org/journals/annals-of-statistics/volume-14/issue-4/Jackknife-Bootstrap-and-Other-Resampling-Methods-in-Regression-Analysis/10.1214/aos/1176350142.full). *The Annals of Statistics*.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}