{
  "hash": "d9b2c00f9be4aa66ad568d90c1e8c212",
  "result": {
    "engine": "knitr",
    "markdown": "# Monte Carlo Simulation and Variance Reduction\n\n\\newcommand{\\unif}{\\operatorname{Unif}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\geom}{\\operatorname{Geom}}\n\\newcommand{\\beta}{\\operatorname{Beta}}\n\\newcommand{\\bern}{\\operatorname{Bern}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\iid}{\\overset{iid}{\\sim}}\n\\newcommand{\\cov}{\\mathbb{C}ov}\n\n\n\nMonte Carlo (MC) integration is a simulation-based method for *approximating integrals* using random sampling.\n\nIn numerical integration, methods such as the trapezoidal rule use a *deterministic approach*. MC integration, on the other hand, employs a *non-deterministic* approach: each realization provides a different outcome.\n\n## Basic Monte Carlo Integration\n\nTo approximate the integral of a function $f(x)$ over the interval $[a, b]$, we can use the following formula:\n\nConsider the problem of estimating $\\theta = \\int_0^1 g(x)dx$. If $X_1,\\dots , X_m\\sim\\operatorname{Unif}(0,1)$, then the MC estimator is given by:\n\n$$\\hat{\\theta}=\\bar{g}_m(X)=\\frac{1}{m}\\sum_{i=1}^m g(X_i)$$ converges to $\\mathbb{E}[g(X)]$ as $m\\to\\infty$ with probability 1, by *Strong law of Large Number* (SLLN). The simple MC estimator is unbiased, i.e., $\\bar{g}_m(X)$.\n\n::: {.callout-example title=\"Sample MC Integration\"}\nCompute a MC estimate \n$$\n\\theta = \\int_0^1 \\exp(-x)dx,\n$$\nand compare the estimate with the theoretical value\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(777)\nn <- 1E3\nx <- runif(n)\n# simulated estimator\ntheta_hat <- exp(-x) |> mean()\n\n# theoretical value\ntheta_true <- 1 - exp(-1)\n\n# put them in a tibble\n(results <- tibble(\n  Method = c(\"Simulated\", \"Theoretical\"),\n  Value  = c(theta_hat, theta_true)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  Method      Value\n  <chr>       <dbl>\n1 Simulated   0.641\n2 Theoretical 0.632\n```\n\n\n:::\n:::\n\n\n:::\n\n\nTo simulate $\\int_a^b g(t)dt$, use change of variable so the limit becomes from $0$ to $1$. This can be done through a *linear transformation* of the variable $t$: $y:=\\frac{t-a}{b-a}$. Then, $t=a+(b-a)y$ and $dt=(b-a)dy$. Thus, we have\n$$\n\\int_a^b g(t)dt = (b-a)\\int_0^1 g(a+(b-a)y)dy.\n$$\nAlternatively, instead of using $\\unif(0,1)$, we can replace it with other densities with supports between $a$ and $b$. One instance is, \n$$\n\\int_a^b g(t)dt = \\int_a^b \\frac{g(t)}{f}f dt = (b-a) \\int_a^b  \\frac{g(t)}{b-a} dt.\n$$\nThis is a $b-a$ times the expectation of $g(X)$ where $X\\sim \\unif(a,b)$. Therefore, this integral can be estimated by averaging through the function $g(\\cdot)$ over the interval from $a$ to $b$ multiply by $b-a$.\n\n::: {.callout-example title=\"MC Integration with different limits\"}\n\nCompute a MC estimate of \n$$\n\\theta = \\int_2^4 \\exp(-x)dx,\n$$\nand compare the estimate with the exact value of the integral.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tibble)\n\nset.seed(777)\nm <- 1E3\nx <- runif(m, min = 2, max = 4)\n\n# simulated estimator\ntheta_hat <- mean(exp(-x)) * (4 - 2)\n\n# theoretical value\ntheta_true <- exp(-2) - exp(-4)\n\n# put into tibble\n(results <- tibble(\n  Method = c(\"Simulated\", \"Theoretical\"),\n  Value  = c(theta_hat, theta_true)\n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  Method      Value\n  <chr>       <dbl>\n1 Simulated   0.120\n2 Theoretical 0.117\n```\n\n\n:::\n:::\n\n:::\n\n::: {.callout-algorithm}\nTo summarize, the simple Monte Carlo estimator of the integral $\\theta = \\int_a^b g(x)dx$ is computed as follows.\n\n1. Generate $X_1, \\dots , X_m\\iid \\unif(a,b)$,\n\n2. Compute $\\bar{g}(X) = \\frac{1}{m} g(X_i)$.\n\n3. $\\hat{\\theta} = (b − a)\\bar{g}(X)$.\n:::\n\n\n::: {.callout-example title=\"MC integration with unbounded inteval\"}\n\nCompute a MC estimate of a standard normal cdf\n\n$$\n\\Phi(x)=\\int_{-\\infty}^x \\frac{1}{\\sqrt{2 \\pi}} e^{-t^2 / 2} d t\n$$\n\n1. Note, we cannot apply the algorithm above directly because\nthe limits of integration cover an *unbounded interval*. However, we can break this problem into two cases: (i) $x \\ge 0$ and (ii) $x < 0$, and use the *symmetry* of the normal density to handle the second case. Then the problem is to estimate $\\theta =\\int_0^x \\theta = \\int_0^x \\exp(−t^2/2) dt$ for $x > 0$. This can be done by generating random\n$\\unif(0,x)$ numbers, but it would mean changing the parameters of the\nuniform distribution for each different value of the cdf required. Suppose that\nwe prefer an algorithm that always samples from $\\unif(0,1)$.\nThis can be accomplished by a change of variables. Making the substitution\n$y = t/x$, we have $dt = x dy$ and\n\n$$\n\\theta=\\int_0^1 x e^{-(x y)^2 / 2} d y\n$$\nThus, $\\theta = \\E_Y[x\\exp(-(xY)^2/2)]$, where the rv $Y\\sim \\unif(0,1)$. Generate iid $\\unif(0,1)$ random numbers $u_1,\\dots,u_m$, and compute\n$$\\hat{\\theta}=\\overline{g_m(u)}=\\frac{1}{m} \\sum_{i=1}^m x e^{-\\left(u_i x\\right)^2 / 2}.\n$$\n\n:::\n\n------------------------------------------------------------------------\n\nReference used:\n\n- Chapter 6 of Rizzo, M.L. (2007). *Statistical Computing with R*. CRC Press, Roca Baton.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}