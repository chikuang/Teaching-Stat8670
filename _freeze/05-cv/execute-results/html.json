{
  "hash": "5490d0af267cb085413c4ba0fa0dd0b6",
  "result": {
    "engine": "knitr",
    "markdown": "# Cross Validation\n\n\\newcommand{\\err}{\\mathrm{err}}\n\\newcommand{\\Err}{\\mathrm{Err}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\T}{\\mathcal{T}}\n\\newcommand{\\D}{\\mathcal{D}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\var}{\\mathbb{V}ar}\n\\newcommand{\\bias}{\\mathbb{B}\\mathrm{ias}}\n\\newcommand{\\mse}{\\mathrm{MSE}}\n\n\n\n## Introduction\n\nThis chapter covers resampling technique called *cross-validation*.\n\nCross-validation (CV) is a statistical method used to estimate the *skill* of machine learning (ML) models. It is commonly used in applied ML to compare and select a model for a given predictive modeling problem because it is easy to understand, easy to implement, and results in skill estimates that generally have a lower *bias* than other methods.\n\n## Resampling methods\n\n*Resampling methods* is a way to use the observed data to generate hypothetical samples. It treats an observed sample as a *finite population*, and random samples are generated/resampled from it to estimate population characteristics and make inferences about the sampled population. It is useful when:\n\n-   Do not know the underlying distribution of a population\n\n-   The formula may be difficult to be calculated.\n\nSome commonly used resampling methods include:\n\n-   **Bootstrap**: Bootstrap methods are often used when the distribution of the target population is not specified; the sample is the only information available.\n\n-   **Jackknife**: The jackknife is a resampling technique used to estimate the bias and variance of a statistic. It is like a leave-one-out (LOO) cross-validation.\n\n-   **Cross-validation**: Cross-validation is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.\n\n-   **Permutation tests**: Permutation tests are a type of non-parametric statistical test that involves rearranging the data points to test a hypothesis. They are used to determine whether the observed effect is statistically significant by comparing it to the distribution of effects obtained through random permutations of the data.\n\n## Machine Learning Models\n\n-   **Left**: What machine learning can do\n\n-   **Right**: Model/Methods\n\n::::: columns\n::: column\n![](fig/ML_gpt.png){width=\"100%\"}\n:::\n\n::: column\n![](fig/ML_models.png){width=\"100%\"}\n:::\n:::::\n\n## Training and testing/validating sets\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](fig/train_test.jpeg){fig-align='center' width=60%}\n:::\n:::\n\n\n**Questions**\\\nHow do we choose between different models $f_1,\\dots,f_m$?\n\n<!-- ## Data split -->\n\n<!-- * Train a model $\\hat{f}$ on the \\blue{training set}.  -->\n\n<!-- * Use the $\\hat{f}$ with the \\orange{features} to predict the \\red{outcomes}, $\\hat{y}$ -->\n\n<!-- * Difference between $\\red{y}$ and $\\hat{y}$. -->\n\n<!-- ![Data split.](fig/data-split.png){width=55%} -->\n\n<!-- ## Evaluation -->\n\n<!-- There are different metrics to evaluation the prediction. For instance -->\n\n<!-- If $y$ is continuous variable, we can use Mean-Square error. -->\n\n<!-- If $y$ is categorical, we can use the error rate.  -->\n\n<!-- ## Bias-Variance Tradeoff -->\n\n<!-- If we have a complex model, we may have many different tuning parameters.  -->\n\n<!-- \\begin{beamercolorbox}[rounded=true,shadow=true,sep=10pt]{customblock_blue} -->\n\n<!--     \\textbf{\\large Bias-Variance Tradeoff}\\\\ -->\n\n<!-- $$ -->\n\n<!-- \\mse[y,\\hat{y}] \\propto \\var(\\hat{y}) + \\bias^2(\\hat{y}) -->\n\n<!-- $$ -->\n\n<!-- \\end{beamercolorbox} -->\n\n## Setup\n\nSuppose we have a **supervised learning** model $f(X)\\to Y$.\n\nDenote the training set by $\\mathcal{T}=\\{(X_i,Y_i)\\}_{i=1}^{N_{train}}$.\n\nHow to choose between the models $f_1,\\dots,f_m$?\n\nIdeal: $(X,Y)\\sim F_{X,Y}$.\n\nDefine the generalization error (Population error) as $$\n  \\mathrm{Err}(f) = \\mathbb{E}_{X,Y}[\\{Y-f(X)\\}^2] \n$$\n\nChoose $f$ by $$\n  \\arg\\min_{f\\in\\{f_1,\\dots,f_m\\}} \\mathrm{Err}(f)\n$$\n\nBut, $(X,Y)\\sim F_{X,Y}$ is usally **unknown**!\n\n**Question**:\n\nWhat to do if we do not know about $F_{X,Y}$?\n\nLet $V:=\\{(X_i,Y_i)\\}_{i=1}^{N_{Val}}$ be the validating/testing set.\n\n$$\n  \\mathrm{Err}(f) = E[(Y-f(X))^2] \\approx \\frac{1}{N_{Val}}\\sum_{i=1}^{N_{Val}}(Y_i-f(X_i))^2 =: \\mathrm{err}_{Val}(f).\n$$\n\nWhen $N_{Val}\\to\\infty$, $\\mathrm{err}_{Val}(f)\\to \\mathrm{Err}(f)$.\n\nWe can then do $$\n  \\arg\\min_{f\\in\\{f_1,\\dots,f_m\\}} \\mathrm{err}_{Val}(f)\n$$ If $N_{val}$ is large, we can have good estimate of $\\mathrm{Err}(f)$.\n\nBut actually, we may only have *small* validating set.\n\nProblems with simple Train-Test Split:\n\n-   Splitting 50-50 wastes data that could improve the model.\n\n-   Splitting 80-20 may leave too little test data for reliable evaluation.\n\n\nWhat we can do? **Cross-validation**!\n\nCross-validation uses **all** data efficiently for training and testing.\n\n## What is Cross-Validation?\n\nSuppose there are 5 folds ($K=5$).\n\n![](fig/yy/Training_ipad2.jpeg)\n\nPut together, we have\n\n$$\n  \\mathrm{err}_{cv}(f) = \\frac{1}{N}\\sum_{k=1}^5 \\sum_{i\\in S_k}(y_i-\\hat{f}^{[s_k^C]}(x_i))^2.\n$$\n\nThen cross-validation is to find $$\n  f^* = \\arg\\min_{f\\in\\{f_1,\\dots,f_m\\}} \\mathrm{err}_{cv}(f).\n$$\n\n## K-Fold Cross-Validation\n\nLet $\\mathcal{D}=\\{X_i,Y_i\\}_{i=1}^N$ be our data.\n\n::: {.callout-algorithm title=\"K-fold CV\"}\n1.  Split the data into K approximately equal sizes parts/fold $K$\n\n2.  For each $k=1,2,\\dots,K$, repeat the following steps:\n\n    (i) Leave the $k$th fold $S_k$ from the data $\\mathcal{D}$, and denote the remaining data as $S_k^C$. We fit the model to $S_k^C$ and denote the corresponding model we obtained by $\\hat{f}^{[S_k^C]}$\n    (ii) Calculate the total prediction error on the fitted model $\\hat{f}^{[S_k^C]}$ on the left-out fold $S_k$ $$\n           \\mathrm{err}_{cv,k}(f) = \\sum_{i\\in S_k} L(Y_i, \\hat{f}^{[S_k^C]}(X_i)).\n         $$\n\n3.  The **CV estimate** of prediction error is $$\n      \\mathrm{err}_{cv}(f) = \\frac{1}{N}\\sum_{k=1}^K \\mathrm{err}_{cv,k}(f).\n    $$\n:::\n\nSo if we have $M$ models, $f_1,f_2,\\dots,f_M$, we can use cross-validation to select the best model by computing the cross-validation error for each model $\\mathrm{err}_{cv}(f_1), \\mathrm{err}_{cv}(f_2),\\dots \\mathrm{err}_{cv}(f_M)$\n\nThen the best model is $$\n  f^*=\\arg\\min_{f\\in\\{f_1,\\dots,f_M\\}} \\mathrm{err}_{cv}(f).\n$$\n\nThere are two many use of the K-fold CV\n\n1.  Tune hyperparameters\n\n::: {.callout-example title=\"Hyperparameter Tuning using CV in linear regression\"}\nSuppose we have a family of models:\n\n$y=\\beta_0+\\sum_{j=1}^L\\beta_kx^k+\\varepsilon$, $L$ is the hyperparameter here.\n\nM1. $y = \\beta_0 + \\beta_1 x_1 + \\varepsilon$\n\nM2. $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x^2 + \\varepsilon$\n\nM3. $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x^2 + \\beta_3 x^3+ \\varepsilon$\n\nWe may use K-fold CV to choose the best $L$.\n:::\n\n2.  To better evaluate the performance of a model\n\nThe number of folds depends on the data size.\n\n## Discussion\n\n**Question**:\n\nWhat would you consider when choosing $K$?\\*\\*\n\nAnswer: The Bias-Variance Decomposition!\n\n\\begin{align*}\n\\text{Generalization error} &~=~ \\text{variance} &~+~& \\text{bias} &~+~& \\text{irreducible error} \\\\\n\\mathbb{E}_\\mathcal{T}[(y-f(x;\\mathcal{T}))^2] &~=~ \\mathbb{V}ar(x) &~+~& \\mathbb{B}\\mathrm{ias}^2(x) &~+~& \\varepsilon^2.\n\\end{align*}\n\nSee Section 7.3, Equation (7.9) Hastie et al. (2009).\n\n------------------------------------------------------------------------\n\n**Pop-up quiz**:\n\nQ: What are the characteristic a good model $f$ should have?\n\n1.  Low bias, high variance\n\n2.  High bias, low variance\n\n3.  Low bias, low variance\n\n4.  High bias, high variance\n\n5.  None of above\n\n------------------------------------------------------------------------\n\n<!-- ## Modeling steps (from Dubin) -->\n\n<!-- E. Steyerberg[^note3], an expert in model prediction and validation, recommends seven modeling steps: -->\n\n<!-- [^note3]:  Steyerberg, E.W. and Vergouwe, Y. (2014) Towards better clinical prediction models: seven steps for development and an ABCD for validation. European Heart Journal, 35, 1925--1931. -->\n\n<!-- 1. data inspection -->\n\n<!-- 2. coding of predictors (based on a priori plans) -->\n\n<!-- 3. model specification -->\n\n<!-- 4. model estimation -->\n\n<!-- 5. model performance -->\n\n<!-- 6. model validation -->\n\n<!-- 7. presentation of results -->\n\n<!-- We will be focusing mainly on (5) and (6). -->\n\n<!-- ## What is cross validation? -->\n\n<!-- Cross validation (CV) is a widely used technique to estimate **prediction error**.  -->\n\n<!-- One would like to think cross-validation estimates the **prediction error** for the model at hand, fit to the training data -->\n\n<!-- ##  -->\n\n<!-- \\begin{beamercolorbox}[rounded=true,shadow=true,sep=10pt]{customblock_pink} -->\n\n<!--     \\textbf{\\large Pop-up quiz}\\\\ -->\n\n<!--   Q: If you are given an arbitrary data set? What is the split ratio between training and testing data you would choose? -->\n\n<!-- \\vspace{0.5cm}  -->\n\n<!-- 1. 50-50 -->\n\n<!-- 2. 90-10 -->\n\n<!-- 3. 80-20 -->\n\n<!-- 4. 70-30 -->\n\n<!-- 5. Depends -->\n\n<!-- 6. Unsure -->\n\n<!-- \\end{beamercolorbox} -->\n\n<!-- ## Key Concepts: Bias and Variance -->\n\n<!-- Bias -->\n\n<!-- * Systematic error: The difference between predicted values and the true target. -->\n\n<!-- * Example: A model missing the target entirely. -->\n\n<!-- Variance -->\n\n<!-- * Sensitivity to data changes: How much predictions change with new data. -->\n\n<!-- * Example: Hitting different spots on the target each time. -->\n\n<!-- Goal -->\n\n<!-- * Minimize both bias and variance. -->\n\n![](fig/bias-variance.png). <!-- fig/bias-variance.png} -->\n\n-   Bias (Systematic error): The difference between predicted values and the true target.\n\n-   Variance (Sensitivity to data changes): How much predictions change with new data.\n\n-   Goal: Minimize both bias and variance.\n\n![Picture borrowed from [^05-cv-1].](fig/b-v-tradeoff)\n\n[^05-cv-1]: https://djsaunde.wordpress.com/2017/07/17/the-bias-variance-tradeoff/\n\n[^05-cv-2]: https://djsaunde.wordpress.com/2017/07/17/the-bias-variance-tradeoff/\n\n![](fig/fig2_crop.png)\n\nWith new data $(X,Y)$ from the same distribution:\n\n![](fig/fig3_crop.png)\n\n<!-- ## Prediction  -->\n\n<!-- Suppose we have a training data and testing data -->\n\n<!-- \\[ -->\n\n<!-- \\err = \\E[L(Y, \\hat{f}(X))], -->\n\n<!-- \\] -->\n\n<!-- the average generalization error when the method $\\hat{f}(X)$ is applied to an independent test sample from the joint distribution of $X$ and $Y$. -->\n\n<!-- ## Notation -->\n\n<!-- Let the training set that consist $N$ samples as  -->\n\n<!-- \\[ -->\n\n<!--   \\T = \\{(X_i, Y_i)\\}_{i=1}^N, -->\n\n<!-- \\] -->\n\n<!-- where -->\n\n<!-- * $X_i\\in \\R^P$ are the features. -->\n\n<!-- * $Y_i\\in \\R$ are the responses. -->\n\n<!-- We train the model, and use some *prediction rule* to determine $\\hat{Y}=\\hat{f}_\\T(X)$. -->\n\n<!-- ## Understanding K -Fold Cross-Validation {.allowframebreaks} -->\n\n<!-- There are two many use of the K-fold CV -->\n\n<!-- 1. Tune hyper parameters  -->\n\n<!-- 2. To better evaluate the performance of a model -->\n\n<!-- The number of folds depends on the data size. -->\n\n<!-- 1. The cross-validation approach involves randomly dividing the set of observations into $K$ groups, or folds, of approximately equal size.  -->\n\n<!-- 2. The first fold is treated as a validation set and the method is fit on the remaining $K-1$ folds -->\n\n<!-- 3. Repeat step 2 for $K$ times; each time, a different group of observations is treated as a validation set.  -->\n\n<!-- \\framebreak -->\n\n<!-- Let $\\T=\\{X_i,Y_i\\}_{i=1}^N$ be the training data. -->\n\n<!-- 1. Split the data into K roughly equal sizes parts $K$ -->\n\n<!-- 2. For each $k=1,2,\\dots,K$, repeat the following steps: -->\n\n<!--     (i) Leave the $k$th fold $\\T_k$ (i.e., part) from the data $\\T$, and denote the remaining data as $\\T_{-k}$. We fit the model to $\\T_{-k}$ and denote the corresponding model we obtained by $\\hat{f}_{\\T_{-k}}$ -->\n\n<!--     (ii) Calculate the total prediction error on the fitted model $\\hat{f}_{\\T_{-k}}$ on the left-out fold $\\T_k$ -->\n\n<!-- \\[ -->\n\n<!--   cv_k = \\sum_{i\\in \\T_k} L(Y_i, \\hat{f}_{\\T_{-k}}(X_i)). -->\n\n<!-- \\] -->\n\n<!-- 3. The **CV estimate** of prediction error is  -->\n\n<!-- \\[ -->\n\n<!--   \\widehat{\\err}_{cv} = \\frac{1}{K}\\sum_{k=1}^K cv_k. -->\n\n<!-- \\] -->\n\n## Choice of Fold K\n\nBias-Variance Tradeoff in CV:\n\nThe choice of K is a tradeoff between bias and variance.\n\n<!-- If we have our fitted model $\\hat{f}$, we then have -->\n\n<!-- \\begin{align*} -->\n\n<!-- \\mse(x_0)  -->\n\n<!--   &= \\E_\\T[f(x_0)-\\hat{y}_0]^2 \\\\ -->\n\n<!--   &= \\E_\\T[\\hat{y}_0-\\E_\\T(\\hat{y}_0)]^2 + \\{\\E_\\T(\\hat{y}_0-f(x_0)\\}^2\\\\ -->\n\n<!--   &= \\var_\\T(\\hat{y}_0) + \\bias^2(\\hat{y}_0) -->\n\n<!-- \\end{align*} -->\n\nQ: What values of $K, 2 \\leq K \\leq N$ should we use?\n\n-   Large $K$: **high variance**, but **small bias**.\n\n-   Small $K$: **low variance**, but **high bias**.\n\n<!-- training set. CV overestimates the test error for the model fit on -->\n\n<!--       the entire data set. Thus, the CV-estimate of prediction -->\n\n<!--       error is always biased high, and  -->\n\nBias decreases as $K$ increases.\n\n<!-- ##  -->\n\n<!-- An extreme case $K=N$:  -->\n\n<!-- * the CV estimator is approximately \\purple{unbiased} for the true -->\n\n<!-- (expected) prediction error, but can have \\purple{high variance} because the N “training sets” are so similar to one another. But computational burden! -->\n\n<!-- $K = 5$  -->\n\n<!-- * Cross-validation has \\purple{lower variance}. But \\purple{bias} -->\n\n<!-- could be a problem, depending on how the performance of the learning method varies with the size of the training set. -->\n\n## Example: Stock market\n\nWe look at the dataset in **Smarket** package in R.\n\nIt contains the daily percentage returns for the S&P 500 stock index between 2001 and 2005.\n\n$N = 1250$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ISLR2)\nlibrary(kableExtra)\n\nattach(Smarket)\n\n# Create the table and scale it to fit the page\nhead(Smarket) %>%\n  kable(\"latex\", caption =\" First Few Rows of Smarket Dataset\") %>%\n  kable_styling(full_width = FALSE, position = \"center\", font_size = 8)\n```\n\n::: {.cell-output-display}\n\\begin{table}\n\\centering\n\\caption{\\label{tab:unnamed-chunk-2} First Few Rows of Smarket Dataset}\n\\centering\n\\fontsize{8}{10}\\selectfont\n\\begin{tabular}[t]{r|r|r|r|r|r|r|r|l}\n\\hline\nYear & Lag1 & Lag2 & Lag3 & Lag4 & Lag5 & Volume & Today & Direction\\\\\n\\hline\n2001 & 0.381 & -0.192 & -2.624 & -1.055 & 5.010 & 1.1913 & 0.959 & Up\\\\\n\\hline\n2001 & 0.959 & 0.381 & -0.192 & -2.624 & -1.055 & 1.2965 & 1.032 & Up\\\\\n\\hline\n2001 & 1.032 & 0.959 & 0.381 & -0.192 & -2.624 & 1.4112 & -0.623 & Down\\\\\n\\hline\n2001 & -0.623 & 1.032 & 0.959 & 0.381 & -0.192 & 1.2760 & 0.614 & Up\\\\\n\\hline\n2001 & 0.614 & -0.623 & 1.032 & 0.959 & 0.381 & 1.2057 & 0.213 & Up\\\\\n\\hline\n2001 & 0.213 & 0.614 & -0.623 & 1.032 & 0.959 & 1.3491 & 1.392 & Up\\\\\n\\hline\n\\end{tabular}\n\\end{table}\n\n\n:::\n:::\n\n\n## Leave-One-Out Cross-Validation (LOOCV)\n\n-   When $K=N$, the size of the training data, it is leave-one-out cross validation.\n\n-   Instead of creating two subsets of comparable size, a single observation $(x_i, y_i)$ is used for the validation set and the remaining observations make up the training set.\n\n-   Repeat this for each observation and get the average.\n\n![Illustration for Leave one out CV.](fig/LOOCV.png){width=\"80%\"}\n\n![5 fold CV.](fig/yeying/pic5.png)\n\n![Result of the 5-fold CV with 10 runs.](fig/yeying/pic10.png)\n\n<!-- ## Cross validation may be used to choose the hyper parameter  -->\n\n<!-- * Linear Regression (\\purple{No hyper parameter}) -->\n\n<!-- * LASSO Regression -->\n\n<!-- * Ridge Regression -->\n\n<!-- * Random Forest -->\n\n<!-- * Gradient Boosting -->\n\n<!-- * Neural Networks -->\n\n<!-- Many of the models have something called **Hyperparameter**.  -->\n\n<!-- ##  -->\n\n<!-- Some models have hyperparameters that need to be tuned, some have only a few, some have more. -->\n\n<!-- For example, in LASSO regression, there is only 1 hyperparameter -->\n\n<!-- whereas in Random Forest, the hyperparameters are -->\n\n<!--   1. max depth  -->\n\n<!--   2. min sample split -->\n\n<!--   3. max terminal node -->\n\n<!--   4. min sample leaf -->\n\n<!--   5. ...  -->\n\n<!-- \\begin{beamercolorbox}[rounded=true,shadow=true,sep=10pt]{customblock} -->\n\n<!--     \\textbf{\\large }\\\\ -->\n\n<!--     CV can help in choosing the hyperparameter(s). -->\n\n<!-- \\end{beamercolorbox} -->\n\n## Take Home Messages\n\nWhat is CV?\n\n-   A method to estimate prediction error using all data efficiently.\n\nWhy K-fold?:\n\n-   Balances bias and variance effectively.\n\nLOOCV:\n\n-   Special case with K = N, unbiased but expensive.\n\nPractical Tips:\n\n-   K = 5 or K = 10 is common and (usually) works well.\n\n-   Use CV to tune hyperparameters and compare models.\n\nNote:\n\n-   Cross-validation can be applied in various contexts!\n\n::: {.callout-example title=\"Model selection using CV\"}\nEach panel reproduces your model fit:\n\n-   Linear: $y = \\beta_0 + \\beta_1 x$\n-   Quadratic: $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2$\n-   Exponential: $\\log y = \\beta_0 + \\beta_1 x$\n-   Log–Log: $\\log y = \\beta_0 + \\beta_1 \\log x$\n\nOnce the model is estimated, we want to assess the fit. CV can be used to estimate the prediction errors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DAAG); attach(ironslag)\nlibrary(ggplot2)\nlibrary(patchwork)\n\n# --- raw data ---\n  df <- data.frame(chemical, magnetic)\na  <- seq(10, 40, 0.1)\n\n# --- fits (using coefficients explicitly) ---\nL1 <- lm(magnetic ~ chemical, data = df)\nyhat1 <- L1$coef[1] + L1$coef[2] * a\n\nL2 <- lm(magnetic ~ chemical + I(chemical^2), data = df)\nyhat2 <- L2$coef[1] + L2$coef[2] * a + L2$coef[3] * a^2\n\nL3 <- lm(log(magnetic) ~ chemical, data = df)\nlogyhat3 <- L3$coef[1] + L3$coef[2] * a\nyhat3 <- exp(logyhat3)\n\nL4 <- lm(log(magnetic) ~ log(chemical), data = df)\nlogyhat4 <- L4$coef[1] + L4$coef[2] * log(a)\n\n# --- assemble data for plotting ---\nfits <- data.frame(\n  a = a,\n  yhat1 = yhat1,\n  yhat2 = yhat2,\n  yhat3 = yhat3,\n  loga  = log(a),\n  logyhat4 = logyhat4\n)\n\n# --- plots ---\np1 <- ggplot(df, aes(x = chemical, y = magnetic)) +\n  geom_point(shape = 16) +\n  geom_line(data = fits, aes(x = a, y = yhat1), linewidth = 1.1, color = \"steelblue\") +\n  ggtitle(\"Linear\") +\n  theme_bw(base_size = 13) +\n  theme(plot.title = element_text(hjust = 0.5))\n\np2 <- ggplot(df, aes(x = chemical, y = magnetic)) +\n  geom_point(shape = 16) +\n  geom_line(data = fits, aes(x = a, y = yhat2), linewidth = 1.1, color = \"darkgreen\") +\n  ggtitle(\"Quadratic\") +\n  theme_bw(base_size = 13) +\n  theme(plot.title = element_text(hjust = 0.5))\n\np3 <- ggplot(df, aes(x = chemical, y = magnetic)) +\n  geom_point(shape = 16) +\n  geom_line(data = fits, aes(x = a, y = yhat3), linewidth = 1.1, color = \"firebrick\") +\n  ggtitle(\"Exponential\") +\n  theme_bw(base_size = 13) +\n  theme(plot.title = element_text(hjust = 0.5))\n\np4 <- ggplot(df, aes(x = log(chemical), y = log(magnetic))) +\n  geom_point(shape = 16) +\n  geom_line(data = fits, aes(x = loga, y = logyhat4), linewidth = 1.1, color = \"purple\") +\n  ggtitle(\"Log–Log\") +\n  theme_bw(base_size = 13) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# --- combine in 2×2 grid ---\n(p1 + p2) / (p3 + p4)\n```\n\n::: {.cell-output-display}\n![](05-cv_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nattach(ironslag)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from ironslag (pos = 3):\n\n    chemical, magnetic\n```\n\n\n:::\n\n```{.r .cell-code}\nn <- length(magnetic) # in DAAG ironslag\ne1 <- e2 <- e3 <- e4 <- numeric(n)\n# for n-fold cross validation\n# fit models on leave-one-out samples\nfor (k in 1:n) {\n  y <- magnetic[-k]\n  x <- chemical[-k]\n  J1 <- lm(y ~ x)\n  yhat1 <- J1$coef[1] + J1$coef[2] * chemical[k]\n  e1[k] <- magnetic[k] - yhat1\n  \n  J2 <- lm(y ~ x + I(x^2))\n  yhat2 <- J2$coef[1] + J2$coef[2] * chemical[k] +\n    J2$coef[3] * chemical[k]^2\n  e2[k] <- magnetic[k] - yhat2\n  \n  J3 <- lm(log(y) ~ x)\n  logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[k]\n  yhat3 <- exp(logyhat3)\n  e3[k] <- magnetic[k] - yhat3\n  \n  J4 <- lm(log(y) ~ log(x))\n  logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[k])\n  yhat4 <- exp(logyhat4)\n  e4[k] <- magnetic[k] - yhat4\n}\n\n# compute MSEs\nmse <- c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2))\n\n# put into a tidy data frame\nresults <- data.frame(\n  Model = c(\"Linear\", \"Quadratic\", \"Exponential\", \"Log–Log\"),\n  MSE = round(mse, 4)\n)\n\n# display as a clean table\nknitr::kable(results, \n             caption = \"Model Comparison of Mean Squared Errors\", align = c(\"l\", \"c\"))\n```\n\n::: {.cell-output-display}\n\n\nTable: Model Comparison of Mean Squared Errors\n\n|Model       |   MSE   |\n|:-----------|:-------:|\n|Linear      | 19.5564 |\n|Quadratic   | 17.8525 |\n|Exponential | 18.4419 |\n|Log–Log     | 20.4542 |\n\n\n:::\n\n```{.r .cell-code}\nL2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = magnetic ~ chemical + I(chemical^2), data = df)\n\nCoefficients:\n  (Intercept)       chemical  I(chemical^2)  \n     24.49262       -1.39334        0.05452  \n```\n\n\n:::\n:::\n\n\nSo the best fitted model is $$\n\\hat{Y} = \n24.493 +\n-1.393\\,X +\n0.055\\,X^2\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2)) #layout for graphs\nplot(L2$fit, L2$res) #residuals vs fitted values\nabline(0, 0) #reference line\nqqnorm(L2$res) #normal probability plot\nqqline(L2$res) #reference line\npar(mfrow = c(1, 1)) #restore display\n```\n\n::: {.cell-output-display}\n![](05-cv_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n:::\n\n------------------------------------------------------------------------\n\n-   Note: This lecture is based on the book by Hastie et al. (2009), and James et al. (2013).\n\n-   Section 7.11 in Hastie, T., Tibshirani R. and Friedman, J. (2008). [The Elements of Statistical Learning](https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf). Springer, 2nd edition.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}