{
  "hash": "778d592b4e5bc09f74a90c41f3b2459d",
  "result": {
    "engine": "knitr",
    "markdown": "# Cross Validation\n\n\\newcommand{\\err}{\\mathrm{err}}\n\\newcommand{\\Err}{\\mathrm{Err}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\T}{\\mathcal{T}}\n\\newcommand{\\D}{\\mathcal{D}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\var}{\\mathbb{V}ar}\n\\newcommand{\\bias}{\\mathbb{B}\\mathrm{ias}}\n\\newcommand{\\mse}{\\mathrm{MSE}}\n\n\n\n\n## Introduction\n\nThis chapter covers resampling technique called *cross-validation*.\n\nCross-validation (CV) is a statistical method used to estimate the *skill* of machine learning (ML) models. It is commonly used in applied ML to compare and select a model for a given predictive modeling problem because it is easy to understand, easy to implement, and results in skill estimates that generally have a lower *bias* than other methods.\n\n\n## Resampling methods\n\n*Resampling methods* is a way to use the observed data to generate hypothetical samples. It treats an observed sample as a *finite population*, and random samples are generated/resampled from it to estimate population characteristics and make inferences about the sampled population. It is useful when:\n\n* Do not know the underlying distribution of a population\n\n* The formula may be difficult to be calculated.\n\nSome commonly used resampling methods include:\n\n-   **Bootstrap**: Bootstrap methods are often used when the distribution of the target population is not specified; the sample is the only information available.\n\n-   **Jackknife**: The jackknife is a resampling technique used to estimate the bias and variance of a statistic. It is like a leave-one-out (LOO) cross-validation.\n\n-   **Cross-validation**: Cross-validation is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.\n\n-   **Permutation tests**: Permutation tests are a type of non-parametric statistical test that involves rearranging the data points to test a hypothesis. They are used to determine whether the observed effect is statistically significant by comparing it to the distribution of effects obtained through random permutations of the data.\n\n\n\n## Machine Learning Models\n\n* **Left**: What machine learning can do\n\n* **Right**: Model/Methods\n\n::: columns\n::: column\n![](fig/ML_gpt.png){width=100%}\n:::\n::: column\n![](fig/ML_models.png){width=100%}\n:::\n:::\n\n\n## Training and testing/validating sets\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](fig/train_test.jpeg){fig-align='center' width=60%}\n:::\n:::\n\n\n**Questions**\\\nHow do we choose between different models $f_1,\\dots,f_m$?\n\n<!-- ## Data split -->\n\n<!-- * Train a model $\\hat{f}$ on the \\blue{training set}.  -->\n\n<!-- * Use the $\\hat{f}$ with the \\orange{features} to predict the \\red{outcomes}, $\\hat{y}$ -->\n\n<!-- * Difference between $\\red{y}$ and $\\hat{y}$. -->\n\n<!-- ![Data split.](fig/data-split.png){width=55%} -->\n\n<!-- ## Evaluation -->\n\n<!-- There are different metrics to evaluation the prediction. For instance -->\n\n<!-- If $y$ is continuous variable, we can use Mean-Square error. -->\n\n<!-- If $y$ is categorical, we can use the error rate.  -->\n\n<!-- ## Bias-Variance Tradeoff -->\n\n<!-- If we have a complex model, we may have many different tuning parameters.  -->\n<!-- \\begin{beamercolorbox}[rounded=true,shadow=true,sep=10pt]{customblock_blue} -->\n<!--     \\textbf{\\large Bias-Variance Tradeoff}\\\\ -->\n<!-- $$ -->\n<!-- \\mse[y,\\hat{y}] \\propto \\var(\\hat{y}) + \\bias^2(\\hat{y}) -->\n<!-- $$ -->\n<!-- \\end{beamercolorbox} -->\n\n## Setup\n\nSuppose we have a **supervised learning** model $f(X)\\to Y$.\n\nDenote the training set by $\\T=\\{(X_i,Y_i)\\}_{i=1}^{N_{train}}$.\n\nHow to choose between the models $f_1,\\dots,f_m$?\n\nIdeal: $(X,Y)\\sim F_{X,Y}$.\n\nDefine the generalization error (Population error) as\n$$\n  \\Err(f) = \\E_{X,Y}[\\{Y-f(X)\\}^2] \n$$\n\nChoose $f$ by \n$$\n  \\arg\\min_{f\\in\\{f_1,\\dots,f_m\\}} \\Err(f)\n$$\n\n\nSuppose we have a **supervised learning** model $f(X)\\to Y$.\n\nDenote the training set by $\\T=\\{(X_i,Y_i)\\}_{i=1}^{N_{train}}$.\n\nHow to choose between the models $f_1,\\dots,f_m$?\n\nIdeal: $(X,Y)\\sim F_{X,Y}$  (\\textbf{\\large \\purple{unknown}}).\n\nDefine the generalization error (Population error) as\n$$\n  \\Err(f) = E_{X,Y}[(Y-f(X))^2] \n$$\n\nChoose $f$ by \n$$\n  \\arg\\min_{f\\in\\{f_1,\\dots,f_m\\}} \\Err(f)\n$$\n\n\n**Question**:\n\nWhat to do if we do not know about $F_{X_Y}$?\n\n\n\nLet $V:=\\{(X_i,Y_i)\\}_{i=1}^{N_{Val}}$ be the validating/testing set.\n\n$$\n  \\Err(f) = E[(Y-f(X))^2] \\approx \\frac{1}{N_{Val}}\\sum_{i=1}^{N_{Val}}(Y_i-f(X_i))^2 =: \\err_{Val}(f).\n$$\n\nWhen $N_{Val}\\to\\infty$, $\\err_{Val}(f)\\to \\Err(f)$.\n\nWe can then do \n$$\n  \\arg\\min_{f\\in\\{f_1,\\dots,f_m\\}} \\err_{Val}(f)\n$$\nIf $N_{val}$ is large, we can have good estimate of $\\Err(f)$.\n\n\n\nBut actually, we may only have *small* validating set.\n\n\n\nProblems with simple Train-Test Split:\n\n* Splitting 50-50 wastes data that could improve the model.\n\n* Splitting 80-20 may leave too little test data for reliable evaluation.\n\n\\pause\nWhat we can do? \\purple{Cross-validation}!\n\t\nCross-validation uses **all** data efficiently for training and testing.\n\n## What is Cross-Validation?\n\nSuppose there are 5 folds ($K=5$).\n\n![](fig/yy/Training_ipad2.jpeg)\n\nPut together, we have\n\n$$\n  \\err_{cv}(f) = \\frac{1}{N}\\sum_{k=1}^5 \\sum_{i\\in S_k}(y_i-\\hat{f}^{[s_k^C]}(x_i))^2.\n$$\n\nThen cross-validation is to find\n$$\n  f^* = \\arg\\min_{f\\in\\{f_1,\\dots,f_m\\}} \\err_{cv}(f).\n$$\n\n## K-Fold Cross-Validation\n\nLet $\\D=\\{X_i,Y_i\\}_{i=1}^N$ be our data.\n\n1. Split the data into K approximately equal sizes parts/fold $K$\n\n2. For each $k=1,2,\\dots,K$, repeat the following steps:\n    (i) Leave the $k$th fold $S_k$ from the data $\\D$, and denote the remaining data as $S_k^C$. We fit the model to $S_k^C$ and denote the corresponding model we obtained by $\\hat{f}^{[S_k^C]}$\n    (ii) Calculate the total prediction error on the fitted model $\\hat{f}^{[S_k^C]}$ on the left-out fold $S_k$\n$$\n  \\err_{cv,k}(f) = \\sum_{i\\in S_k} L(Y_i, \\hat{f}^{[S_k^C]}(X_i)).\n$$\n\n3. The **CV estimate** of prediction error is \n$$\n  \\err_{cv}(f) = \\frac{1}{N}\\sum_{k=1}^K \\err_{cv,k}(f).\n$$\n\n\n---\n\nSo if we have $M$ models, $f_1,f_2,\\dots,f_M$, we can use cross-validation to select the best model by computing the cross-validation error for each model $\\err_{cv}(f_1), \\err_{cv}(f_2),\\dots \\err_{cv}(f_M)$\n\nThen the best model is \n$$\n  f^*=\\arg\\min_{f\\in\\{f_1,\\dots,f_M\\}} \\err_{cv}(f).\n$$\n\nThere are two many use of the K-fold CV\n\n1. Tune hyperparameters\n  \n    e.g., In linear regression: $y=\\beta_0+\\sum_{j=1}^L\\beta_kx^k+\\varepsilon$, $L$ is the hyperparameter here.\n    \n    M1. $y = \\beta_0 + \\beta_1 x_1 + \\varepsilon$\n  \n    M2. $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x^2 + \\varepsilon$\n  \n    M3. $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x^2 + \\beta_3 x^3+ \\varepsilon$\n\n2. To better evaluate the performance of a model\n\nThe number of folds depends on the data size.\n\n## Discussion \n\n**Question**: \n\nWhat would you consider when choosing $K$?**\n\nAnswer: The Bias-Variance Decomposition!\n\n\\begin{align*}\n\\text{Generalization error} &~=~ \\text{variance} &~+~& \\text{bias} &~+~& \\text{irreducible error} \\\\\n\\E_\\T[(y-f(x;\\T))^2] &~=~ \\var(x) &~+~& \\bias^2(x) &~+~& \\varepsilon^2.\n\\end{align*}\n\nSee Section 7.3, Equation (7.9) Hastie et al. (2009).\n\n---\n \n**Pop-up quiz**:\n  \nQ: What are the characteristic a good model $f$ should have?\n\n1. Low bias, high variance\n\n2. High bias, low variance\n\n3. Low bias, low variance\n\n4. High bias, high variance\n\n5. None of above\n    \n---\n\n<!-- ## Modeling steps (from Dubin) -->\n\n<!-- E. Steyerberg[^note3], an expert in model prediction and validation, recommends seven modeling steps: -->\n\n<!-- [^note3]:  Steyerberg, E.W. and Vergouwe, Y. (2014) Towards better clinical prediction models: seven steps for development and an ABCD for validation. European Heart Journal, 35, 1925--1931. -->\n\n<!-- 1. data inspection -->\n\n<!-- 2. coding of predictors (based on a priori plans) -->\n\n<!-- 3. model specification -->\n\n<!-- 4. model estimation -->\n\n<!-- 5. model performance -->\n\n<!-- 6. model validation -->\n\n<!-- 7. presentation of results -->\n\n<!-- We will be focusing mainly on (5) and (6). -->\n\n<!-- ## What is cross validation? -->\n\n<!-- Cross validation (CV) is a widely used technique to estimate **prediction error**.  -->\n\n<!-- One would like to think cross-validation estimates the **prediction error** for the model at hand, fit to the training data -->\n\n\n\n<!-- ##  -->\n\n<!-- \\begin{beamercolorbox}[rounded=true,shadow=true,sep=10pt]{customblock_pink} -->\n<!--     \\textbf{\\large Pop-up quiz}\\\\ -->\n<!--   Q: If you are given an arbitrary data set? What is the split ratio between training and testing data you would choose? -->\n\n<!-- \\vspace{0.5cm}  -->\n\n<!-- 1. 50-50 -->\n\n<!-- 2. 90-10 -->\n\n<!-- 3. 80-20 -->\n\n<!-- 4. 70-30 -->\n\n<!-- 5. Depends -->\n\n<!-- 6. Unsure -->\n\n<!-- \\end{beamercolorbox} -->\n\n<!-- ## Key Concepts: Bias and Variance -->\n\n<!-- Bias -->\n\n<!-- * Systematic error: The difference between predicted values and the true target. -->\n<!-- * Example: A model missing the target entirely. -->\n\n<!-- Variance -->\n\n<!-- * Sensitivity to data changes: How much predictions change with new data. -->\n\n<!-- * Example: Hitting different spots on the target each time. -->\n\n\n<!-- Goal -->\n\n<!-- * Minimize both bias and variance. -->\n\n<!-- ![](fig/bias-variance.png). -->\n\n\n\\begin{multicols}{2}\n\n  \\null \\vfill\n  \\includegraphics[width=.58\\textwidth]{fig/bias-variance.png}\n  \\vfill \\null\n\n\\columnbreak\n\n  \\null \\vfill\n  \\begin{itemize}\n    \\item Bias (Systematic error): The difference between predicted values and the true target.\n    \\item Variance (Sensitivity to data changes): How much predictions change with new data.\n    \\item Goal: Minimize both bias and variance.\n  \\end{itemize}\n  \\vfill \\null\n\\end{multicols}\n\n\n![Picture borrowed from [^note6].  ](fig/b-v-tradeoff)\n\n[^note6]: https://djsaunde.wordpress.com/2017/07/17/the-bias-variance-tradeoff/\n\n\n![](fig/fig2_crop.png)\n\nWith new data $(X,Y)$ from the same distribution:\n\n![](fig/fig3_crop.png)\n\n\n\n<!-- ## Prediction  -->\n\n<!-- Suppose we have a training data and testing data -->\n<!-- \\[ -->\n<!-- \\err = \\E[L(Y, \\hat{f}(X))], -->\n<!-- \\] -->\n\n<!-- the average generalization error when the method $\\hat{f}(X)$ is applied to an independent test sample from the joint distribution of $X$ and $Y$. -->\n\n<!-- ## Notation -->\n\n<!-- Let the training set that consist $N$ samples as  -->\n<!-- \\[ -->\n<!--   \\T = \\{(X_i, Y_i)\\}_{i=1}^N, -->\n<!-- \\] -->\n<!-- where -->\n\n<!-- * $X_i\\in \\R^P$ are the features. -->\n<!-- * $Y_i\\in \\R$ are the responses. -->\n\n<!-- We train the model, and use some *prediction rule* to determine $\\hat{Y}=\\hat{f}_\\T(X)$. -->\n\n<!-- ## Understanding K -Fold Cross-Validation {.allowframebreaks} -->\n\n<!-- There are two many use of the K-fold CV -->\n\n<!-- 1. Tune hyper parameters  -->\n<!-- 2. To better evaluate the performance of a model -->\n\n<!-- The number of folds depends on the data size. -->\n\n\n<!-- 1. The cross-validation approach involves randomly dividing the set of observations into $K$ groups, or folds, of approximately equal size.  -->\n<!-- 2. The first fold is treated as a validation set and the method is fit on the remaining $K-1$ folds -->\n\n<!-- 3. Repeat step 2 for $K$ times; each time, a different group of observations is treated as a validation set.  -->\n\n<!-- \\framebreak -->\n\n<!-- Let $\\T=\\{X_i,Y_i\\}_{i=1}^N$ be the training data. -->\n\n<!-- 1. Split the data into K roughly equal sizes parts $K$ -->\n\n\n<!-- 2. For each $k=1,2,\\dots,K$, repeat the following steps: -->\n<!--     (i) Leave the $k$th fold $\\T_k$ (i.e., part) from the data $\\T$, and denote the remaining data as $\\T_{-k}$. We fit the model to $\\T_{-k}$ and denote the corresponding model we obtained by $\\hat{f}_{\\T_{-k}}$ -->\n<!--     (ii) Calculate the total prediction error on the fitted model $\\hat{f}_{\\T_{-k}}$ on the left-out fold $\\T_k$ -->\n<!-- \\[ -->\n<!--   cv_k = \\sum_{i\\in \\T_k} L(Y_i, \\hat{f}_{\\T_{-k}}(X_i)). -->\n<!-- \\] -->\n\n<!-- 3. The **CV estimate** of prediction error is  -->\n<!-- \\[ -->\n<!--   \\widehat{\\err}_{cv} = \\frac{1}{K}\\sum_{k=1}^K cv_k. -->\n<!-- \\] -->\n\n## Choice of Fold K\n\n\\begin{beamercolorbox}[rounded=true,shadow=true,sep=10pt]{customblock_blue}\n    \\textbf{\\large Bias-Variance Tradeoff}\\\\\n    \\vspace{0.1in}\n    The choice of K is a tradeoff between bias and variance.\n\\end{beamercolorbox}\n\n<!-- If we have our fitted model $\\hat{f}$, we then have -->\n<!-- \\begin{align*} -->\n<!-- \\mse(x_0)  -->\n<!--   &= \\E_\\T[f(x_0)-\\hat{y}_0]^2 \\\\ -->\n<!--   &= \\E_\\T[\\hat{y}_0-\\E_\\T(\\hat{y}_0)]^2 + \\{\\E_\\T(\\hat{y}_0-f(x_0)\\}^2\\\\ -->\n<!--   &= \\var_\\T(\\hat{y}_0) + \\bias^2(\\hat{y}_0) -->\n<!-- \\end{align*} -->\n  \n\\pause\nQ: What values of $K, 2 \\leq K \\leq N$ should we use?\n\\pause\n\n* Large $K$: **high variance**, but **small bias**.\n\n* Small $K$: **low variance**, but **high bias**.\n\n\n<!-- training set. CV overestimates the test error for the model fit on -->\n<!--       the entire data set. Thus, the CV-estimate of prediction -->\n<!--       error is always biased high, and  -->\n\n      \nBias decreases as $K$ increases.\n\n<!-- ##  -->\n\n<!-- An extreme case $K=N$:  -->\n\n<!-- * the CV estimator is approximately \\purple{unbiased} for the true -->\n<!-- (expected) prediction error, but can have \\purple{high variance} because the N “training sets” are so similar to one another. But computational burden! -->\n\n<!-- $K = 5$  -->\n\n<!-- * Cross-validation has \\purple{lower variance}. But \\purple{bias} -->\n<!-- could be a problem, depending on how the performance of the learning method varies with the size of the training set. -->\n\n\n## Example: Stock market \n\nWe look at the dataset in **Smarket** package in R.\n\nIt contains the daily percentage returns for the S&P 500 stock index between 2001 and 2005.\n\n$N = 1250$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ISLR2)\nlibrary(kableExtra)\n\nattach(Smarket)\n\n# Create the table and scale it to fit the page\nhead(Smarket) %>%\n  kable(\"latex\", caption =\" First Few Rows of Smarket Dataset\") %>%\n  kable_styling(full_width = FALSE, position = \"center\", font_size = 8)\n```\n\n::: {.cell-output-display}\n\\begin{table}\n\\centering\n\\caption{\\label{tab:unnamed-chunk-2} First Few Rows of Smarket Dataset}\n\\centering\n\\fontsize{8}{10}\\selectfont\n\\begin{tabular}[t]{r|r|r|r|r|r|r|r|l}\n\\hline\nYear & Lag1 & Lag2 & Lag3 & Lag4 & Lag5 & Volume & Today & Direction\\\\\n\\hline\n2001 & 0.381 & -0.192 & -2.624 & -1.055 & 5.010 & 1.1913 & 0.959 & Up\\\\\n\\hline\n2001 & 0.959 & 0.381 & -0.192 & -2.624 & -1.055 & 1.2965 & 1.032 & Up\\\\\n\\hline\n2001 & 1.032 & 0.959 & 0.381 & -0.192 & -2.624 & 1.4112 & -0.623 & Down\\\\\n\\hline\n2001 & -0.623 & 1.032 & 0.959 & 0.381 & -0.192 & 1.2760 & 0.614 & Up\\\\\n\\hline\n2001 & 0.614 & -0.623 & 1.032 & 0.959 & 0.381 & 1.2057 & 0.213 & Up\\\\\n\\hline\n2001 & 0.213 & 0.614 & -0.623 & 1.032 & 0.959 & 1.3491 & 1.392 & Up\\\\\n\\hline\n\\end{tabular}\n\\end{table}\n\n\n:::\n:::\n\n\n\n## Leave-One-Out Cross-Validation (LOOCV)\n\n* When $K=N$, the size of the training data, it is leave-one-out cross validation.\n\n* Instead of creating two subsets of comparable size, a single observation $(x_i, y_i)$ is used for the validation set and the remaining observations make up the training set. \n   \n* Repeat this for each observation and get the average.\n\n![Illustration for Leave one out CV.](fig/LOOCV.png){width=80%}\n\n\n\n![5 fold CV.](fig/yeying/pic5.png)\n\n\n<!-- ![Result of the 5-fold CV with 10 runs.](fig/yeying/pic10.pdf) -->\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Result of the 5-fold CV with 10 runs.](fig/yeying/pic10.pdf){fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n\n\n<!-- ## Cross validation may be used to choose the hyper parameter  -->\n\n\n<!-- * Linear Regression (\\purple{No hyper parameter}) -->\n<!-- * LASSO Regression -->\n<!-- * Ridge Regression -->\n<!-- * Random Forest -->\n<!-- * Gradient Boosting -->\n<!-- * Neural Networks -->\n\n<!-- Many of the models have something called **Hyperparameter**.  -->\n\n<!-- ##  -->\n\n<!-- Some models have hyperparameters that need to be tuned, some have only a few, some have more. -->\n\n\n<!-- For example, in LASSO regression, there is only 1 hyperparameter -->\n\n\n<!-- whereas in Random Forest, the hyperparameters are -->\n\n<!--   1. max depth  -->\n<!--   2. min sample split -->\n<!--   3. max terminal node -->\n<!--   4. min sample leaf -->\n<!--   5. ...  -->\n\n<!-- \\begin{beamercolorbox}[rounded=true,shadow=true,sep=10pt]{customblock} -->\n<!--     \\textbf{\\large }\\\\ -->\n<!--     CV can help in choosing the hyperparameter(s). -->\n<!-- \\end{beamercolorbox} -->\n\n\n## Take Home Messages\n\nWhat is CV?\n\n  * A method to estimate prediction error using all data efficiently.\n\nWhy K-fold?:\n\n* Balances bias and variance effectively.\n\nLOOCV:\n\n* Special case with K = N, unbiased but expensive.\n\nPractical Tips:\n\n* K = 5  or  K = 10  is common and (usually) works well.\n\n* Use CV to tune hyperparameters and compare models.\n\nNote:  \n\n- Cross-validation can be applied in various contexts!\n\n---\n\n- Note: This lecture is based on the book by Hastie et al. (2009), and James et al. (2013).\n\n- Section 7.11 in Hastie, T., Tibshirani R. and Friedman, J. (2008). [The Elements of Statistical Learning](https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf). Springer, 2nd edition.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}