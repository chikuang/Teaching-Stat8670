{
  "hash": "e3995de4b4a4e058e1b22b36474690d6",
  "result": {
    "engine": "knitr",
    "markdown": "# Numerical Approaches and Optimization in 1-D\n\nThe optimization plays an important role in statistical computing, especially in the context of maximum likelihood estimation (MLE) and other statistical inference methods. This chapter will cover various optimization techniques used in statistical computing.\n\nThere is a general principle that will be repeated in this chapter that Kenneth Lange calls *optimization transfer* in his 1999 paper. The basic idea applies to the problem of maximizing a function $f$.\n\n1.  Direct optimize the function $f$.\n    -   It can be difficult\n2.  Optimize a surrogate function $g$ that is easier to optimize than $f$.\n3.  So here, instead of optimize $f$, we optimize $g$.\n\nNote 1: steps 2&3 are repeated until convergence.\n\nNote 2: maximizing $f$ is equivalent to minimizing $-f$.\n\nNote 3: the surrogate function $g$ should be chosen such that it is easier to optimize than $f$.\n\nFor instance, for a linear regression \\begin{equation}\n  y = X\\boldsymbol{\\beta} + \\varepsilon. \\label{eq:linmod}\n\\end{equation}\n\nFrom regression class, we know that the (ordinary) least-squares estimation (OLE) for $\\boldsymbol{\\beta}$ is given by $\\hat{\\boldsymbol{\\beta}}=(X^\\top X)^{-1} X^\\top y$. It is convenient as the solution is in the **closed-form**! However, in the most case, the closed-form solutions will not be available.\n\nFor GLMs or non-linear regression, we need to do this **iterativelly**!\n\n## Theory versus Computation\n\nOne confusing aspect of statistical computing is that often there is a disconnect between what is printed in a statistical computing textbook and what should be implemented on the computer.\n\n-   In textbooks, simpler to **present solutions as convenient mathematical formulas whenever possible**, in order to communicate basic ideas and to provide some insight.\n    -   However, directly translating these formulas into computer code is usually not advisable because there are many problematic aspects of computers that are simply not relevant when writing things down on paper.\n\nSome potential issues includ:\n\n1.  Memory overflow: The computer has a limited amount of memory, and it is possible to run out of memory when working with large datasets or complex models.\n\n2.  Numerical Precision: Sometimes, due to the cut precision of floating-point arithmetic, calculations that are mathematically equivalent can yield different results on a computer.\n\n    -   Example 1: round $1/3$ to two decimal places, we get $0.33$. Then, $3 \\cdot (1/3)$ is exactly $1$, but $3 \\cdot 0.33$ is $0.99$.\n    -   Example 2: $1 - 0.99999999$ is $0.00000001$ (=1E-8), but if we round $0.99999999$ to two decimal places, we get $1.00$, and then $1 - 1.00$ is $0$. If we round $0.00000001$ to two decimal places, we get $0.00$.\n    -   Example 3: $\\pi$\n\n3.  (Lienar) Dependence: The detection of linear dependence in matrix computations is influenced by machine precision. Since computers operate with finite precision, situations often arise where true linear dependence exists, but the computer cannot distinguish it from independence.\n\n    -   Example: Consider the matrix $$\n         A = \\begin{pmatrix}\n         1 & 2 & 3 \\\\\n         4 & 5 & 6 \\\\\n         7 & 8 & 9 \\\\\n         \\end{pmatrix}\n         $$ The 3rd column is a linear combination of the first two columns (i.e., col3 = col1 + col2). However, due to machine precision limitations, the computer might not recognize this exact linear dependence, leading to numerical instability in computations involving this matrix. With a small distortion, we have $$\n         B = \\begin{pmatrix}\n         1 & 2 & 3 \\\\\n         4 & 5 & 6 \\\\\n         7 & 8 & 9 + 10^{-5} \\\\\n         \\end{pmatrix}\n         $$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(\n  c(1, 2, 3,\n    4, 5, 6,\n    7, 8, 9),\n  nrow = 3, ncol = 3, byrow = TRUE)\nB <- A\nB[3, 3] <- B[3, 3] + 1E-5\n\nqr(A)$rank\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2\n```\n\n\n:::\n\n```{.r .cell-code}\nqr(B)$rank\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3\n```\n\n\n:::\n:::\n\n\n## Matrix Inversion\n\nIn many statistical analyses, such as linear regression and specify the distribution (such as normal distribution), matrix inversion plays a central role.\n\n### Example 1: Normal distribution\n\nWe know that, a normal density with the parameters mean $\\mu$ and standard deviation $\\sigma$ is $$\nf\\left(x \\mid \\mu, \\sigma^2\\right)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\left\\{-\\frac{1}{2 \\sigma^2}(x-\\mu)^2\\right\\}\n$$ or we may work on the multivariate normal distribution case which is a bit more involved.\n\n::: {.callout-definition title=\"Multivariate normal distribution\"}\n$\\boldsymbol{X} = (X_1,\\dots, X_d)$ is said to be a multivariate normal distribution if and only if it is a linear comibnation of independent and identically distributed standard normals: $$\n\\boldsymbol{X} = \\boldsymbol{CZ} + \\mu,\\quad \\boldsymbol{Z}=(Z_1,\\dots,Z_d),\\quad Z_i \\stackrel{iid}{\\sim} N(0,1).\n$$\n:::\n\nThe property of the multivariate normal are:\n\n-   mean vector: $E(\\boldsymbol{X}) = \\mu$\n-   variance: $Var(\\boldsymbol{X}) = \\boldsymbol{CZC}^\\top = \\boldsymbol{C} var(\\boldsymbol{Z})\\boldsymbol{C}^\\top:=  \\boldsymbol{\\Sigma}$\n\nNotation: $\\boldsymbol{X} \\sim N(\\mu, \\boldsymbol{\\Sigma})$.\n\nPDF: $$\nf(\\boldsymbol{x} \\mid \\mu, \\Sigma)=(2 \\pi)^{-d / 2} \\cdot \\exp \\left\\{-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^{\\prime} \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})-\\frac{1}{2} \\log |\\boldsymbol{\\Sigma}|\\right\\}.\n$$ Some of the potential ways to do this is to take logarithm of the PDF (Think about why).\n\n### Example 2: Linear regression\n\nRecall the linear regression model . The OLE for $\\boldsymbol{\\beta}$ is given by $\\hat{\\boldsymbol{\\beta}}=(X^\\top X)^{-1} X^\\top y$.\n\nWe can solve this using the R command\n\n``` r\nbeta_hat <- solve(t(X) %*% X) %*% t(X) %*% y\n```\n\nwhere `solve()` is the R function for matrix inversion. However, it is not a desired way (think about why).\n\nA better way is to go back to the formula, and look at $$\nX^\\top X\\boldsymbol{\\beta}= X^\\top y,\n$$ and solve this using the R command\n\n``` r\nsolve( crossprod(X), crossprod(X, y) ) \n# this is the same as \n# solve(t(X) %*% X, t(X) %*% y)\n```\n\nHere, we avoid explicitly calculating the inverse of $X^\\top X$. Instead, we use gaussian elimination to solve the system of equations, which is generally more numerically stable and efficient.\n\n#### Speed comparison\n\n``` r\nset.seed(2025-09-03)\nX <- matrix(rnorm(5000 * 100), 5000, 100)\ny <- rnorm(5000)\nlibrary(microbenchmark)\nmicrobenchmark(solve(t(X) %*% X) %*% t(X) %*% y)\n```\n\n```         \nUnit: milliseconds\n                             expr      min       lq\n solve(t(X) %*% X) %*% t(X) %*% y 28.83505 30.16593\n     mean   median       uq      max neval\n 31.96782 30.79489 32.63315 111.0151   100\nWarning message:\nIn microbenchmark(solve(t(X) %*% X) %*% t(X) %*% y) :\n  less accurate nanosecond times to avoid potential integer overflows\n```\n\n``` r\nmicrobenchmark(solve(t(X) %*% X) %*% t(X) %*% y,\n               solve(crossprod(X), crossprod(X, y)))\n```\n\n```         \nUnit: milliseconds\n                                 expr      min       lq\n     solve(t(X) %*% X) %*% t(X) %*% y 28.90135 30.11608\n solve(crossprod(X), crossprod(X, y)) 25.05859 25.27480\n     mean   median       uq      max neval\n 31.78686 31.38513 32.66482 53.03354   100\n 26.15771 25.81678 26.89188 29.12045   100\n```\n\n### Take home message:\n\nThe take home here is that the issues arise from the finite precision of computer arithmetic and the limited memory available on computers. When implementing statistical methods on a computer, it is crucial to consider these limitations and choose algorithms and implementations that are robust to numerical issues.\n\n### Multi-collinearity\n\nThe above approach may break down when there is any multi-colinearity in the $\\boldsymbol{X}$ matrix. For example, we can tack on a column to $\\boldsymbol{X}$ that is very similar (but not identical) to the first column of $\\boldsymbol{X}$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(7777)\nN <- 3000\nK <- 100\ny <- rnorm(N)\nX <- matrix(rnorm(N * K), N, K)\nW <- cbind(X, X[, 1] + rnorm(N, sd = 1E-15))\n```\n:::\n\n\n``` r\nsolve(crossprod(W), crossprod(W, y))\n\nError in `solve.default()`:\n! system is computationally singular: reciprocal condition number = 1.36748e-32\n```\n\nThe algorithm does not work because the cross product matrix $W^\\top W$ is **singular**. In practice, matrices like these can come up a lot in data analysis and it would be useful to have a way to deal with it automatically.\n\nR takes a different approach to solving for the unknown coefficients in a linear model. R uses the QR decomposition, which is not as fast, but has the added benefit of being able to automatically detect and handle colinear columns in the matrix.\n\nHere, we use the fact that X can be decomposed as $\\boldsymbol{X}=QR$, where $Q$ is an orthonormal matrix and $R$ is an upper triangular matrix. Given that, we can rewrite $X^\\top X \\boldsymbol{\\beta}= X^\\top y$ as \\begin{align*}\nR^\\top Q^\\top Q R \\boldsymbol{\\beta}&= R^\\top Q^\\top y\\\\\nR^\\top I R \\boldsymbol{\\beta}&= R^\\top Q^\\top y\\\\\nR^\\top R \\boldsymbol{\\beta}&= R^\\top Q^\\top y,\n\\end{align*} this leads to $R\\boldsymbol{\\beta}= Q^\\top y$. Now we can perform the Gaussian elimination to do it. Because $R$ is an upper triangular matrix, the computational speed is much faster. Here, we **avoid to compute the cross product** $X^\\top X$, which is numerical unstable if it is not *standardized* properly\n\nWe can see in R code that even with our singular matrix $W$ above, the QR decomposition continues without error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nQw <- qr(W)\nstr(Qw)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 4\n $ qr   : num [1:3000, 1:101] 54.43933 0.00123 -0.02004 -0.00671 -0.00178 ...\n $ rank : int 100\n $ qraux: num [1:101] 1.01 1.01 1.01 1 1 ...\n $ pivot: int [1:101] 1 2 3 4 5 6 7 8 9 10 ...\n - attr(*, \"class\")= chr \"qr\"\n```\n\n\n:::\n:::\n\n\nNote that the output of `qr()` computes the rank of $W$ to be 100, not 101 as the last column is collinear to the 1st column. From there, we can get $\\hat{\\boldsymbol{\\beta}}$ if we want using `qr.coef()`,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbetahat <- qr.coef(Qw, y)\nhead(betahat, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  0.024314718  0.000916951 -0.005980588\n```\n\n\n:::\n\n```{.r .cell-code}\ntail(betahat, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  0.01545039 -0.01010440          NA\n```\n\n\n:::\n:::\n\n\nQ: Why there is an `NA`?\n\n### Trade-off\n\nThere isn’t always elegance and flourish. When we take the robust approach, we accept that it comes at a cost.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(microbenchmark)\nm <- microbenchmark(solve(t(X) %*% X) %*% t(X) %*% y,\n                    solve(crossprod(X), crossprod(X, y)),\n                    qr.coef(qr(X), y))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in microbenchmark(solve(t(X) %*% X) %*% t(X) %*% y, solve(crossprod(X),\n: less accurate nanosecond times to avoid potential integer overflows\n```\n\n\n:::\n\n```{.r .cell-code}\nautoplot(m)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\ni Please use tidy evaluation idioms with `aes()`.\ni See also `vignette(\"ggplot2-in-packages\")` for more information.\ni The deprecated feature was likely used in the microbenchmark package.\n  Please report the issue at\n  <https://github.com/joshuaulrich/microbenchmark/issues/>.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](02-optimization_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nCompared with the approaches discussed above, this method performs similarly to the naive approach but is much more stable and reliable.\n\nIn practice, we rarely call functions such as `qr()` or `qr.coef()` directly, since higher-level functions like lm() handle these computations automatically. However, in certain specialized and performance-critical settings, it can be advantageous to use alternative matrix decompositions to compute regression coefficients, especially when the computation must be repeated many times in a loop (i.e., *Vectorization*)\n\n### Multivariate Normal revisit\n\nComputing the multivariate normal (MVN) density is a common task, for example, when fitting spatial models or Gaussian process models. Because maximum likelihood estimation(MLE) and likelihood ratio tests (LRT) often require evaluating the likelihood many times, efficiency is crucial.\n\nAfter taking the log of the MVN density, we have\n\n$$\n\\ell(\\boldsymbol{x}\\mid \\boldsymbol{\\mu},\\Sigma) := \\log \\left\\{ f(\\boldsymbol{x}\\mid \\boldsymbol{\\mu},\\Sigma) \\right\\} \n= -\\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\Sigma| - \\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^\\top \\Sigma^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu}).\n$$ On the right hand side, the first term is a constant, the second term is linear, and the last term is quadratic, which requires much more computational power.\n\n#### A Naive Implementation\n\nWe first center the data $\\boldsymbol{z}:=\\boldsymbol{x}- \\mu$. Then we have $\\boldsymbol{z}^\\top \\Sigma^{-1} \\boldsymbol{z}$. This simiplified the question for a bit.\n\nHere, much like the linear regression example above, the key bottleneck is the inversion of the $p$-dimensional covariance matrix $\\Sigma$. If we take $\\boldsymbol{z}$ to be a $p\\times 1$ column vector, then a literal translation of the mathematics into R code might look something like this,\n\n``` r\nt(z) %*% solve(Sigma) %*% z\n```\n\nTo illustrate, let’s simulate some data and compute the quadratic form the naive way:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2025-09-03)\n\n# Generate data\nz <- matrix(rnorm(200 * 100), 200, 100)\nS <- cov(z)\n\n# Naive quadratic form\nquad.naive <- function(z, S) {\n  Sinv <- solve(S)\n  rowSums((z %*% Sinv) * z)\n}\n\nlibrary(dplyr)\nquad.naive(z, S) %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  70.67   93.61   99.94  100.54  107.31  126.73 \n```\n\n\n:::\n:::\n\n\n#### A Better Way: Cholesky Decomposition\n\nBecause the covariance matrix \\Sigma is symmetric and positive definite, we can exploit its **Cholesky decomposition**. That is, we write $\\Sigma = R^\\top R$, where $R$ is a upper triangular matrix. Then, $$\n\\boldsymbol{z}^\\top \\Sigma^{-1} \\boldsymbol{z}= \\boldsymbol{z}^\\top (R^\\top R)^{-1} \\boldsymbol{z}= \\boldsymbol{z}^\\top R^{-1}R^{-\\top} \\boldsymbol{z}= (R^{-\\top}\\boldsymbol{z})^\\top (R^{-\\top} \\boldsymbol{z}) := \\boldsymbol{v}^\\top \\boldsymbol{v}.\n$$ Note that $\\boldsymbol{v}\\in \\mathbb R^p$ is the solution to the linear system $R^\\top \\boldsymbol{v}= \\boldsymbol{z}$. Because $R$ is upper triangular, we can solve this system efficiently using back substitution. Also, we can solve this without doing the inversion.\n\nOnce we have $\\boldsymbol{v}$ we can compute its quadratic form $\\boldsymbol{v}^\\top \\boldsymbol{v}$ by the `crossprod()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquad.chol <- function(z, S) {\n  R <- chol(S)\n  v <- backsolve(R, t(z), transpose = TRUE)\n  colSums(v * v)\n}\n\nquad.chol(z, S) %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  70.67   93.61   99.94  100.54  107.31  126.73 \n```\n\n\n:::\n:::\n\n\n#### By product\n\nAnother benefit of the Cholesky decomposition is that it gives us a simple way to compute the log-determinant of $\\Sigma$. The log-determinant of $\\Sigma$ is simply two times the sum of the log of the diagonal elements of R. (Why?)\n\n#### Performance comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(microbenchmark)\nlibrary(ggplot2)\nm2 <- microbenchmark(\n  naive = quad.naive(z, S),\n  chol  = quad.chol(z, S)\n)\nautoplot(m2)\n```\n\n::: {.cell-output-display}\n![](02-optimization_files/figure-pdf/unnamed-chunk-8-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nQ: Why one is faster than the other?\n\n#### Take home message 2\n\nThe naive algorithm simply inverts the covariance matrix. The Cholesky-based approach, on the other hand, exploits the fact that covariance matrices are symmetric and positive definite. This results in an implementation that is both faster and numerically more stable—exactly the kind of optimization that makes a difference in real-world statistical computing.\n\nThus, a knowledge of statistics and numerical analysis can often lead to better algorithms, often invaluable!\n\n------------------------------------------------------------------------\n\n## Nonlinear functions\n\nOn the top, we have *linear functions*, such as $y=f(x) = ax + b$ or in the linear regression $y=X\\beta +\\epsilon$. It is a small class of the functions, and may be relatively limited.\n\nE.g., what if we have a quadratic relationship? Then $y=f(x) = ax^2 + bx + c$.\n\nSuch nonlinear relationship is very common, , such as $f(x) = a\\sin(bx + c)$ or $f(x) = a\\exp(bx) + c$, and they may not have a closed-form solution like in the linear regression case.\n\nFrom now on, we will be talking about the numerical approaches to solve these problems.\n\n## Type of Optimization Algorithms\n\nThere are in general two types of the optimization algorithms: (1). **deterministic** and (2). **metaheuristic**. Deterministic and metaheuristic algorithms represent two distinct paradigms in optimization.\n\n\\*. **Deterministic methods**: such as gradient descent, produce the same solution for a given input and follow a predictable path toward an optimum.\n\n\\*. In contrast, **metaheuristic approaches**: incorporate randomness and do not guarantee the best possible solution. However, they are often more effective at avoiding local optima and exploring complex search spaces.\n\n## Deterministic Algorithms\n\nNumerical approximation, what you learned in the mathematical optimization course. Some of the algorithms include:\n\n-   Gradient Descent\n-   Newton's Method\n-   Conjugate Gradient Method\n-   Quasi-Newton Methods (e.g., BFGS)\n-   Interior Point Methods\n\nThey often rely on the **Karush–Kuhn–Tucker** (KKT) conditions.\n\n### Root finding\n\nThe *root finding* is probably the first numerical approach you learned in the numerical analysis course. Consider a function $f: \\mathbb R\\to \\mathbb R$. The point $x\\in \\mathbb R$ is called a *root* of $f$ if $f(x) = 0$.\n\nQ: Why do we care about the root finding?\n\nThis idea has broad applications. While finding the values of x such that f(x) = 0 is useful in many settings, a more general task is to determine the values of x for which f(x) = y. The same techniques used to find the roots of a function can be applied here by rewriting the problem as $$\n\\tilde{f}(x) := f(x) - y = 0.\n$$ In this way, new function $\\tilde{f}(x)$ has a root at the solution to, $f(x)=y$, original equation.\n\nFor linear function, it is trivial. For quadratic function, we can use the quadratic formula, i.e., $$\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}.\n$$ However, for more complex functions, we need to use numerical methods to solve it iteratively. Below, we are going to go over some numerical algorithms.\n\n### One-dimensional case\n\nWe first look at the one-dimensional case. The function we want to optimize is\n\n$$f(x) = x^3 - x + 1$$\n\n### Bisection method\n\nBisection method is just like a *binary search*.\n\n::: {.callout-algorithm title=\"Bisection Method\"}\n**Step 1.** Selection two points $a,b\\in \\chi \\subseteq \\mathbb R$, where $\\chi$ is the domain of $f$. Make sure that $a$ and $b$ have opposite signs, i.e., $f(a)f(b) < 0$.\n\n**Step 2.** Compute the midpoint $c = (a+b)/2$.\n\n**Step 3.** Evaluate and check the sign of $f(c)$. If $f(c)$ has the same sign as $f(a)$, then set $a=c$. Otherwise, set $b=c$.\n\n**Step 4.** Iterate Steps 2 and 3 until the interval $[a,b]$ is sufficiently small.\n:::\n\nThe intuition here is that we are shirking the search space $\\chi$ by half in each iteration.\n\nQ: Why this algorithm work and what are the assumptions? 1. We require the function to be continuous 2. We require the function to have opposite signs at the two endpoints $a,b\\in\\chi\\subseteq \\mathbb R$. 3. We do not require the differentiability!\n\nQ: But what's the cost?\n\nQ: Can this work for every function?\n\n#### Example\n\nSuppose the design region is\n\n\n::: {.cell}\n\n```{.r .cell-code}\na <- 1\nb <- 4\ncurve(0.5*x^3 - 0.5*x - 18, from = a, to = b, xlab = \"x\", ylab = \"f(x)\")\nfun_obj <- function(x) 0.5*x^3 - 0.5*x - 18\n\nmy_bisec <- function(fun_obj, a, b, tol = 1E-2, ind_draw = FALSE) {\n  if (fun_obj(a) * fun_obj(b) > 0) {\n    stop(\"f(a) and f(b) must have opposite signs!\")\n  }\n  iter <- 0\n  while ((b - a) / 2 > tol) {\n    c <- (a + b) / 2\n    \n    if (ind_draw == TRUE) {\n    # Draw vertical line\n    abline(v = c, col = \"red\", lty = 2)\n    # Label the iteration above the x-axis\n    text(c, par(\"usr\")[3] + 2, labels = iter + 1, col = \"blue\", pos = 3, cex = 0.8)\n    }\n\n    \n    if (fun_obj(c) == 0) {\n      return(c)\n    } else if (fun_obj(a) * fun_obj(c) < 0) {\n      b <- c\n    } else {\n      a <- c\n    }\n    iter <- iter + 1\n  }\n  val_x <- (a + b) / 2\n  val_fx <- fun_obj(val_x)\n  return(list(root = val_x, f_root = val_fx, iter = iter))\n}\n\n# Run it\nres_plot <- my_bisec(fun_obj, a, b, ind_draw = TRUE)\n```\n\n::: {.cell-output-display}\n![](02-optimization_files/figure-pdf/unnamed-chunk-9-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nres_plot\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$root\n[1] 3.408203\n\n$f_root\n[1] 0.09048409\n\n$iter\n[1] 8\n```\n\n\n:::\n\n```{.r .cell-code}\nres <- my_bisec(fun_obj, a, b)\nplot(function(x) fun_obj(x), from = a, to = b)\nabline(h = 0, col = \"blue\", lty = 2)\ntitle(main = paste0(\"Bisection Method with \", res$iter, \" iterations\"))\nabline(v = res$root, col = \"red\", lwd = 2)\ntext(res$root, par(\"usr\")[3] + 5, \n     labels = paste0(\"Root ~= \", round(res$root, 3)), \n     col = \"red\", pos = 3, cex = 0.9, font = 2)\n```\n\n::: {.cell-output-display}\n![](02-optimization_files/figure-pdf/unnamed-chunk-9-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n### Newton-Raphson method\n\nThe Newton-Raphson method (or simply Newton's method) is an iterative numerical method for finding successively better approximations to the roots (or zeroes) of a real-valued function.\n\nHere, we assume that the function $f$ is differentiable. The idea here is to use the Taylor expansion of the function. Suppose we are search a small neighbour of the solution $x \\in \\mathbb R$, say $x_j \\in \\mathbb R$ is a small number. Then Then we first order Taylor series to approximate $f(x_j+h)$ around $x_j$ is $$\nf(x)\\approx f(x_j) +  f^\\prime(x_j) (x-x_j),\n$$ where $f^\\prime(x) := \\partial_x f(x)$ is the first derivative of $f(x)$. So the root of this approximation can be improved by updating its place to where $f(x_{j+1}) = 0$.\n\nSo if $f(x_j+h)$ is the root, then we have $$ 0 = f(x_j) + f^\\prime(x_j) h \\implies h = -\\frac{f(x_j)}{f^\\prime(x_j)}.$$\n\nThen, we can come back to $x_{j+1}= x_j+h$, and plug the value of $h$ in from above, we have $$\nx_{j+1} = x_j - \\frac{f(x_j)}{f^\\prime(x_j)}.\n$$\n\nThe algorithm is given as below:\n\n::: {.callout-algorithm title=\"Newton-Raphson Method\"}\nLet $f:\\mathbb R\\to\\mathbb R$ be differentiable.\n\nStep 0: Choose a function $f(x)$: This is the function for which you want to find a root (i.e., solve $f(x) = 0$).\n\nStep 1: Calculate the derivative $f^\\prime(x)$: You will need it to apply the formula.\n\nStep 2: Make an initial guess $x_0$: Select a starting point $x_0$ near the expected root.\n\nStep 3: Update the estimate: Use the Newton's method formula to compute the next estimate $x_1$ using $x_0$ by $$x_{j+1} = x_j - \\frac{f(x_j)}{f^\\prime(x_j)}.$$\n\nStep 4: Repeat Steps 2 and 3 until the values converge to a root or reach a desired tolerance.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Function and derivative\nf  <- function(x) 0.5*x^3 - 0.5*x - 18\ndf <- function(x) 1.5*x^2 - 0.5\n\n## Newton–Raphson with iterate tracking\nnewton_raphson <- function(f, df, x0, tol = 1e-5, \n                           maxit = 100, eps = 1e-5) {\n  x <- x0\n  xs <- x0      # store iterates (x0, x1, x2, ...)\n  for (k in 1:maxit) {\n    fx  <- f(x)\n    dfx <- df(x)\n    x_new <- x - fx/dfx\n    xs <- c(xs, x_new)\n    if (abs(x_new - x) < tol || abs(fx) < tol) {\n      return(list(root = x_new, iter = k, path = xs))\n    }\n    x <- x_new\n  }\n  list(root = x, iter = maxit, path = xs)\n}\n\n## Starting point\n```\n:::\n\n\nIf we start at -1 which is far away from\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx0 <- -1\nres <- newton_raphson(f, df, x0)\na <- -2; b <- 5\nplot(function(x) f(x), from = a, to = b, \n     xlab = \"x\", ylab = \"f(x)\",\n     main = paste(\"Newton-Raphson (Iterations:\", res$iter, \")\"))\nabline(h = 0, col = \"blue\", lty = 2)\n\n## Draw vertical lines for each iterate with labels 0,1,2,...\nfor (i in seq_along(res$path)) {\n  xi <- res$path[i]\n  abline(v = xi, col = \"red\", lty = 2)\n  text(xi, par(\"usr\")[3] + 2, labels = i - 1, col = \"blue\", pos = 3, cex = 0.9)\n}\n\n## Final root marker + label\nabline(v = res$root, col = \"darkgreen\", lwd = 2)\ntext(res$root, par(\"usr\")[3] + 5,\n     labels = paste0(\"Root ~= \", round(res$root, 5),\n                     \" ; f(root) ~= \", signif(f(res$root), 3)),\n     col = \"darkgreen\", pos = 3, cex = 0.95, font = 2)\n```\n\n::: {.cell-output-display}\n![](02-optimization_files/figure-pdf/unnamed-chunk-10-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nres\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$root\n[1] 3.402848\n\n$iter\n[1] 9\n\n$path\n [1] -1.000000 17.000000 11.387991  7.704327  5.368534  4.042133  3.500619\n [8]  3.405629  3.402850  3.402848\n```\n\n\n:::\n:::\n\n\nIf we start at 3 which is near to the point\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx0 <- 3\nres <- newton_raphson(f, df, x0)\n## Plot range that shows the iterates and root\na <- -2; b <- 5\nplot(function(x) f(x), from = a, to = b, \n     xlab = \"x\", ylab = \"f(x)\",\n     main = paste(\"Newton-Raphson (Iterations:\", res$iter, \")\"))\nabline(h = 0, col = \"blue\", lty = 2)\n\n## Draw vertical lines for each iterate with labels 0,1,2,...\nfor (i in seq_along(res$path)) {\n  xi <- res$path[i]\n  abline(v = xi, col = \"red\", lty = 2)\n  text(xi, par(\"usr\")[3] + 2, labels = i - 1, col = \"blue\", pos = 3, cex = 0.9)\n}\n\n## Final root marker + label\nabline(v = res$root, col = \"darkgreen\", lwd = 2)\ntext(res$root, par(\"usr\")[3] + 5,\n     labels = paste0(\"Root ~= \", round(res$root, 5),\n                     \" ; f(root) ~= \", signif(f(res$root), 3)),\n     col = \"darkgreen\", pos = 3, cex = 0.95, font = 2)\n```\n\n::: {.cell-output-display}\n![](02-optimization_files/figure-pdf/unnamed-chunk-11-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nres\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$root\n[1] 3.402848\n\n$iter\n[1] 4\n\n$path\n[1] 3.000000 3.461538 3.403866 3.402848 3.402848\n```\n\n\n:::\n:::\n\n\nRemarks:\n\n-   Assumptions: $f$ is differentiable in a neighborhood of the root $r$.\n\n-   Failure cases: if $f^\\prime(x_j)=0$ (or is very small), the update is ill-defined/unstable; if the initial guess is far, the method can diverge or jump to a different root.\n\n-   Practical checks: stop when $|f(x_j)|\\le \\delta$ or $|x_{j+1}-x_j| \\le \\delta$ is below tolerance $\\delta$.\n\n### Second Method\n\nThe secant method can be thought of as a finite-difference approximation of Newton's method, so it is considered a quasi-Newton method. It is simialr to Newton's method, but it does not require the computation of the derivative of the function. Instead, it approximates the derivative using two previous points.\n\nIn the second method, we require the **first two point**s, say $x_0, x_1 \\in \\mathbb R$. Then, we can approximate the derivative of $f$ at $x_1$ using the finite difference formula. Instead of calculate the derivative $f^\\prime(x_1)$, we approximate it as using the secant line. In calculate, we know that, $f^\\prime(x_1) \\approx \\frac{f(x_1)-f(x_0)}{x_1-x_0}$, if $x_1$ and $x_0$ are close. Then, we can plug this approximation into the Newton's update formula to get $$x_j  = x_{j-1}  - f(x_{j-1}) \\frac{x_{j-1}-x_{j-2}}{f(x_{j-1}) - f(x_{j-2})} = \\frac{x_{j-2} f\\left(x_{j-1}\\right)-x_{j-1} f\\left(x_{j-2}\\right)}{f\\left(x_{j-1}\\right)-f\\left(x_{j-2}\\right)} .$$\n\n## Hill climbing\n\nIn numerical analysis, *hill climbing* is a mathematical optimization technique which belongs to the family of *local search*. The Newton method and secant method can be thought as questions in hill climbing.\n\nThe algorithm starts with an arbitrary solution to a problem, then iteratively makes small changes to the solution, each time moving to a neighboring solution that is better than the current one. The process continues until no neighboring solution is better than the current solution, at which point the algorithm terminates.\n\nIn the world of optimization, finding the best solution to complex problems can be challenging, especially when the solution space is vast and filled with local optima.\n\n### In R\n\n`uniroot()`, `optim()` , `nlm()`, and `mle()` functions. Note that you may *approximate the derivative/gradient*.\n\n## Converegence\n\nIn order to compare the efficiency of the set of algorithms, one may compare *their abilities* for finding the optimals. However, what if, say, two algorithms both can find optimals, which one is better? The **convergence rate** comes in. Convergence rate is a measure of how quickly an iterative algorithm approaches its limit or optimal solution, which mean, how fast the algorithm converges to the optimals.\n\nIn the previous lecture(s), we saw that we can use R functions such `microbenchmark::microbenchmark()`, to measure the performance. However, it may takes a long time and a lot of computational resources. For such cases, we may use the theoretical convergence rate to compare the efficiency of the algorithms.\n\nThe convergence rate is often classified into several categories. It acts like the sequence $\\{x_j\\}$ we learned in grade schools. Here, $\\{x_j\\}$ is a sequence of estimates generated by the algorithm at each iterations, and $x^*$ is the true solution or optimal value we are trying to reach. The error at iteration $n$ is defined as $e_j = d(x_j,x^*)$, where the typical metric here is the absolute distance $d(x_j,x^*)=|x_j-x^*|$ (note, in spaces, different metric to define the distance). The convergence rate describes how quickly this error sequence $\\{e_j\\}$ decreases as $j$ increases. For\n\n$$\n\\lim_{j \\to \\infty} \\frac{\\left|x_{j+1}-x^* \\right|}{\\left|x_j-x^* \\right|^q }=\\mu.\n$$\n\n### Linear Convergence\n\nIf order $q = 1$ and $0 < \\mu < 1$, the sequence $\\{x_j\\}\\in\\mathbb R^d$ converges to $x^*$ linearly. That is, $x_j\\to x^*$ as $j\\to\\infty$ in $\\mathbb R^d$ if there existence a constant $\\mu$ such that $$\n  \\frac{\\left\\|x_{n+1}-x_{\\infty}\\right\\|}{\\left\\|x_n-x_{\\infty}\\right\\|} \\le \\mu,\\quad \\text{ as } \\quad n\\to\\infty.\n$$ This means that the error decreases proportionally to its current value, leading to a steady but relatively slow convergence.\n\n### Superlinear Convergence\n\nSuppose $\\{x_n\\}$ converges to $x^*$, if order $q = 1$ and $\\mu = 0$, the sequence $\\{x_n\\}$ converges to $x^*$ superlinearly. That is, $x_n$ is said to be converges to $x^*$ as $n\\to\\infty$ superlinearly if\n\n$$\n\\lim _{n \\to \\infty} \\frac{\\left\\|x_{n+1}-x_{\\infty}\\right\\|}{\\left\\|x_n-x_{\\infty}\\right\\|}=0.\n$$ It is clearly that the superlinear is a stronger condition than the linear convergence, such that $\\mu=0$.\n\n### Quadratic Convergence\n\nIf order $q = 2$ and $\\mu > 0$, the sequence $\\{x_n\\}$ converges to $x^*$ quadratically. That is, $x_n$ is said to be converges to $x^*$ as $n\\to\\infty$ quadratically if\n\n$$\n\\frac{\\left\\|x_{n+1}-x_{\\infty}\\right\\|}{\\left\\|x_n-x_{\\infty}\\right\\|^2} \\le \\mu, \\quad \\text{ as } \\quad n\\to\\infty.\n$$\n\n## Heuristic Algorithms\n\nMany of the heuristic algorithms are inspired by the nature, such as the genetic algorithm (GA) and particle swarm optimization (PSO). These algorithms are often used for complex optimization problems where traditional methods may struggle to find a solution. Some of the popular heuristic algorithms include:\n\n-   Genetic Algorithm (GA)\n-   Particle Swarm Optimization (PSO)\n-   Simulated Annealing (SA)\n-   Ant Colony Optimization (ACO)\n\n### Simulating Annealing\n\nSimulated annealing (SA) is a **stochastic** technique for approximating the global optimum of a given function.\n\nInspired by the physical process of annealing in metallurgy, Simulated Annealing is a probabilistic technique used for solving both combinatorial and continuous optimization problems.\n\nWhat is Simulated Annealing?\n\nSimulated Annealing is an optimization algorithm designed to search for an optimal or near-optimal solution in a large solution space. The name and concept are derived from the process of annealing in metallurgy, where a material is heated and then slowly cooled to remove defects and achieve a stable crystalline structure. In Simulated Annealing, the \"heat\" corresponds to the degree of randomness in the search process, which decreases over time (cooling schedule) to refine the solution. The method is widely used in combinatorial optimization, where problems often have numerous local optima that standard techniques like gradient descent might get stuck in. Simulated Annealing excels in escaping these local minima by introducing controlled randomness in its search, allowing for a more thorough exploration of the solution space.\n\nSome terminology:\n\n-   Temperature: controls how likely the algorithm is to accept worse solutions as it explores the search space.\n\n::: {.callout-algorithm title=\"Simulated Annealing\"}\nStep 1 (Initilization): Begin with an initial solution $S_ο$ and an initial temperature $T_ο$.\n\nStep 2 (Neighborhood Search): At step $j$, a new solution $S^\\prime$ is generated by making a small change (or perturbation) to $S_j$.\n\nStep 3 (Evaluation): evaluate the objective function $f(S^\\prime)$ Step 3.1: If $f^(S^\\prime)$ is *better* than $f(S_j)$, we *accept* it and take it as $S_{j+1}$. Step 3.2: If $f(S^\\prime)$ is *worse* than $f(S_j)$, we may still accept it with a certain probability $P(S_{j+1}=S^\\prime)=\\exp(-\\Delta E/T_j)$, where $E$ is the energy $f(S^\\prime)-f(S_j)$.\n\nStep 4 Cooling Schedule: Decrease the temperature according to a cooling schedule, e.g., $T_{j+1} = \\alpha T_j$ where $\\alpha \\in (0,1)$ is a cooling rate.\n\nStep 5 (Evaluation): Repeat Steps 2 and 3 for a certain number of iterations or until convergence criteria are met.\n:::\n\nExample:\n\nFigure 1 in [my paper](https://arxiv.org/pdf/2405.02983)\n\nAdvantages:\n\n-   Global optimization\n\n-   Flexibility\n\n-   Intuitive\n\n-   Derivative?\n\nLimitations:\n\n-   Parameter semsitivity\n\n-   Computational time\n\n-   Slow convergence\n\n#### R implementation\n\nAn paper about an implementation in R by [Husmann et al.](https://www.maths.bris.ac.uk/R/web/packages/optimization/vignettes/vignette_master.pdf) and another package called [GenSA](https://cran.r-project.org/web/packages/GenSA/GenSA.pdf).\n\n## Genetic Algorthm\n\nGenetic Algorithm (GA) is a metaheuristic optimization technique inspired by the process of natural evolution/selection.\n\nGA are based on an analogy with the genetic structure and behavior of chromosomes of the population.\n\nSTEP 1: Start with an initial generation $G_0$ of potential solutions (individuals). Each individual is represented by a chromosome, which is typically encoded as a binary string, real-valued vector, or other suitable representation. Evaluate the objective function on those points.\n\nStep 2: Generate the next generation $G_{j+1}$ from the current generation $G_j$ using genetic operators: a). Selection: Retain the individual that is considered as *good* b). Crossover: Create children variables from the parents c). Mutation\n\nStep 3: Repeat Step 2 until the algorithm converges or reaches a stopping criterion.\n\n### Particle Swarm Optimization\n\nParticle Swarm Optimization (PSO) was proposed by Kennedy and Eberhart in 1995. It is inspried by the movement of the species in nature, such as fishes or birds.\n\nThe algorithm is based on a *population*, not a single current point.\n\nAt iteration $n$ of the algorithm, a particle has a velocity $v(n)$ that depends on the follows.\n\n-   The location of the best objective function value that it has encountered, $s(n)$.\n\n-   The location of the best objective function value among its neighbors, $g(n)$.\n\n-   The previous velocity $v(n – 1)$.\n\nThe position of a particle x(n) updates according to its velocity: $$x(n+1)=x(n)+v(n),$$ adjusted to stay within the bounds. The velocity of a particle updates approximately according to this equation:\n\n$$v(n+1) = W(n)v(n)+r(1)(s(n)−x(n))+r(2)(g(n)−x(n)).$$\n\nHere, $r(1),r(2) \\in [0,1]$ are random scalar values, and $W(n)$ is an *inertia factor* that adjusts during the iterations. The full algorithm uses randomly varying neighborhoods and includes modifications when it encounters an improving point.\n\nNote: There are A LOT of variations of the PSO and other swarm-based algorithms used in the literature.\n\nIn R, there is a PSO implementation in the `pso` package. The associated manual may be found [here](https://cran.r-project.org/web/packages/pso/pso.pdf).\n\n------------------------------------------------------------------------\n\nExamples are borrowed from the following sources:\n\n-   Peng, R.D. [Advanced Statistical Computing](https://bookdown.org/rdpeng/advstatcomp/).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}