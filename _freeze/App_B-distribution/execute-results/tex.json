{
  "hash": "f52010c1af3873828c64eda7bff0d7c4",
  "result": {
    "engine": "knitr",
    "markdown": "# Appendix: Distributions\n\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\var}{\\mathbb{V}ar}\n\n## Discrete Distributions\n\n### Discrete uniform distributions\n\nif the variable can take on finite, countable values with equal probability, it is said to be discrete uniform. The probability mass function of a discrete uniform. random variable $X$ is given by\n$$\nf(x) = \\frac{1}{x_n-x_1+1},\\quad \\text{for}\\quad x=x_1,\\dots,x_n.\n$$\n\n::: {.callout-theorem}\nThe mean and variance of the discrete uniform distribution are\n$$\n\\E X = \\frac{x_1+x_n}{2},\\quad \\text{and}\\quad \\var(X) = \\frac{(x_n-x_1+1)^2 -1}{12}.$$\n:::\n\n#### Implementation in R\n\nThere is no buildt-in function t osimlate from the discrete uniform distributions. Hence, we need to download additional packages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(extraDistr)\nlibrary(extraDistr)\n# Generate random sample from Uniform(0, 10)\nA <- rdunif(10000, 0, 10) \n\n# Histogram of the sample\nhist(A)\n```\n\n::: {.cell-output-display}\n![](App_B-distribution_files/figure-pdf/unnamed-chunk-1-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Sample mean and variance\nmean(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.9627\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10.00951\n```\n\n\n:::\n\n```{.r .cell-code}\n# Theoretical mean of Uniform(a, b) is (a + b)/2\n(mean.est <- (min(A) + max(A)) / 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5\n```\n\n\n:::\n\n```{.r .cell-code}\n# Theoretical variance of Uniform(a, b) is (b - a)^2 / 12\n(var.est <- ((max(A) - min(A) + 1 )^2 -1 ) / 12)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10\n```\n\n\n:::\n:::\n\n\n### Bernoulli Distribution\n\nThere are only 2 possible outcomes in an experiment: *True* and *False*, with probabiltiy $\\theta$ and $1-\\theta$, respectively, the random variable $X$ follows a Bernoulli distribution, denoted by $X\\sim \\text{Bern}(\\theta)$. The probabiltiy distribution has probablity mass function as\n$$f(x) = \\theta^x (1-\\theta)^{1-x},\\quad x=0,1.$$\n\nNote that Bernoulli distribution is a special case of the Binomial  distribution with $n=1$.\n\n#### Implementation in R\n\n``` r\nrbinom(10, 1, 0.7) # 0.7 is the probability of success\n```\n\n### Binomial Distribution\n\nBernoulli trials (experiment where a Bernoulli Distribution applies) are not very realistic in real-life scenario. In statistics, repeated experiments are important. When the experiments is repeated, $\\theta$ is the same for each of the trials, and the trials are independent, and they are only two mutually excludes outcomes (T, and F), we can model this using the *Binomial Distribution*. If $X$ follows the Binomial Distribution, it has a probablitliy mass function as\n$$b(x ; n, \\theta)=\\binom{n}{x} \\theta^x(1-\\theta)^{n-x}, x=0,1,2,3, \\ldots, n.$$\n\nThe Binomial Distribution is useful to predict the number of heads in $n$ coin tosses, the number of people infected with a disease in a certain population of known size, etc. The parameters $n$ and $\\theta$ must be given.\n\n::: {.callout-theorem}\nThe mean and variance of the Binomial Distribution \n$$\n\\E X = n\\theta,\\quad \\text{and}\\quad \\var(X) = n\\theta(1-\\theta).$$\n:::\n\n#### Implementation in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX = rbinom(10000, 4, prob = 0.3)\nmean(X == 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4053\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(X > 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3482\n```\n\n\n:::\n:::\n\n\n### Geometric Distribution\n\nIf we are interested in the number of trials until the first success, then this is \nmodeled by the geometric distribution. A random variable $X$ follows a geometric \ndistribution if and only if its probability distribution is given by:\n$$\ng(x;\\theta) = \\theta (1 - \\theta)^{\\,x-1}, \\quad x = 1,2,3,\\ldots\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to generate n samples from Geometric(p)\ngeom.gener <- function(n, p) {\n  tmp <- NULL\n  for (i in 1:n) {\n    u <- runif(1)  # Uniform(0,1)\n    x <- 1\n    p.x <- p\n    sum <- p.x\n    \n    while (sum < u) {\n      x <- x + 1\n      p.x <- p.x * (1 - p)\n      sum <- sum + p.x\n    }\n    \n    tmp <- c(tmp, x)\n  }\n  return(tmp)\n}\n\n# Example: generate 100,000 samples with p = 0.75\nx <- geom.gener(100000, 0.75)\n\n# Relative frequencies\ntable(x) / 100000\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nx\n      1       2       3       4       5       6       7       8       9 \n0.74828 0.18913 0.04680 0.01179 0.00317 0.00063 0.00014 0.00005 0.00001 \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR \nX = rgeom(100000, prob= 0.75)\ntable(X)/100000\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX\n      0       1       2       3       4       5       6       7       8 \n0.74811 0.18907 0.04773 0.01132 0.00282 0.00064 0.00024 0.00005 0.00002 \n```\n\n\n:::\n:::\n\n\nDo you notice the difference between the two functions? R starts the geometric distribution at $x = 0$ and not $x = 1$. We can check that both of these simulate the same distribution by checking a qqplot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Correlation between points in QQ plot\ncor(qqplot(x, X + 1)$x, qqplot(x, X + 1)$y)\n```\n\n::: {.cell-output-display}\n![](App_B-distribution_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.998222\n```\n\n\n:::\n:::\n\n\n### Hypergeometric Distribution\n\nThe motivating question: what happens in a Binomial setting when our trials are NOT independent? In other words, what happens when we sample without replacement?\n\nConsider a set of N elements, of which M are successes. We are interested in obtaining X successes in n trials. This situation is modeled by the Hypergeometric Distribution, which has pdf:\n\n$$h(x,n,M,N) = \\frac{\\binom{M}{x}\\binom{N-M}{n-x}}{\\binom{N}{n}}, \\quad x = 0,1,2,\\ldots,n$$\n\n::: {.callout-theorem}\nThe mean and the variance are\n$$\\E X = \\frac{nM}{N}, \\qquad\n\\var(X) = \\frac{nM(N-M)(N-n)}{N^2(N-1)}.$$\n:::\n\n#### Implementation in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define population: 12 ones and 13 zeros\nx <- c(1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\n\n# Take one sample of size 8 without replacement\ns1 <- sample(x, 8, replace = FALSE)\ns1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 0 0 1 0 1 1 0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Count how many ones in that sample\nsum(s1 == 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4\n```\n\n\n:::\n\n```{.r .cell-code}\n# Repeat the sampling 10,000 times\ny <- replicate(10000, sample(x, 8, replace = FALSE))\n\n# Column sums = number of ones in each sample\nz <- colSums(y)\n\n# Histogram of simulated distribution\nhist(z)\n```\n\n::: {.cell-output-display}\n![](App_B-distribution_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Probability of getting exactly 5 ones\nmean(z == 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.107\n```\n\n\n:::\n\n```{.r .cell-code}\n# Sample mean and variance of distribution\nmean(z)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.1804\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(z)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.367393\n```\n\n\n:::\n:::\n\n\n### Poisson Distribution\n\nCalculating Binomial probabilities when $n$ is large can be highly tedious and time-consuming. As $n$ approaches infinity and the probability of success approaches $0$, where $n\\theta$ remains fixed. We can define $n\\theta = \\lambda$, and obtain the distribution called Poisson.\n\nA random variable X has the Poisson Distribution if and only if its probability distribution is given by:\n\n$$P(x;\\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}, \\quad x = 0,1,2,\\ldots$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate 10,000 samples from Poisson(λ = 2)\nX <- rpois(10000, 2)\n\n# Histogram of simulated data\nhist(X)\n```\n\n::: {.cell-output-display}\n![](App_B-distribution_files/figure-pdf/poisson-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Probability that X > 3 (estimated by simulation)\nmean(X > 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1456\n```\n\n\n:::\n\n```{.r .cell-code}\n# Probability that X = 0 (estimated by simulation)\nmean(X == 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1318\n```\n\n\n:::\n\n```{.r .cell-code}\n# Theoretical probability that X = 0\nexp(-2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1353353\n```\n\n\n:::\n\n```{.r .cell-code}\n# Sample mean and variance of simulated distribution\nmean(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.0172\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.025707\n```\n\n\n:::\n:::\n\n\n::: {.callout-theorem}\nThe mean and variance of the Poisson distribution are\n\n$$\\E X = \\lambda, \\qquad \\var(X) = \\lambda$$\n:::\n\nThe Poisson distribution is derived as the limiting case of the Binomial (with the above mentioned restrictions) BUT there are many more applications. It models:\n\n1.\tNumber of successes to occur in a given time period\n\n2.\tNumber of telephone calls received in a given time\n\n3.\tNumber of misprints on a page\n\n4.\tNumber of customers entering a bank during various intervals of time\n\n⸻\n\nWe are now moving onto the *continuous distributions* that play an important role in Statistical Theory.\n\n## Continuous Distributions\n\n### Uniform Distribution\n\nSimilar to the discrete uniform distribution except all values within an interval have equal probability. The parameters of the Uniform Density are \\alpha and \\beta (\\alpha < \\beta). The random variable X has the Uniform Distribution if it is continuous and its probability density function is given by\n$$\nf_X(x) =\n\\begin{cases}\n\\dfrac{1}{\\beta - \\alpha}, & \\alpha < x < \\beta, \\\\[1ex]\n0, & \\text{otherwise}.\n\\end{cases}\n$$\n\n::: {.callout-theorem}\n The mean and variance of the Uniform Distribution are\n$$\\E X = \\frac{\\alpha + \\beta}{2},\n\\qquad\n\\var(X) = \\frac{(\\beta - \\alpha)^2}{12}.\n$$\n:::\n\n\n#### Implementation in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- runif(1000)\n\n# Histogram of simulated data\nhist(X)\n```\n\n::: {.cell-output-display}\n![](App_B-distribution_files/figure-pdf/Uniform-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nmean(X);  var(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5035435\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.08632385\n```\n\n\n:::\n:::\n\n\n\n### Gamma Distribution\n\nThe Gamma function is defined as\n$$\\Gamma(\\alpha) = \\int_{0}^{\\infty} y^{\\alpha - 1} e^{-y} \\, dy.$$\n\n::: {.callout-theorem}\nFor $\\alpha > 0$,\n$$\n\\Gamma(\\alpha) = (\\alpha - 1)\\Gamma(\\alpha - 1).$$\n:::\n\n::: {.callout-theorem}\nFor any positive integer $\\alpha > 0$,\n$$\\Gamma(\\alpha) = (\\alpha - 1)!$$\n:::\n\nA continuous random variable follows a Gamma Distribution if and only if its probability density function is of the form\n$$f_X(x) =\n\\begin{cases}\n\\dfrac{1}{\\beta^{\\alpha}\\Gamma(\\alpha)} x^{\\alpha - 1} e^{-x/\\beta}, & x > 0, \\\\[2ex]\n0, & \\text{otherwise},\n\\end{cases}$$\nwhere $\\alpha, \\beta$ are positive. The parameters $\\alpha$ and $\\beta$ determine the shape of the distribution.\n$\\alpha$ is called the shape parameter and $\\beta$ is called the scale parameter.\n\n::: {.callout-theorem}\nTheorem: The mean and variance of the Gamma Distribution are\n$$\\E X = \\alpha \\beta,\n\\qquad\n\\var(X) = \\alpha \\beta^2$$\n:::\n\n#### Implementation in R\n\n``` r\nrgamma(n = 10, shape = 10, scale = 3)\n```\n\n### Exponential Distribution\n\nA special case of the Gamma Distribution arises when $\\alpha = 1$. To differentiate it from the Gamma distribution, we also let $\\beta = \\theta$. A continuous random variable follows an Exponential Distribution if and only if its probability density function is of the form:\n$$f_X(x) =\n\\begin{cases}\n\\dfrac{1}{\\theta} e^{-x/\\theta}, & x > 0, \\\\[2ex]\n0, & \\text{otherwise}.\n\\end{cases}$$\n\n\nExponential Distributions have many applications. One of them is a waiting time until the first success of a Poisson process. In this situation, it is often better to model the phenomenon in terms of a rate parameter (i.e. 4 calls per week). Thus, the distribution of waiting times becomes:\n$$\nf_Y(y) =\n\\begin{cases}\n\\lambda e^{-\\lambda y}, & y > 0, \\\\[2ex]\n0, & \\text{otherwise}.\n\\end{cases}$$\n\n#### Memoryless Property\n\nThis is also called the *Markov Property.* The distribution of waiting time does not depend on how long you have already waited. In other words:\n$$\nP(X > s + t \\mid X > t) = P(X > s).\n$$\n\n::: {.callout-theorem}\nThe mean and variance of the exponential distribution are\n$$\\E X = \\theta,\n\\qquad\n\\var(X) = \\theta^2.$$\n:::\n\n#### Implementation in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate 1000 samples from Exponential(λ = 1)\nX <- rexp(1000)\nhist(X)\n```\n\n::: {.cell-output-display}\n![](App_B-distribution_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nmean(X); var(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9746219\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9302842\n```\n\n\n:::\n:::\n\n\n\n\n### Chi-Square Distribution\n\nThere is another special form of the gamma distribution when $\\alpha =\\nu/2$ and $\\beta = 2.\n\\nu$ is pronounced “nu” and is called the degrees of freedom.\n\nA continuous random variable follows a Chi-Square Distribution if and only if its probability density is given by\n$$f_X(x) =\n\\begin{cases}\n\\dfrac{1}{2^{\\nu/2}\\Gamma\\!\\left(\\tfrac{\\nu}{2}\\right)} x^{\\tfrac{\\nu}{2} - 1} e^{-x/2}, & x > 0, \\\\[2ex]\n0, & \\text{otherwise}.\n\\end{cases}$$\n\n\n::: {.callout-theorem}\nThe mean and variance of the Chi-Square Distribution are\n\n$$\\E X = \\nu,\n\\qquad\n\\var(X) = 2\\nu.$$\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- rchisq(1000, df = 5)\n\nhist(X)\n```\n\n::: {.cell-output-display}\n![](App_B-distribution_files/figure-pdf/chisq-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nmean(X);  var(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.077642\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10.5748\n```\n\n\n:::\n:::\n\n\n### Beta Distribution\n\nThis is **Not** a special case of the Gamma distribution\n\nThe Beta function is defined as \n$$\nB(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\n= \\int_0^1 x^{\\alpha - 1} (1-x)^{\\beta - 1} \\, dx.$$\n\nLike any probability density, the area underneath it must be equal to 1.\nSo, we can rearrange the above definition and write:\n$$1 = \\int_0^1 \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\nx^{\\alpha-1} (1-x)^{\\beta-1} \\, dx.$$\n\n\nA continuous random variable follows a Beta Distribution if and only if its probability density function is given by\n$$\nf_X(x) =\n\\begin{cases}\n\\dfrac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\nx^{\\alpha-1}(1-x)^{\\beta-1}, & 0 < x < 1, \\\\[2ex]\n0, & \\text{otherwise},\n\\end{cases}$$\nwhere $\\alpha > 0$, $\\beta > 0$.\n\n\n::: {.callout-theorem}\nThe mean and variance of the Beta Distribution are\n$$\\E X = \\frac{\\alpha}{\\alpha + \\beta},\n\\qquad\n\\var(X) = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}.\n$$\n\n#### Implementation in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# Sequence of x values\nt <- seq(0, 1, by = 0.01)\n\n# Create data frame with multiple Beta densities\ndf <- data.frame(\n  x = rep(t, 4),\n  density = c(dbeta(t, 2, 2),\n              dbeta(t, 2, 8),\n              dbeta(t, 8, 2),\n              dbeta(t, 1, 1)),\n  dist = factor(rep(c(\"Beta(2,2)\", \"Beta(2,8)\", \"Beta(8,2)\", \"Beta(1,1)\"),\n                    each = length(t)))\n)\n\n# Plot with ggplot\nggplot(df, aes(x = x, y = density, color = dist)) +\n  geom_line(size = 1) +\n  labs(x = \"X\", y = \"Beta Density\", title = \"Beta Distributions\") +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\ni Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](App_B-distribution_files/figure-pdf/beta-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nQ: What do you noitce about $B(1,1)$? What distribution is this?\n\nThe Beta distribution is related to the Binomial distribution when computing maximum likelihood estimators !(We will make use of this property later when we do Bayesian analysis.)\n$$\\text{Beta}_{pdf}(p, n, k) = (n+1)\\binom{n}{k} p^k (1-p)^{n-k},$$\nwhere\n$p^\\prime = \\text{Binomial}_{pmf}(k,n,p), \\quad k = \\text{mode}(\\text{Binomial}(n,p)).$\n\nTo relate the binomial distribution to the scale and shape parameters of the beta distribution:\n$\\alpha = k + 1, \\quad \\beta = n - \\alpha + 2$.\n\n#### Implementation in R\n\nSuppose we flip a coin 20 times and find that we have 8 heads. Thus, our MLE is $8/20 = 0.4$.\nWe can visualize the likelihood function for this scenario:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Likelihood plot using Binomial likelihood\nlikeli_bino.plot <- function(y, n) {\n  L <- function(p) dbinom(y, n, p)\n  mle <- optimize(L, interval = c(0, 1), maximum = TRUE)$max\n  \n  p <- (1:100) / 100\n  \n  # Likelihood\n  plot(p, L(p), type = \"l\")\n  abline(v = mle)\n  \n  # Log-likelihood\n  plot(p, log(L(p)), type = \"l\", main = \"binomial\")\n  abline(v = mle)\n}\npar(mfrow = c(1,1), mar = c(4,4,2,1))\nlikeli_bino.plot(8, 20)\n```\n\n::: {.cell-output-display}\n![](App_B-distribution_files/figure-pdf/beta-binomial-1.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](App_B-distribution_files/figure-pdf/beta-binomial-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Likelihood plot using Beta distribution\nlikeli_beta.plot <- function(y, n) {\n  L <- function(p) dbeta(p, y + 1, n - (y + 1) + 2)\n  mle <- optimize(L, interval = c(0, 1), maximum = TRUE)$max\n  \n  p <- (1:100) / 100\n  \n  # Likelihood\n  plot(p, L(p), type = \"l\")\n  abline(v = mle)\n  \n  # Log-likelihood\n  plot(p, log(L(p)), type = \"l\", main = \"Beta\")\n  abline(v = mle)\n  \n  mle\n}\nlikeli_beta.plot(8, 20)\n```\n\n::: {.cell-output-display}\n![](App_B-distribution_files/figure-pdf/beta-binomial-3.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](App_B-distribution_files/figure-pdf/beta-binomial-4.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3999996\n```\n\n\n:::\n\n```{.r .cell-code}\noverlay.likeli <- function(y, n) {\n  # Binomial likelihood\n  L_binom <- function(p) dbinom(y, n, p)\n  \n  # Beta likelihood (with α = y+1, β = n-y+1)\n  L_beta <- function(p) dbeta(p, y + 1, n - y + 1)\n  \n  # Sequence of p values\n  p <- seq(0, 1, length.out = 200)\n  \n  # Scale the beta likelihood so it’s comparable\n  scale_factor <- max(L_binom(p)) / max(L_beta(p))\n  par(mfrow=c(1,1))\n  # Plot Binomial likelihood\n  plot(p, L_binom(p), type = \"l\", col = \"blue\", lwd = 2,\n       ylab = \"Likelihood\", xlab = \"p\",\n       main = \"Binomial vs Beta Likelihood\")\n  \n  # Add Beta likelihood (scaled for comparison)\n  lines(p, L_beta(p) * scale_factor, col = \"red\", lwd = 2, lty = 2)\n  \n  # Add legend\n  legend(\"topright\",\n         legend = c(\"Binomial Likelihood\", \"Beta Likelihood (scaled)\"),\n         col = c(\"blue\", \"red\"),\n         lty = c(1, 2), lwd = 2)\n}\n\n# Example: 8 successes out of 20 trials\n\noverlay.likeli(8, 20)\n```\n\n::: {.cell-output-display}\n![](App_B-distribution_files/figure-pdf/beta-binomial-5.pdf){fig-pos='H'}\n:::\n:::\n\n\n### Gaussian Distribution\n\nThis is probably the most famous statistical distribution. It is defined by its mean ($\\mu$) and variance ($\\sigma^2$). It is also known as the Normal Distribution.\nA continuous random variable follows a Normal Distribution if and only if its probability density function is given by\n\n$$\nf_X(x) =\n\\begin{cases}\n\\dfrac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\!\\left( -\\dfrac{(x - \\mu)^2}{2\\sigma^2} \\right), & -\\infty < x < \\infty, \\\\[2ex]\n0, & \\text{otherwise}.\n\\end{cases}\n$$\n\nNote: The Standard Normal Distribution has density with $\\E X = 0$ and $\\var(X) = 1$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- rnorm(1000, 0, 1)\nqqnorm(X)\n```\n\n::: {.cell-output-display}\n![](App_B-distribution_files/figure-pdf/normal-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Approximate probabilities of being within 1, 2, 3 standard deviations\nmean(-1 < X & X < 1)  # ~ 68%\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.655\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(-2 < X & X < 2)  # ~ 95%\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.971\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(-3 < X & X < 3)  # ~ 99.7%\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.999\n```\n\n\n:::\n:::\n\n\n\n### T-Distribution\n\nAnother special distribution in statistical inference is the *Student’s t-Distribution*.\n$$f_X(x) =\n\\begin{cases}\n\\dfrac{\\Gamma\\!\\left(\\tfrac{\\nu+1}{2}\\right)}\n{\\sqrt{\\nu \\pi}\\,\\Gamma\\!\\left(\\tfrac{\\nu}{2}\\right)}\n\\left(1 + \\dfrac{x^2}{\\nu}\\right)^{-\\tfrac{\\nu+1}{2}}, & -\\infty < x < \\infty, \\\\[2ex]\n0, & \\text{otherwise},\n\\end{cases}$$\nwhere $\\nu$ is the degrees of freedom. As $\\nu \\to \\infty$, the pdf converges to the normal distribution.\n\n### Implementation in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_df <- 5\nX <- rt(1000, df = my_df)\n\nmean(X); var(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.05728994\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.678916\n```\n\n\n:::\n\n```{.r .cell-code}\nmy_df / (my_df - 2)   # Theoretical variance for df > 2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.666667\n```\n\n\n:::\n:::\n\n\n### Distribution Function Technique\n\nFor continuous random variables, a simple method for finding the probability density of a function of random variables is to find the distribution function and then differentiate to find the pdf.\n\nTo find an expression for the distribution function, let $Y = u(x_1, x_2, \\ldots, x_n)$, where $u$ is a function. Then,\n$$F(Y) = P(Y \\leq y) = P(u(x_1, x_2, \\ldots, x_n) \\leq y).$$\nThen,\n$$f(y) = \\frac{dF(Y)}{dy}.$$\n\nExample: \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create sequence from 0 to 1\nt <- seq(0, 1, length = 1000)\n\n# Define density function Y = 3 * (1 - t)^2\nY <- 3 * (1 - t)^2\n\n# Plot density\nplot(t, Y, type = \"l\")\n```\n\n::: {.cell-output-display}\n![](App_B-distribution_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Sample from t with probability weights Y\nZ <- sample(t, 10000, replace = TRUE, prob = Y)\n\n# Histogram of sampled values\nhist(Z, freq = FALSE)\n\n# Overlay the density curve\nlines(t, Y)\n```\n\n::: {.cell-output-display}\n![](App_B-distribution_files/figure-pdf/unnamed-chunk-7-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Apply transformation: X = (1 - Z)^3\nX.sample <- (1 - Z)^3\n\n# Histogram of transformed sample\nhist(X.sample)\n```\n\n::: {.cell-output-display}\n![](App_B-distribution_files/figure-pdf/unnamed-chunk-7-3.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Compare with uniform distribution\nU <- runif(10000)\n\n# QQ-plot to check distributional similarity\nqqplot(U, X.sample)\n```\n\n::: {.cell-output-display}\n![](App_B-distribution_files/figure-pdf/unnamed-chunk-7-4.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Correlation from QQ-plot\ncor(qqplot(U, X.sample)$x, qqplot(U, X.sample)$y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9999307\n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\nSpecial thanks to Dr. [Brian Pidgeon](https://cas.gsu.edu/profile/brian-pidgeon/) who kindly share the notes.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}