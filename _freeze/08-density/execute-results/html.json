{
  "hash": "057aa7c57018b073ec0bef921196722b",
  "result": {
    "engine": "knitr",
    "markdown": "# Density Estimation\n\n\\newcommand{\\E}{\\mathbb E}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\var}{\\mathbb{V}ar}\n\\newcommand{\\bx}{\\mathbf{x}}\n\\newcommand{\\bX}{\\mathbf{X}}\n\\newcommand{\\cov}{\\mathbb{C}ov}\n\\newcommand{\\mse}{\\mathrm{MSE}}\n\\newcommand{\\corr}{\\mathbb{C}orr}\n\\newcommand{\\unif}{\\operatorname{Unif}}\n\\newcommand{\\geom}{\\operatorname{Geom}}\n\\newcommand{\\bet}{\\operatorname{Beta}}\n\\newcommand{\\bern}{\\operatorname{Bern}}\n\\newcommand{\\iid}{\\overset{iid}{\\sim}}\n\\newcommand{\\ef}{\\operatorname{Eff}}\n\\newcommand{\\htt}{\\hat \\theta}\n\\newcommand{\\b}{\\mathbb b}\n\n\n\nUp to now, we have discussed various statistical methods for estimating parameters, testing hypotheses, and making predictions based on observed data. However, in many real-world applications, we often encounter situations where we need to estimate the underlying probability distribution of a dataset without assuming a specific parametric form. This is where density estimation comes into play.\n\nDensity estimation is a fundamental technique in statistics that allows us to estimate the probability density function (PDF). We know that, density does not always exist!\n\nThere are two main types of density estimation methods:\n\n1. **Parametric Density Estimation**: In this approach, we assume that the data follows a specific parametric distribution (e.g., normal, exponential, etc.) and estimate the parameters of that distribution using methods like Maximum Likelihood Estimation (MLE) or Method of Moments.\n\n2. **Non-Parametric Density Estimation**: This approach does not assume any specific parametric form for the underlying distribution. Instead, it estimates the density directly from the data using techniques such as Kernel Density Estimation (KDE) or Histogram-based methods.\n\n## Kenel Density Estimation (KDE)\n\nKernel Density Estimation (KDE) is a popular non-parametric method for estimating the probability density function of a random variable. The basic idea behind KDE is to place a smooth kernel function (e.g., Gaussian, Epanechnikov, etc.) at each data point and then sum these kernels to obtain a smooth estimate of the density.\n\nThe KDE at a point $x$ is given by:\n$$\\hat{f}(x) = \\frac{1}{n h} \\sum_{i=1}^{n} K\\left(\\frac{x - X_i}{h}\\right),\n$$\nwhere:\n\n- $\\hat{f}(x)$ is the estimated density at point $x$,\n- $n$ is the number of data points,\n- $h$ is the bandwidth (smoothing parameter),\n- $K$ is the kernel function,\n- $X_i$ are the observed data points.\n\nProperties of KDE:\n\n- The choice of kernel function $K$ affects the smoothness of the estimated density. Common choices include Gaussian, Epanechnikov, and Uniform kernels.\n\n- The bandwidth $h$ is a crucial parameter that controls the trade-off between bias and variance in the density estimate. A smaller bandwidth leads to a more detailed estimate (lower bias, higher variance), while a larger bandwidth results in a smoother estimate (higher bias, lower variance).\n\n### Example of KDE in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate sample data\nset.seed(123)\ndata <- rnorm(1000, mean = 0, sd = 1)\n# Perform Kernel Density Estimation\nkde <- density(data, bw = \"nrd0\")  # Using default bandwidth selection\n# Plot the results\nplot(kde, main = \"Kernel Density Estimation\", xlab = \"Value\", ylab = \"Density\")\n```\n\n::: {.cell-output-display}\n![](08-density_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# compare with the histogram\nplot(kde, main = \"Kernel Density Estimation\", xlab = \"Value\", ylab = \"Density\", col = \"blue\", lwd = 2)\nhist(data, probability = TRUE, breaks = 30, col = rgb(0.8, 0.2, 0.2, 0.5), add = TRUE)\nlegend(\"topright\", legend = c(\"KDE\", \"Histogram\"), col = c(\"blue\", rgb(0.8, 0.2, 0.2, 0.5)), lty = c(1, NA), pch = c(NA, 15))\n```\n\n::: {.cell-output-display}\n![](08-density_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n\n\n### Bandwidth Selection\nChoosing an appropriate bandwidth is critical for obtaining a good density estimate. Several methods exist for bandwidth selection, including:\n- **Rule of Thumb**: A simple method based on the standard deviation and sample size.\n- **Cross-Validation**: A data-driven approach that minimizes the integrated squared error.\n- **Plug-in Methods**: These methods estimate the optimal bandwidth based on the data's characteristics.\n\n### Example of Bandwidth Selection in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate sample data\nset.seed(123)\ndata <- rnorm(1000, mean = 0, sd = 1)\n# Perform KDE with different bandwidths\nkde_small_bw <- density(data, bw = 0.1)\nkde_large_bw <- density(data, bw = 1)\n# Plot the results\nplot(kde_small_bw, main = \"KDE with Different Bandwidths\", xlab = \"Value\", ylab = \"Density\", col = \"red\", lwd = 2)\nlines(kde_large_bw, col = \"green\", lwd = 2)\nlegend(\"topright\", legend = c(\"Small Bandwidth (0.1)\", \"Large Bandwidth (1)\"), col = c(\"red\", \"green\"), lty = 1)\n```\n\n::: {.cell-output-display}\n![](08-density_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nReference\n\n-   [Wikipedia](https://en.wikipedia.org/wiki/Density_estimation)\n\n-   Silverman, B.W. (1986). [Density Estimation for Statistics and Data Analysis](https://www.amazon.com/Density-Estimation-Statistics-Data-Analysis/dp/0412246201), Springer.\n\n-   University of Copenhagen, [Density Estimation](https://cswr.nrhstat.org/density)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}