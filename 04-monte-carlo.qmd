# Monte Carlo Simulation and Variance Reduction

\newcommand{\unif}{\operatorname{Unif}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\geom}{\operatorname{Geom}}
\newcommand{\beta}{\operatorname{Beta}}
\newcommand{\bern}{\operatorname{Bern}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\iid}{\overset{iid}{\sim}}
\newcommand{\cov}{\mathbb{C}ov}

```{r setup, message =FALSE, warning = FALSE, include = FALSE, eval = TRUE}
pacman::p_load(MASS, microbenchmark, ggplot2, dplyr, tibble)
# Set global theme
theme_set(
  theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5)  # center title
    )
)
```

Monte Carlo (MC) integration is a simulation-based method for *approximating integrals* using random sampling.

In numerical integration, methods such as the trapezoidal rule use a *deterministic approach*. MC integration, on the other hand, employs a *non-deterministic* approach: each realization provides a different outcome.

## Basic Monte Carlo Integration

To approximate the integral of a function $f(x)$ over the interval $[a, b]$, we can use the following formula:

Consider the problem of estimating $\theta = \int_0^1 g(x)dx$. If $X_1,\dots , X_m\sim\operatorname{Unif}(0,1)$, then the MC estimator is given by:

$$\hat{\theta}=\bar{g}_m(X)=\frac{1}{m}\sum_{i=1}^m g(X_i)$$ converges to $\mathbb{E}[g(X)]$ as $m\to\infty$ with probability 1, by *Strong law of Large Number* (SLLN). The simple MC estimator is unbiased, i.e., $\bar{g}_m(X)$.

::: {.callout-example title="Sample MC Integration"}
Compute a MC estimate 
$$
\theta = \int_0^1 \exp(-x)dx,
$$
and compare the estimate with the theoretical value

``` {r MC integration}
set.seed(777)
n <- 1E3
x <- runif(n)
# simulated estimator
theta_hat <- exp(-x) |> mean()

# theoretical value
theta_true <- 1 - exp(-1)

# put them in a tibble
(results <- tibble(
  Method = c("Simulated", "Theoretical"),
  Value  = c(theta_hat, theta_true)))
```

:::


To simulate $\int_a^b g(t)dt$, use change of variable so the limit becomes from $0$ to $1$. This can be done through a *linear transformation* of the variable $t$: $y:=\frac{t-a}{b-a}$. Then, $t=a+(b-a)y$ and $dt=(b-a)dy$. Thus, we have
$$
\int_a^b g(t)dt = (b-a)\int_0^1 g(a+(b-a)y)dy.
$$
Alternatively, instead of using $\unif(0,1)$, we can replace it with other densities with supports between $a$ and $b$. One instance is, 
$$
\int_a^b g(t)dt = \int_a^b \frac{g(t)}{f}f dt = (b-a) \int_a^b  \frac{g(t)}{b-a} dt.
$$
This is a $b-a$ times the expectation of $g(X)$ where $X\sim \unif(a,b)$. Therefore, this integral can be estimated by averaging through the function $g(\cdot)$ over the interval from $a$ to $b$ multiply by $b-a$.

::: {.callout-example title="MC Integration with different limits"}

Compute a MC estimate of 
$$
\theta = \int_2^4 \exp(-x)dx,
$$
and compare the estimate with the exact value of the integral.

``` {r MC2-integral}
library(tibble)

set.seed(777)
m <- 1E3
x <- runif(m, min = 2, max = 4)

# simulated estimator
theta_hat <- mean(exp(-x)) * (4 - 2)

# theoretical value
theta_true <- exp(-2) - exp(-4)

# put into tibble
(results <- tibble(
  Method = c("Simulated", "Theoretical"),
  Value  = c(theta_hat, theta_true)
))
```
:::

::: {.callout-algorithm}
To summarize, the simple Monte Carlo estimator of the integral $\theta = \int_a^b g(x)dx$ is computed as follows.

1. Generate $X_1, \dots , X_m\iid \unif(a,b)$,

2. Compute $\bar{g}(X) = \frac{1}{m} g(X_i)$.

3. $\hat{\theta} = (b − a)\bar{g}(X)$.
:::


::: {.callout-example title="MC integration with unbounded inteval"}

Compute a MC estimate of a standard normal cdf

$$
\Phi(x)=\int_{-\infty}^x \frac{1}{\sqrt{2 \pi}} e^{-t^2 / 2} d t
$$

1. Note, we cannot apply the algorithm above directly because
the limits of integration cover an *unbounded interval*. However, we can break this problem into two cases: (i) $x \ge 0$ and (ii) $x < 0$, and use the *symmetry* of the normal density to handle the second case. Then the problem is to estimate $\theta =\int_0^x \theta = \int_0^x \exp(−t^2/2) dt$ for $x > 0$. This can be done by generating random
$\unif(0,x)$ numbers, but it would mean changing the parameters of the
uniform distribution for each different value of the cdf required. Suppose that
we prefer an algorithm that always samples from $\unif(0,1)$.
This can be accomplished by a change of variables. Making the substitution
$y = t/x$, we have $dt = x dy$ and

$$
\theta=\int_0^1 x e^{-(x y)^2 / 2} d y
$$
Thus, $\theta = \E_Y[x\exp(-(xY)^2/2)]$, where the rv $Y\sim \unif(0,1)$. Generate iid $\unif(0,1)$ random numbers $u_1,\dots,u_m$, and compute
$$\hat{\theta}=\overline{g_m(u)}=\frac{1}{m} \sum_{i=1}^m x e^{-\left(u_i x\right)^2 / 2}.
$$

:::

------------------------------------------------------------------------

Reference used:

- Chapter 6 of Rizzo, M.L. (2007). *Statistical Computing with R*. CRC Press, Roca Baton.
