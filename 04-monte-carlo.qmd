# Monte Carlo Simulation and Variance Reduction

\newcommand{\unif}{\operatorname{Unif}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}{\mathbb{V}ar}
\newcommand{\geom}{\operatorname{Geom}}
\newcommand{\beta}{\operatorname{Beta}}
\newcommand{\bern}{\operatorname{Bern}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\iid}{\overset{iid}{\sim}}
\newcommand{\cov}{\mathbb{C}ov}

```{r setup, message =FALSE, warning = FALSE, include = FALSE, eval = TRUE}
pacman::p_load(MASS, microbenchmark, ggplot2, dplyr, tibble)
# Set global theme
theme_set(
  theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5)  # center title
    )
)
```

Monte Carlo (MC) integration is a simulation-based method for *approximating integrals* using random sampling.

In numerical integration, methods such as the trapezoidal rule use a *deterministic approach*. MC integration, on the other hand, employs a *non-deterministic* approach: each realization provides a different outcome.

## Basic Monte Carlo Integration

To approximate the integral of a function $f(x)$ over the interval $[a, b]$, we can use the following formula:

Consider the problem of estimating $\theta = \int_0^1 g(x)dx$. If $X_1,\dots , X_m\sim\operatorname{Unif}(0,1)$, then the MC estimator is given by:

$$\hat{\theta}=\bar{g}_m(X)=\frac{1}{m}\sum_{i=1}^m g(X_i)$$ converges to $\mathbb{E}[g(X)]$ as $m\to\infty$ with probability 1, by *Strong law of Large Number* (SLLN). The simple MC estimator is unbiased, i.e., $\bar{g}_m(X)$.

::: {.callout-example title="Sample MC Integration"}
Compute a MC estimate 
$$
\theta = \int_0^1 \exp(-x)dx,
$$
and compare the estimate with the theoretical value

``` {r MC integration}
set.seed(777)
n <- 1E3
x <- runif(n)
# simulated estimator
theta_hat <- exp(-x) |> mean()

# theoretical value
theta_true <- 1 - exp(-1)

# put them in a tibble
(results <- tibble(
  Method = c("Simulated", "Theoretical"),
  Value  = c(theta_hat, theta_true)))
```

:::


To simulate $\int_a^b g(t)dt$, use change of variable so the limit becomes from $0$ to $1$. This can be done through a *linear transformation* of the variable $t$: $y:=\frac{t-a}{b-a}$. Then, $t=a+(b-a)y$ and $dt=(b-a)dy$. Thus, we have
$$
\int_a^b g(t)dt = (b-a)\int_0^1 g(a+(b-a)y)dy.
$$
Alternatively, instead of using $\unif(0,1)$, we can replace it with other densities with supports between $a$ and $b$. One instance is, 
$$
\int_a^b g(t)dt = \int_a^b \frac{g(t)}{f}f dt = (b-a) \int_a^b  \frac{g(t)}{b-a} dt.
$$
This is a $b-a$ times the expectation of $g(X)$ where $X\sim \unif(a,b)$. Therefore, this integral can be estimated by averaging through the function $g(\cdot)$ over the interval from $a$ to $b$ multiply by $b-a$.

::: {.callout-example title="MC Integration with different limits"}

Compute a MC estimate of 
$$
\theta = \int_2^4 \exp(-x)dx,
$$
and compare the estimate with the exact value of the integral.

``` {r MC2-integral}
library(tibble)

set.seed(777)
m <- 1E3
x <- runif(m, min = 2, max = 4)

# simulated estimator
theta_hat <- exp(-x) |> mean() * (4 - 2)

# theoretical value
theta_true <- exp(-2) - exp(-4)

# put into tibble
(results <- tibble(
  Method = c("Simulated", "Theoretical"),
  Value  = c(theta_hat, theta_true)
))
```
:::

::: {.callout-algorithm}
To summarize, the simple Monte Carlo estimator of the integral $\theta = \int_a^b g(x)dx$ is computed as follows.

1. Generate $X_1, \dots , X_m\iid \unif(a,b)$,

2. Compute $\bar{g}(X) = \frac{1}{m} g(X_i)$.

3. $\hat{\theta} = (b − a)\bar{g}(X)$.
:::


::: {.callout-example title="MC integration with unbounded inteval"}

Compute a MC estimate of a standard normal cdf

$$
\Phi(x)=\int_{-\infty}^x \frac{1}{\sqrt{2 \pi}} e^{-t^2 / 2} d t
$$

1. Note, we cannot apply the algorithm above directly because
the limits of integration cover an *unbounded interval*. However, we can break this problem into two cases: (i) $x \ge 0$ and (ii) $x < 0$, and use the *symmetry* of the normal density to handle the second case. Then the problem is to estimate $\theta =\int_0^x \theta = \int_0^x \exp(−t^2/2) dt$ for $x > 0$. This can be done by generating random
$\unif(0,x)$ numbers, but it would mean changing the parameters of the
uniform distribution for each different value of the cdf required. Suppose that
we prefer an algorithm that always samples from $\unif(0,1)$.
This can be accomplished by a change of variables. Making the substitution
$y = t/x$, we have $dt = x dy$ and

$$
\theta=\int_0^1 x e^{-(x y)^2 / 2} d y
$$
Thus, $\theta = \E_Y[x\exp(-(xY)^2/2)]$, where the rv $Y\sim \unif(0,1)$. Generate iid $\unif(0,1)$ random numbers $u_1,\dots,u_m$, and compute
$$\hat{\theta}=\overline{g_m(u)}=\frac{1}{m} \sum_{i=1}^m x e^{-\left(u_i x\right)^2 / 2}.
$$

The sample mean $\hat{\theta}$ converges to $\E \hat{\theta} = \theta$ as $m\to \infty$. If $x > 0$, the estimate
of $\Phi(x) = 1/2 + \hat{\theta}/\sqrt{2\pi}$. If $x < 0$ compute $\Phi(x) = 1 − \Phi(−x)$.

``` {r MC-normal-cdf}
x <- seq(.1, 2.5, length = 10)
m <- 10000
u <- runif(m)
cdf <- numeric(length(x))
for (i in 1:length(x)) {
  g <- x[i] * exp(-(u * x[i])^2 / 2)
  cdf[i] <- mean(g) / sqrt(2 * pi) + 0.5
}
Phi <- pnorm(x)
print(round(rbind(x, cdf, Phi), 3))
```

Notice that it would have been simpler to generate random Uniform(0, x)
random variables and skip the transformation. In
fact, the integrand of the previous example is itself a density function, and we can generate random variables from this density. This provides a more direct approach to estimating the integral.

:::

::: {.callout-example title="MC integration with indicator function"}

Let $I(\cdot)$ be the indicator function, and $Z\sim N(0,1)$. Then for any constant $x$ we have $\E[I(Z ≤ x)] = P (Z ≤ x) =\Phi(x)$, the standard normal cdf evaluated at $x$.

Generate a random sample $z_1, \dots , z_m\sim N(0,1)$. Then the theoretical mean and sample mean are 
$$\hat{\theta} = \frac{1}{m} \sum_{i=1}^m I(z_i \le x),$$
and 
$$\E[\hat{\theta}] = P(Z \le x) = \Phi(x).$$

``` {r MC-normal-cdf2}
set.seed(777)
x <- seq(0.1, 2.5, length = 10)
m <- 1E4
z <- rnorm(m)
dim(x) <- length(x)
p <- apply(x, MARGIN = 1,
  FUN = function(x, z) {mean(z < x)}, z = z)
Phi <- pnorm(x)

rbind(x, p, Phi) |> round(3)
```


In this example, compared with the previous example, it appears that
we have better agreement with `pnorm()` in the upper tail, but worse agreement
near the center.

:::

::: {.callout-note}
Summarizing, if $f (x)$ is a probability density function supported on a set
A, (that is, $f (x) \ge 0$ for all $x\in\R$ and $\int_A f (x) = 1$), to estimate the integral
$$\theta = \int_A g(x)f (x)dx,$$
generate a random sample $x_1,\dots,x_m$ from the distribution $f (x)$, and compute the sample mean
$$\hat{\theta} = \frac{1}{m}\sum_{i=1}^mg(x_i).$$ Then  $\hat{\theta}\overset{p}{\to}\theta$ 
:::


The standard error of $\hat{\theta} = \frac{1}{m}\sum_{i=1}^m g(x_i)$.

Recall that, $\var{\hat{\theta}}=\sigma^2/m$ where $\sigma^2 = \var{g(X)}$. When the distribution of $X$ is unknown, we substitute for $F_X$ the empirical distribution $F_m$ of the
sample $x_1, \dots , x_m$. The variance of $\hat{\theta}$ can be estimated by
$$
\frac{\hat{\sigma}}{m} = \frac{1}{m^2}\sum_{i=1}^m [g(x_i) - \bar{g}(x)]^2.
$$

Note that
$$\frac{1}{m}\sum_{i=1}^m[g(x_i)-\bar{g}(x_i)],$$
is the *plug-in estimate* of $\var\{g(X)\}$. That is, it is the variance of $U$ , where
$U$ is uniformly distributed on the set of replicates $\{g(xi)\}$. The corresponding estimated standard error of $\hat{\theta}$ is 
$$
\widehat{se}(\hat{\theta}) = \frac{\hat{\sigma}}{\sqrt{m}} = \frac{1}{m}\left\{\sum_{i=1}^m [ g(x_i)- \bar{g}(x)]^2\right\}.
$$
The CLT implies 
$$
\frac{\hat{\theta}-E[\hat{\theta}]}{\sqrt{\operatorname{Var} \hat{\theta}}} \overset{D}{\to} N(0,1),
$$
as $m\to\infty$.Hence, if $m$ is sufficiently large, $\hat{\theta}$
is approximately normal with mean $\theta$. The large-sample, approximately normal distribution of $\hat{\theta}$ can be applied to put confidence limits or error bounds
on the MC estimate of the integral, and check for convergence

::: {.callout-example title = "Error bound of MC integration"}

Compute the 95% confidence interval for $\Phi(2)$ and $\Phi(2.5)$.

``` {r error-bound}
set.seed(777)
x <- 2
m <- 10000
z <- rnorm(m)
g <- (z < x) #the indicator function
v <- mean((g - mean(g))^2) / m
cdf <- mean(g)
c(cdf, v)
c(cdf - 1.96 * sqrt(v), cdf + 1.96 * sqrt(v))
```

The interpretation is:


the probability $P (I(Z < x) = 1)$ is $\Phi(2)\approx 0.977$. Here $g(X)$ has the distribution of the sample proportion of 1’s in $m = 10000$ Bernoulli trials with
$p\approx 0.977$, and the variance of $g(X)$ is therefore $(0.977)(1 − 0.977)/10000 =2.223e-06$. The MC estimate $2.228e-06$ of variance is quite close to this value.

Q: What about $\Phi(2.5)$?
:::

## Variance and Efficiency


### Efficiency
Suppose we have two (unbiased) estimators $\hat{\theta}_1$ and $\hat{\theta}_2$ for θ. We say $\hat{\theta}_1$ is *statistically* more efficient than $\hat{\theta}_2$ if
$$\var(\hat{\theta}_1) < \var(\hat{\theta}_2).$$
What is the variance of $hat{\theta}_i$ is unknown or hard to be calculated?

can substitute it by sample estimate of the variance for each estimator.

Note, the variance can *always be reduced by increasing the number of
replicates*, so computational efficiency is also relevant.

------------------------------------------------------------------------

Reference used:

- Chapter 6 of Rizzo, M.L. (2007). *Statistical Computing with R*. CRC Press, Roca Baton.
